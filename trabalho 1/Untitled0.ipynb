{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Projeto 1\n",
        "\n",
        "Este trabalho compara a efici√™ncia e efic√°cia de m√∫ltiplos algoritmos de Aprendizado por Refor√ßo (RL), sem eles MonteCarlo, Sarsa(Lambda) e Q_learning, em diferentes cen√°rios e variando seus par√¢metros quando necess√°rio. O ambiente/problema que foi usado como teste s√£o labirintos, onde os algoritmos precisam encontrar uma politica √≥tima para que o agente consiga chegar at√© o final do labirinto.\n",
        "\n",
        "### Ambiente\n",
        "Como dito anteriormente, os ambientes s√£o labirintos. Eles s√£o descritos por meio de arquivos .txt, onde s√£o apontados as paredes, caminhos, estados terminais, formato e suas recompensas. Os arquivos t√™m essa cara:\n",
        "```\n",
        "4 4\n",
        ". path -1\n",
        "@ agent -1\n",
        "$ goal 100\n",
        "# wall -1000\n",
        "######\n",
        "#...$#\n",
        "#@...#\n",
        "######\n",
        "```\n",
        "\n",
        "Aqui √© descrito um labirinto de tamanho 4x4, onde os caminhos s√£o '.' e possuem recompensa de -1. Por sua o estado terminal, ou objetivo, √© representado por '$' e possui recompensa de 100. Por fim, ap√≥s a descri√ß√£o dos simbolos, h√° a descri√ß√£o do pr√≥prio formato do labirinto.\n",
        "\n",
        "Diferentes formatos de labirintos foram adotados para o trabalho, sendo alguns bem aplos, outros com diversos caminhos, e outros extremamente estreitos. As recompensas tamb√©m foram alteradas para avaliar seu impacto, j√° que a escolha de uma fun√ß√£o de recompensa boa tamb√©m faz parte do processo quando se trabalha com RL.\n",
        "\n",
        "### C√≥digo\n",
        "As primeiras c√©lulas desse notebook cont√©m o c√≥digo usado para implementa√ß√£o e obten√ß√£o dos resultados. Por√©m para facilitar, foram geradas imagens com os resultados obtidos, e a an√°lise foi feita em cima delas. O c√≥digo fica a disposi√ß√£o para quem quiser rodar novamente, mudar valores, testar outras configura√ß√µes de labirinto.\n",
        "\n",
        "### M√©tricas\n",
        "Para compara√ß√£o dos algoritmos entre si, foram extra√≠das as recompensas m√©dias e o tamanho do caminho percorrido a partir de um mesmo ponto do mapa. Com isso √© poss√≠vel analisar o qu√£o eficiente foi o caminho encontrado pelo algoritmo, caso encontrado, e se ele conseguiu convergir. H√° tamb√©m a analise de desempenho e custo computacional, em m√©dia quantos epis√≥dios foram necess√°rios para que o algoritmo converja? Quanto tempo de treinamento um algoritmo precisou para isso? H√° tamb√©m as analises individuais. Quantos hiperparametros s√£o necess√°rios para ajustar o algoritmo, e o qu√£o dificil √© encontrar um valor √≥timo?  \n",
        "\n",
        "Al√©m disso, um mapa de calor gerado a partir da fun√ß√£o valor ser√° usado para ilustrar o qu√£o bom √© estar em cada estado, ou seja, em cada posi√ß√£o do labirinto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "lftFAb-BqMew"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from itertools import product\n",
        "import random\n",
        "from threading import Thread\n",
        "import time\n",
        "from argparse import ArgumentParser\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OwSgimmVxFIw"
      },
      "outputs": [],
      "source": [
        "class Renderer():\n",
        "    def __init__(self, chief, content, title=None, dimensions=(800, 800)):\n",
        "        self.chief = chief\n",
        "        self.content = content\n",
        "        self.contents = [content] # para trocar entre janelas\n",
        "        self.iConteudoAtual = 0 # para marcar qual o atual dentre os varios\n",
        "        self.title = title\n",
        "        self.dimensions = dimensions\n",
        "        self.running = True\n",
        "        if not self.title:\n",
        "            self.title = type(chief).__name__   # title √© o nome da classe\n",
        "        \n",
        "        self.load_sprites()\n",
        "\n",
        "    def add_content(self,content):\n",
        "        self.contents.append(content)\n",
        "\n",
        "    def load_sprites(self):\n",
        "        self.sprites = dict()\n",
        "        self.sprites[\"path\"] = '‚¨õ'\n",
        "        self.sprites[\"wall\"] = 'üß±'\n",
        "        self.sprites[\"goal\"] = '‚öΩ'\n",
        "        self.sprites[\"agent\"] = 'üëæ' \n",
        "        self.sprites[\"right\"] = '‚û°Ô∏è'\n",
        "        self.sprites[\"up\"] = '‚¨ÜÔ∏è'\n",
        "        self.sprites[\"left\"] = '‚¨ÖÔ∏è' \n",
        "        self.sprites[\"down\"] = '‚¨áÔ∏è'\n",
        "\n",
        "\n",
        "    def show(self):\n",
        "        for i in range(len(self.content)):\n",
        "            for j in range(len(self.content[0])):\n",
        "                cell = self.content[i][j]\n",
        "                # se o content de cell estiver no dicionario de sprites\n",
        "                if cell in self.sprites:\n",
        "                    obj = cell\n",
        "                else:\n",
        "                    obj = self.chief.symbols[cell]\n",
        "                print(self.sprites.get(obj,'‚ùå'),end='')\n",
        "            print('')\n",
        "\n",
        "    def create_heatmap(data, cmap='viridis', title='Heatmap'):\n",
        "        \"\"\"\n",
        "        Create a heatmap from a list of lists of floats.\n",
        "\n",
        "        Parameters:\n",
        "        - data: List of lists of floats representing the heatmap data.\n",
        "        - cmap: Colormap for the heatmap (default is 'viridis').\n",
        "        - title: Title for the heatmap (default is 'Heatmap').\n",
        "        \"\"\"\n",
        "        data = np.array(data, dtype=float)\n",
        "\n",
        "        # Create a figure and axis\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        # Display the heatmap using imshow\n",
        "        im = ax.imshow(data, cmap=cmap)\n",
        "\n",
        "        # Add a colorbar to the right of the heatmap\n",
        "        cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "        # Set the title\n",
        "        ax.set_title(title)\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n",
        "\n",
        "    def show_path(self, path):\n",
        "        for i in range(len(self.content)):\n",
        "            for j in range(len(self.content[0])):\n",
        "                cell = self.content[i][j]\n",
        "                if((i,j) in path.keys()):\n",
        "                    cell = path[(i,j)]\n",
        "                # se o content de cell estiver no dicionario de sprites\n",
        "                if cell in self.sprites:\n",
        "                    obj = cell\n",
        "                else:\n",
        "                    obj = self.chief.symbols[cell]\n",
        "                print(self.sprites.get(obj,'‚ùå'),end='')\n",
        "            print('')\n",
        "\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    actions = ['up', 'down', 'left', 'right']\n",
        "    def __init__(self, x, y, environment, gamma = 0.9, display=True):\n",
        "        self.environment = environment\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.gamma = gamma\n",
        "        self.display = display\n",
        "\n",
        "    def action_idx(self, action: str):\n",
        "        return self.actions.index(action)\n",
        "\n",
        "\n",
        "    def startQ(self, shape, start_value = float(\"-inf\")):\n",
        "        \"\"\"\n",
        "        livroQ √© uma lista de listas de dicionarios,\n",
        "        ele armazena \n",
        "        \n",
        "        \"\"\"\n",
        "        self.book_Q: list[list[dict]] = []\n",
        "        for i in range(shape[0]):\n",
        "            self.book_Q.append([])\n",
        "            for _ in range(shape[1]):\n",
        "                self.book_Q[i].append(dict())\n",
        "                for action in self.actions:\n",
        "                    self.book_Q[i][-1][action] = start_value\n",
        "\n",
        "    def startV(self, shape):\n",
        "        \"\"\"\n",
        "        livroV √© uma lista de listas de dicionarios,\n",
        "        ele armazena \n",
        "        \n",
        "        \"\"\" \n",
        "        self.book_V: list[list] = []\n",
        "        for i in range(shape[0]):\n",
        "            self.book_V.append([])\n",
        "            for _ in range(shape[1]):\n",
        "                \n",
        "                self.book_V[i].append(float(\"-inf\"))\n",
        "    \n",
        "    def startPolicy(self, shape, randomPolicy):\n",
        "        \"\"\"\n",
        "        A policy √© uma matriz de caracteres que guarda a action principal\n",
        "        a ser tomada ate o momento\n",
        "        \n",
        "        \"\"\"\n",
        "        self.policy: list[list[str]] = []\n",
        "        for i in range(shape[0]):\n",
        "            self.policy.append([])\n",
        "            for j in range(shape[1]):\n",
        "                if self.environment.symbols[self.environment.original_map[i][j]] == \"wall\":\n",
        "                    self.policy[i].append(\"wall\")\n",
        "                    continue\n",
        "                if randomPolicy:\n",
        "                    self.policy[i].append(random.choice(self.actions))\n",
        "                else:\n",
        "                    self.policy[i].append(self.actions[0])\n",
        "        if self.display:\n",
        "            self.render = Renderer(self, self.policy, \"Agente\")\n",
        "\n",
        "    def startReturns(self, shape):\n",
        "        \"\"\"\n",
        "        returns √© uma colecao de pares state action guardando um\n",
        "        dicionario para armazenar o valor maximo de rewards obtidos,\n",
        "        o numero de vezes que o par state action foi visitado e o ultimo\n",
        "        episodio em que o par state action foi visitado\n",
        "        \"\"\"\n",
        "        self.returns: list[list[dict]] = []\n",
        "        for i in range(shape[0]):\n",
        "            self.returns.append([])\n",
        "            for j in range(shape[1]):\n",
        "                self.returns[i].append(dict())\n",
        "                for action in self.actions:\n",
        "                    self.returns[i][j][action] = {\"value\": 0, \"count\": 0, \"lastEpisode\": 0}\n",
        "\n",
        "    def setEnvironment(self, environment):\n",
        "        self.environment = environment\n",
        "    \n",
        "    def setPos(self, position):\n",
        "        self.x = position[1]\n",
        "        self.y = position[0]\n",
        "\n",
        "    def move(self, action):\n",
        "        return self.environment.move(self, action)\n",
        "    \n",
        "    def get_action(self):\n",
        "        return self.policy[self.y][self.x]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlGKK60NqTV4",
        "outputId": "d6167f45-bc5f-49b6-8732-de679f17aeec"
      },
      "outputs": [],
      "source": [
        "class LearningStrategy():\n",
        "    def train(self, episodes):\n",
        "        pass\n",
        "\n",
        "    def setup(self, environment, agent):\n",
        "        self.environment = environment\n",
        "        self.agent = agent\n",
        "\n",
        "class MonteCarlo(LearningStrategy):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.episode_R = []\n",
        "        self.episode_length = []\n",
        "        self.Q = None\n",
        "        self.W = np.random.rand(3)\n",
        "        self.time = []\n",
        "\n",
        "\n",
        "    def get_Q(self, x, y, action, linear_approximation = False):\n",
        "        if not linear_approximation:\n",
        "            return self.Q[x,y,action]\n",
        "        \n",
        "        terms = np.array([x,y,action])\n",
        "        return np.dot(self.W, terms).astype(int)\n",
        "    \n",
        "    def train(self, episodes, randomPolicy = True, exploration_chance = 0):\n",
        "        # Initialize\n",
        "        shape = self.environment.get_size()\n",
        "        self.agent.startPolicy(shape, randomPolicy)\n",
        "        self.agent.startReturns(shape)\n",
        "        self.Q = np.zeros((shape[0],shape[1], len(self.agent.actions)))\n",
        "        self.agent.startV(shape)\n",
        "\n",
        "        for ep in range(episodes):\n",
        "            start_time = time.time()\n",
        "            if ep % (episodes//10) == 0:\n",
        "                print(f\"{ep=}\")\n",
        "                path = self.path_from((1,1))\n",
        "                print('Tamanho do epis√≥dio:', len(path))\n",
        "                print('Recompensa', sum([i[2] for i in path]))\n",
        "                path_dict = {}\n",
        "                for s,a,r in path:\n",
        "                    path_dict[s] = a\n",
        "                self.environment.render.show_path(path_dict)\n",
        "            # escolhe posicao aleatoria valida para o agente\n",
        "            while True:\n",
        "                state = (random.randrange(0, shape[0]), random.randrange(0, shape[1]))\n",
        "                if self.environment.original_map[state[0]][state[1]] in {self.environment.default_symbols[\"path\"], self.environment.default_symbols[\"goal\"]}:\n",
        "                    break\n",
        "            # escolhe uma action diferente da dita pela politica atual\n",
        "            for _ in range(len(self.agent.actions)*2):    # limite maximo de tentativas\n",
        "                action = random.choice(self.agent.actions)\n",
        "                if action != self.agent.policy[state[0]][state[1]]:\n",
        "                    # se a action nao te leva para uma parede\n",
        "                    if self.environment.util(state, action): \n",
        "                        break\n",
        "            else:\n",
        "                action = self.agent.policy[state[0]][state[1]]\n",
        "            self.episode(state, action, max_steps= shape[1]*shape[0], exploration_chance = exploration_chance)\n",
        "            g = 0\n",
        "            for t in range(len(self.agent.recalls)-1, -1, -1): \n",
        "                memory = self.agent.recalls[t]  # memory = (state, action, reforco)\n",
        "                g = self.agent.gamma*g + memory[2]\n",
        "                # verifica se o par state action ja foi inserido em returns\n",
        "                if self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"lastEpisode\"] != ep:\n",
        "                    self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"lastEpisode\"] = ep\n",
        "                    self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"value\"] += g\n",
        "                    self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"count\"] += 1\n",
        "                    media = self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"value\"]/self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"count\"]\n",
        "                    self.Q[memory[0][0],memory[0][1],self.agent.action_idx(memory[1])] = media\n",
        "                    self.agent.book_V[memory[0][0]][memory[0][1]] = media\n",
        "                    self.agent.policy[memory[0][0]][memory[0][1]] = max(self.agent.actions, key = lambda action: self.get_Q(memory[0][0],memory[0][1],self.agent.action_idx(action)))    # recebe a action que maximiza o valor de Q\n",
        "            end_time = time.time()\n",
        "            time_difference_seconds = end_time - start_time\n",
        "            self.time.append(time_difference_seconds)\n",
        "\n",
        "    def episode(self, state, action, max_steps, exploration_chance=0):\n",
        "        step_count = 0\n",
        "        self.agent.recalls = []\n",
        "        self.environment.setAgentPos(state[0], state[1])\n",
        "        episode_R = []\n",
        "        while (not self.environment.in_terminal_state()) and (step_count < max_steps):  # enquanto nao estiver em um state terminal\n",
        "            step_count +=1  # incrementa o numero de passos\n",
        "            lastPos = (self.agent.y, self.agent.x)\n",
        "            reward = self.environment.move(self.agent,action) # realiza a action e recebe a recompensa\n",
        "            episode_R.append(reward)\n",
        "            self.agent.recalls.append((lastPos, action, reward)) # guarda o passo\n",
        "            if random.random() < exploration_chance:\n",
        "                for _ in range(len(self.agent.actions)*2):    # limite maximo de tentativas\n",
        "                    action = random.choice(self.agent.actions)\n",
        "                    # se a action nao te leva para uma parede\n",
        "                    if self.environment.util(state, action): break\n",
        "            else:\n",
        "                action = self.agent.get_action() # escolhe uma action de acordo com a politica\n",
        "        self.episode_R.append(episode_R)\n",
        "        self.episode_length.append(step_count)\n",
        "\n",
        "    def path_from(self, starting_point):\n",
        "        shape = self.environment.get_size()\n",
        "        max_steps = shape[0] * shape[1]\n",
        "        step_count = 0\n",
        "        state = starting_point\n",
        "        self.environment.setAgentPos(state[0], state[1])\n",
        "        tuples = []\n",
        "        while (not self.environment.in_terminal_state()) and (step_count < max_steps):\n",
        "            action =  self.agent.policy[state[0]][state[1]]\n",
        "            R = self.environment.move(self.agent, action)\n",
        "            tuples.append((state,action,R))\n",
        "            state = (self.agent.y, self.agent.x)\n",
        "            step_count+=1\n",
        "        return tuples\n",
        "\n",
        "\n",
        "class SARSA(LearningStrategy):\n",
        "\n",
        "    def __init__(self, lam):\n",
        "        super().__init__()\n",
        "        self.lam = lam\n",
        "        self.episode_R = []\n",
        "        self.episode_length = []\n",
        "        self.Q = None\n",
        "        self.W = np.random.rand(3)\n",
        "        self.time = []\n",
        "\n",
        "    def get_Q(self, x, y, action, linear_approximation = False):\n",
        "        if not linear_approximation:\n",
        "            return self.Q[x,y,action]\n",
        "        \n",
        "        terms = np.array([x,y,action])\n",
        "        return np.dot(self.W, terms).astype(int)\n",
        "        \n",
        "\n",
        "    def get_greedy_action(self,state):\n",
        "        return max(self.agent.actions, key = lambda action: self.get_Q(state[0], state[1], self.agent.action_idx(action)))\n",
        "    \n",
        "    def get_epsilon_greedy(self, exploration_chance, state):\n",
        "        if random.random() < exploration_chance:\n",
        "            return random.choice(self.agent.actions)\n",
        "        else:\n",
        "            return max(self.agent.actions, key = lambda action: self.get_Q(state[0], state[1], self.agent.action_idx(action)))\n",
        "                \n",
        "    def train(self, episodes, random_policy=True, exploration_chance=0.3, alpha=0.001):\n",
        "        shape = self.environment.get_size()\n",
        "        ec = exploration_chance\n",
        "        num_states = shape[0]*shape[1]\n",
        "        linear_decay = exploration_chance/episodes\n",
        "        self.Q = np.zeros((shape[0],shape[1], len(self.agent.actions)))\n",
        "\n",
        "        E = dict()\n",
        "        for ep in range(episodes):\n",
        "            start_time = time.time()\n",
        "            episode_R = []\n",
        "            if ep % (episodes//10) == 0: \n",
        "                print(f\"{ep=}\")\n",
        "                path = self.path_from((1,1))\n",
        "                print('Tamanho do epis√≥dio:', len(path))\n",
        "                print('Recompensa', sum([i[2] for i in path]))\n",
        "                path_dict = {}\n",
        "                for s,a,r in path:\n",
        "                    path_dict[s] = a\n",
        "                self.environment.render.show_path(path_dict)\n",
        "\n",
        "            E = dict()         \n",
        "            \n",
        "            # escolhe posicao aleatoria valida para o agente\n",
        "            while True:\n",
        "                S = (random.randrange(0, shape[0]), random.randrange(0, shape[1]))\n",
        "                if self.environment.original_map[S[0]][S[1]] in {self.environment.default_symbols[\"path\"], self.environment.default_symbols[\"goal\"]}:\n",
        "                    break\n",
        "            self.environment.setAgentPos(S[0], S[1])\n",
        "\n",
        "            A = self.get_epsilon_greedy(ec,S)\n",
        "            A_idx = self.agent.action_idx(A)\n",
        "\n",
        "            step_count = 0\n",
        "            while (not self.environment.in_terminal_state()) and (step_count < num_states):\n",
        "\n",
        "                R = self.environment.move(self.agent, A)\n",
        "                episode_R.append(R)\n",
        "                S_prime = (self.agent.y, self.agent.x)\n",
        "                A_prime = self.get_epsilon_greedy(ec, S_prime)\n",
        "                A_prime_idx = self.agent.action_idx(A)\n",
        "                \n",
        "                pair = (S, A)\n",
        "                E[pair] = E[pair] + 1 if pair in E.keys() else 1\n",
        "\n",
        "                delta = R + self.agent.gamma * self.get_Q(S_prime[0], S_prime[1], A_prime_idx) - self.get_Q(S[0], S[1], A_idx)\n",
        "\n",
        "                for (s, a) in E.keys():\n",
        "                    a_idx = self.agent.action_idx(a)\n",
        "                    self.Q[s[0],s[1], a_idx] += alpha * delta * E[(s,a)]\n",
        "                    E[(s,a)] *= self.agent.gamma * self.lam\n",
        "                \n",
        "                S = S_prime\n",
        "                A = A_prime\n",
        "                step_count += 1\n",
        "            self.episode_R.append(episode_R)\n",
        "            self.episode_length.append(step_count)\n",
        "            ec-=linear_decay\n",
        "            end_time = time.time()\n",
        "            time_difference_seconds = end_time - start_time\n",
        "            self.time.append(time_difference_seconds)\n",
        "            \n",
        "\n",
        "        self.agent.startPolicy(shape, random_policy)\n",
        "        for i in range(shape[0]):\n",
        "            for j in range(shape[1]):\n",
        "                if(self.environment.original_map[i][j] == '#'): self.agent.policy[i][j] = \"wall\"\n",
        "                else: \n",
        "                    self.agent.policy[i][j] = max(self.agent.actions, key = lambda action: self.get_Q(i,j, self.agent.action_idx(action)))\n",
        "\n",
        "    def path_from(self, starting_point):\n",
        "        shape = self.environment.get_size()\n",
        "        max_steps = shape[0] * shape[1]\n",
        "        step_count = 0\n",
        "        S = starting_point\n",
        "        self.environment.setAgentPos(S[0], S[1])\n",
        "        tuples = []\n",
        "        while (not self.environment.in_terminal_state()) and (step_count < max_steps):\n",
        "            A = self.get_greedy_action(S)\n",
        "            R = self.environment.move(self.agent, A)\n",
        "            tuples.append((S,A,R))\n",
        "            S = (self.agent.y, self.agent.x)\n",
        "            step_count+=1\n",
        "        return tuples\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "class LinearFunctionApproximation(LearningStrategy):\n",
        "    ...\n",
        "\n",
        "class QLearning(LearningStrategy):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.episode_R = []\n",
        "        self.episode_length = []\n",
        "        self.Q = None\n",
        "        self.W = np.random.rand(3)\n",
        "        self.time = []\n",
        "\n",
        "    def get_Q(self, x, y, action, linear_approximation = False):\n",
        "        if not linear_approximation:\n",
        "            return self.Q[x,y,action]\n",
        "        \n",
        "        terms = np.array([x,y,action])\n",
        "        return np.dot(self.W, terms).astype(int)\n",
        "\n",
        "    def get_greedy_action(self,state):\n",
        "        return max(self.agent.actions, key = lambda action: self.get_Q(state[0], state[1], self.agent.action_idx(action)))\n",
        "    \n",
        "    def get_epsilon_greedy(self, exploration_chance, state):\n",
        "        if random.random() < exploration_chance:\n",
        "            return random.choice(self.agent.actions)\n",
        "        else:\n",
        "            return max(self.agent.actions, key = lambda action: self.get_Q(state[0], state[1], self.agent.action_idx(action)))\n",
        "                \n",
        "    def train(self, episodes, random_policy=True, exploration_chance=0.3, alpha=0.003):\n",
        "        shape = self.environment.get_size()\n",
        "        ec = exploration_chance\n",
        "        num_states = shape[0]*shape[1]\n",
        "        linear_decay = exploration_chance/episodes\n",
        "        self.Q = np.zeros((shape[0],shape[1], len(self.agent.actions)))\n",
        "\n",
        "        for ep in range(episodes):\n",
        "            start_time = time.time()\n",
        "            episode_R = []\n",
        "            if ep % (episodes//10) == 0: \n",
        "                print(f\"{ep=}\")\n",
        "                path = self.path_from((1,1))\n",
        "                print('Tamanho do epis√≥dio:', len(path))\n",
        "                print('Recompensa', sum([i[2] for i in path]))\n",
        "                path_dict = {}\n",
        "                for s,a,r in path:\n",
        "                    path_dict[s] = a\n",
        "                self.environment.render.show_path(path_dict)\n",
        "\n",
        "            # escolhe posicao aleatoria valida para o agente\n",
        "            while True:\n",
        "                S = (random.randrange(0, shape[0]), random.randrange(0, shape[1]))\n",
        "                if self.environment.original_map[S[0]][S[1]] in {self.environment.default_symbols[\"path\"], self.environment.default_symbols[\"goal\"]}:\n",
        "                    break\n",
        "            self.environment.setAgentPos(S[0], S[1])\n",
        "\n",
        "            #A = random.choice(self.agent.actions)\n",
        "            step_count = 0\n",
        "            while (not self.environment.in_terminal_state()) and (step_count < num_states):\n",
        "                A = self.get_epsilon_greedy(ec,S)\n",
        "                A_idx = self.agent.action_idx(A)\n",
        "\n",
        "                R = self.environment.move(self.agent, A)\n",
        "\n",
        "                episode_R.append(R)\n",
        "\n",
        "                S_prime = (self.agent.y, self.agent.x)\n",
        "                A_prime = self.get_greedy_action(S_prime)\n",
        "                A_prime_idx = self.agent.action_idx(A)\n",
        "\n",
        "\n",
        "                self.Q[S[0], S[1], A_idx] += alpha*(R + self.agent.gamma * self.get_Q(S_prime[0], S_prime[1], A_prime_idx) - self.get_Q(S[0], S[1], A_idx))\n",
        "\n",
        "                \n",
        "                S = S_prime\n",
        "                step_count += 1\n",
        "                \n",
        "            self.episode_R.append(episode_R)\n",
        "            self.episode_length.append(step_count)\n",
        "            ec-=linear_decay\n",
        "            end_time = time.time()\n",
        "            time_difference_seconds = end_time - start_time\n",
        "            self.time.append(time_difference_seconds)\n",
        "            \n",
        "\n",
        "        self.agent.startPolicy(shape, random_policy)\n",
        "        self.agent.startV(shape)\n",
        "        for i in range(shape[0]):\n",
        "            for j in range(shape[1]):\n",
        "                if(self.environment.original_map[i][j] == '#'): self.agent.policy[i][j] = \"wall\"\n",
        "                else: \n",
        "                    self.agent.policy[i][j] = max(self.agent.actions, key = lambda action: self.get_Q(i,j, self.agent.action_idx(action)))\n",
        "                    self.agent.book_V[i][j] = max(self.Q[i,j,:])\n",
        "\n",
        "    def path_from(self, starting_point):\n",
        "        shape = self.environment.get_size()\n",
        "        max_steps = shape[0] * shape[1]\n",
        "        step_count = 0\n",
        "        S = starting_point\n",
        "        self.environment.setAgentPos(S[0], S[1])\n",
        "        tuples = []\n",
        "        while (not self.environment.in_terminal_state()) and (step_count < max_steps):\n",
        "            A = self.get_greedy_action(S)\n",
        "            R = self.environment.move(self.agent, A)\n",
        "            tuples.append((S,A,R))\n",
        "            S = (self.agent.y, self.agent.x)\n",
        "            step_count+=1\n",
        "        return tuples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Environment:\n",
        "    default_symbols = {\"agent\": '@', \"wall\": '#', \"path\": '.', \"goal\":'$'}\n",
        "    def __init__(self, path, display=True, starting_point = (1,1)) -> None:\n",
        "        self.display = display\n",
        "        self.original_map = self.load_map(path)\n",
        "        self.map = self.copy_map(self.original_map)\n",
        "        self.wait_time = 0\n",
        "        self.starting_point = starting_point\n",
        "\n",
        "        if self.display:\n",
        "            self.render = Renderer(self, self.map, \"Ambiente\")\n",
        "            print(self.render)\n",
        "\n",
        "    def copy_map(self, map):\n",
        "        map_copy = []\n",
        "        for row in map:\n",
        "            map_copy.append([])\n",
        "            for cell in row:\n",
        "                map_copy[-1].append(cell)\n",
        "        return map_copy\n",
        "\n",
        "    def getAgent(self) -> Agent:\n",
        "        return self.agent\n",
        "\n",
        "    def in_terminal_state(self):\n",
        "        return self.original_map[self.agent.y][self.agent.x] == self.default_symbols[\"goal\"]\n",
        "\n",
        "\n",
        "    def load_map(self, path):\n",
        "        \"\"\"\n",
        "        Dado o caminho path, le um file txt e retorna uma matriz\n",
        "        O txt consiste de uma row contendo o numero n (n√∫mero de\n",
        "        rows do grid) e m (n√∫mero de caracteres diferentes no grid),\n",
        "        seguido de m rows explicando o que sao os caracteres no \n",
        "        file e por fim n rows contendo o grid que sao caracteres\n",
        "        \"\"\"\n",
        "\n",
        "        grid = []\n",
        "        self.symbols = dict()\n",
        "        self.rewards = {\"agent\": 0, \"wall\": 0, \"path\": 0, \"goal\":0}\n",
        "        with open(path, 'r') as file:\n",
        "            m, n = map(int, file.readline().split())\n",
        "            for _ in range(m):\n",
        "                row = file.readline().split()\n",
        "                self.symbols[row[0]] = row[1]\n",
        "                self.rewards[row[1]] = int(row[2])\n",
        "            for i in range(n):\n",
        "                row = file.readline()\n",
        "                grid.append([])\n",
        "                for j in range(len(row)):\n",
        "                    char = row[j]\n",
        "                    if char == '\\n':\n",
        "                        continue\n",
        "                    if self.symbols[char] == 'agent':\n",
        "                        self.agent = Agent(x=j, y=i, environment=self, display=self.display)\n",
        "                        char = self.default_symbols[\"path\"]\n",
        "                    grid[-1].append(self.default_symbols[self.symbols[char]])\n",
        "        return grid\n",
        "    \n",
        "    def move(self, agent, action):\n",
        "        \"\"\"\n",
        "        Dada uma action, move o agente no map\n",
        "        a action pode ser \"up\", \"down\", \"left\" ou \"right\"\n",
        "        \"\"\"\n",
        "        #time.sleep(self.wait_time)\n",
        "        direction = {\"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)}\n",
        "        final_pos = (agent.y+direction[action][0], agent.x+direction[action][1])\n",
        "        if self.map[final_pos[0]][final_pos[1]] != self.default_symbols[\"wall\"]:\n",
        "            # seta a posicao atual como caminho\n",
        "            self.map[agent.y][agent.x] = self.original_map[agent.y][agent.x]\n",
        "            \n",
        "            # seta a posicao final como o agente\n",
        "            self.map[final_pos[0]][final_pos[1]] = self.default_symbols[\"agent\"]\n",
        "            \n",
        "            # atualiza a posicao do agente\n",
        "            agent.setPos(final_pos)\n",
        "        # retorna o reforco da posicao final \n",
        "        return self.rewards[self.symbols[self.original_map[agent.y][agent.x]]]\n",
        "\n",
        "    def util(self, pos, action):\n",
        "        \"\"\"\n",
        "        Dada uma posicao e uma action, retorna o que tem na posicao destino\n",
        "        \"\"\"\n",
        "        direction = {\"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)}\n",
        "        final_pos = (pos[0]+direction[action][0], pos[1]+direction[action][1])\n",
        "        return self.symbols[self.map[final_pos[0]][final_pos[1]]] != \"wall\"\n",
        "\n",
        "    def get_size(self):\n",
        "        return len(self.map), len(self.map[0])\n",
        "    \n",
        "    def setAgentPos(self, i, j):\n",
        "        self.map[self.agent.y][self.agent.x] = self.original_map[self.agent.y][self.agent.x]\n",
        "        self.agent.setPos((i, j))\n",
        "        self.map[i][j] = self.default_symbols[\"agent\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<__main__.Renderer object at 0x0000026D38FD2B90>\n",
            "ep=0\n",
            "Tamanho do epis√≥dio: 24\n",
            "Recompensa -24\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚¨áÔ∏è‚¨õ‚¨õ‚öΩüß±\n",
            "üß±‚û°Ô∏è‚¨áÔ∏è‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=1000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=2000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=3000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=4000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=5000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=6000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=7000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=8000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=9000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏è‚¨ÜÔ∏èüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "Done\n",
            "0.20281314849853516\n"
          ]
        }
      ],
      "source": [
        "learning_strategy = MonteCarlo()\n",
        "environment = Environment('./salas/sala2.txt', True)\n",
        "agent = environment.getAgent()\n",
        "learning_strategy.setup(environment, agent)\n",
        "\n",
        "learning_strategy.train(10000, exploration_chance=0.5)\n",
        "agent.render.show()\n",
        "print(\"Done\")\n",
        "print(sum(learning_strategy.time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "Renderer.create_heatmap(learning_strategy.agent.book_V, cmap='inferno', title='Sample Heatmap')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
