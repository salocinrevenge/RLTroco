{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pygame\n",
        "from threading import Thread\n",
        "import time\n",
        "\n",
        "class Renderer():\n",
        "    def __init__(self, chefe, conteudo, titulo=None, dimensoes=(800, 800), visual = True):\n",
        "        self.visual = visual\n",
        "        self.chefe = chefe\n",
        "        self.conteudo = conteudo\n",
        "        self.conteudos = [conteudo] # para trocar entre janelas\n",
        "        self.iConteudoAtual = 0 # para marcar qual o atual dentre os varios\n",
        "        self.titulo = titulo\n",
        "        self.dimensoes = dimensoes\n",
        "        self.running = True\n",
        "        if not self.titulo:\n",
        "            self.titulo = type(chefe).__name__   # titulo é o nome da classe\n",
        "        \n",
        "        self.tamanhosprite = 64\n",
        "        self.escala = (self.dimensoes[0]/len(self.conteudo[0]), self.dimensoes[1]/len(self.conteudo))\n",
        "        while self.escala[0] < self.tamanhosprite//8 or self.escala[1] < self.tamanhosprite//8: # redimensiona pra pp\n",
        "            self.dimensoes =(int(self.dimensoes[0] *1.1), int(self.dimensoes[1]*1.1))\n",
        "            self.escala = (self.dimensoes[0]/len(self.conteudo[0]), self.dimensoes[1]/len(self.conteudo))\n",
        "\n",
        "        if visual:\n",
        "            self.carregarSprites()\n",
        "\n",
        "        # cria uma thread que roda o pygame\n",
        "        mostrador = Thread(target=self.mostrar)\n",
        "\n",
        "        # Inicia a thread\n",
        "        mostrador.start()\n",
        "\n",
        "    def addConteudo(self,conteudo):\n",
        "        self.conteudos.append(conteudo)\n",
        "\n",
        "    def desligar(self):\n",
        "        self.running = False\n",
        "\n",
        "    def carregarSprites(self):\n",
        "        self.sprites = dict()\n",
        "        self.sprites[\"path\"] = pygame.transform.scale(pygame.image.load(\"imgs/path.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"wall\"] = pygame.transform.scale(pygame.image.load(\"imgs/wall.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"goal\"] = pygame.transform.scale(pygame.image.load(\"imgs/goal.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"agent\"] = pygame.transform.scale(pygame.image.load(\"imgs/agent.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"right\"] = pygame.transform.scale(pygame.image.load(\"imgs/right.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"up\"] = pygame.transform.scale(pygame.image.load(\"imgs/up.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"left\"] = pygame.transform.scale(pygame.image.load(\"imgs/left.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"down\"] = pygame.transform.scale(pygame.image.load(\"imgs/down.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "\n",
        "\n",
        "    def caracterParaSprite(self, caracter):\n",
        "        match caracter:\n",
        "            case \"wall\":\n",
        "                return \"#\"\n",
        "            case \"right\":\n",
        "                return \"→\"\n",
        "            case \"up\":\n",
        "                return \"↑\"\n",
        "            case \"left\":\n",
        "                return \"←\"\n",
        "            case \"down\":\n",
        "                return \"↓\"\n",
        "            case _:\n",
        "                return \" \"\n",
        "\n",
        "    def mostrar(self):\n",
        "        # renderiza o ambiente\n",
        "        if self.visual:\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode(self.dimensoes)\n",
        "            pygame.display.set_caption(self.titulo)\n",
        "            self.screen.fill((0, 0, 0))\n",
        "\n",
        "        if self.visual:\n",
        "            while self.running:\n",
        "                pygame.time.delay(10)  # delay de 10ms\n",
        "                # Botao de fechar\n",
        "                for event in pygame.event.get():\n",
        "                    if event.type == pygame.QUIT:\n",
        "                        pygame.quit()\n",
        "                        self.running = False\n",
        "                    # se apertar \"p\"\n",
        "                    if event.type == pygame.KEYDOWN:\n",
        "                        if event.key == pygame.K_p:\n",
        "                            self.iConteudoAtual = 1\n",
        "                            self.conteudo = self.conteudos[self.iConteudoAtual]\n",
        "\n",
        "                    if event.type == pygame.KEYUP:\n",
        "                        if event.key == pygame.K_p:\n",
        "                            self.iConteudoAtual = 0\n",
        "                            self.conteudo = self.conteudos[self.iConteudoAtual]\n",
        "\n",
        "                # limpa a tela\n",
        "                self.screen.fill((0,0,0))\n",
        "                \n",
        "                # desenha o conteudo\n",
        "                for i in range(len(self.conteudo)):\n",
        "                    for j in range(len(self.conteudo[0])):\n",
        "                        celula = self.conteudo[i][j]\n",
        "                        # se o conteudo de celula estiver no dicionario de sprites\n",
        "                        if celula in self.sprites:\n",
        "                            objeto = celula\n",
        "                        else:\n",
        "                            objeto = self.chefe.simbolos[celula]\n",
        "                        self.screen.blit(self.sprites[objeto], (j*self.escala[0], i*self.escala[1]))\n",
        "\n",
        "                # Atualizar a tela\n",
        "                pygame.display.update()\n",
        "        else:\n",
        "            for linha in self.conteudo:\n",
        "                for celula in linha:\n",
        "                    print(self.caracterParaSprite(celula), end=\"\")\n",
        "                print()\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class Agent():\n",
        "    acoes = ['up', 'down', 'left', 'right']\n",
        "    def __init__(self, gamma = 0.9) -> None:\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def iniciaQ(self, formato):\n",
        "        \"\"\"\n",
        "        livroQ é uma lista de listas de dicionarios,\n",
        "        ele armazena \n",
        "        \n",
        "        \"\"\"\n",
        "        self.livro_Q = []\n",
        "        for i in range(formato[0]):\n",
        "            self.livro_Q.append([])\n",
        "            for _ in range(formato[1]):\n",
        "                self.livro_Q[i].append(dict())\n",
        "                for acao in self.acoes:\n",
        "                    self.livro_Q[i][-1][acao] = float(\"-inf\")\n",
        "    \n",
        "    def iniciaPolicy(self, formato, politicaAleatoria):\n",
        "        \"\"\"\n",
        "        A policy é uma matriz de caracteres que guarda a acao principal\n",
        "        a ser tomada ate o momento\n",
        "        \n",
        "        \"\"\"\n",
        "        self.policy = []\n",
        "        for i in range(formato[0]):\n",
        "            self.policy.append([])\n",
        "            for j in range(formato[1]):\n",
        "                if self.environment.simbolos[self.environment.mapaOriginal[i][j]] == \"wall\":\n",
        "                    self.policy[i].append(\"wall\")\n",
        "                    continue\n",
        "                if politicaAleatoria:\n",
        "                    self.policy[i].append(random.choice(self.acoes))\n",
        "                else:\n",
        "                    self.policy[i].append(self.acoes[0])\n",
        "        # self.render = self.environment.render.addConteudo(self.policy)\n",
        "\n",
        "    def iniciaRetorno(self, formato):\n",
        "        \"\"\"\n",
        "        returns é uma colecao de pares estado acao guardando um\n",
        "        dicionario para armazenar o valor maximo de reforcos obtidos,\n",
        "        o numero de vezes que o par estado acao foi visitado e o ultimo\n",
        "        episodio em que o par estado acao foi visitado\n",
        "        \"\"\"\n",
        "        self.returns = []\n",
        "        for i in range(formato[0]):\n",
        "            self.returns.append([])\n",
        "            for j in range(formato[1]):\n",
        "                self.returns[i].append(dict())\n",
        "                for acao in self.acoes:\n",
        "                    self.returns[i][j][acao] = {\"value\": 0, \"count\": 0, \"lastEpisode\": 0}\n",
        "\n",
        "    def setEnvironment(self, environment):\n",
        "        self.environment = environment\n",
        "    \n",
        "    def setPos(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def mover(self, acao):\n",
        "        return self.environment.mover(self, acao)\n",
        "    \n",
        "    def get_action(self):\n",
        "        return self.policy[self.y][self.x]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "class Environment:\n",
        "    simbolosPadrao = {\"agent\": '@', \"wall\": '#', \"path\": '.', \"goal\":'$'}\n",
        "    def __init__(self, path = None) -> None:\n",
        "        self.agentPos = []\n",
        "        if path:\n",
        "            self.mapaOriginal = self.carregarMapa(path)\n",
        "        self.mapa = self.copiarMapa(self.mapaOriginal)\n",
        "        # self.render = Renderer(self, self.mapa, \"Ambiente\", visual = False)\n",
        "        self.tempoEspera = 0\n",
        "\n",
        "    def copiarMapa(self, mapa):\n",
        "        mapaCopia = []\n",
        "        for linha in mapa:\n",
        "            mapaCopia.append([])\n",
        "            for celula in linha:\n",
        "                mapaCopia[-1].append(celula)\n",
        "        return mapaCopia\n",
        "\n",
        "    def setAgent(self, agent) -> Agent:\n",
        "        self.agent = agent\n",
        "\n",
        "    def in_terminal_state(self):\n",
        "        return self.mapaOriginal[self.agent.y][self.agent.x] == self.simbolosPadrao[\"goal\"]\n",
        "\n",
        "\n",
        "    def carregarMapa(self, path):\n",
        "        \"\"\"\n",
        "        Dado o caminho path, le um arquivo txt e retorna uma matriz\n",
        "        O txt consiste de uma linha contendo o numero n (número de\n",
        "        linhas do mapa) e m (número de caracteres diferentes no mapa),\n",
        "        seguido de m linhas explicando o que sao os caracteres no \n",
        "        arquivo e por fim n linhas contendo o mapa que sao caracteres\n",
        "        \"\"\"\n",
        "        mapa = []\n",
        "        self.simbolos = dict()\n",
        "        self.reforcos = {\"agent\": 0, \"wall\": 0, \"path\": 0, \"goal\":0}\n",
        "        with open(path, 'r') as arquivo:\n",
        "            m, n = map(int, arquivo.readline().split())\n",
        "            for _ in range(m):\n",
        "                linha = arquivo.readline().split()\n",
        "                self.simbolos[linha[0]] = linha[1]\n",
        "                self.reforcos[linha[1]] = int(linha[2])\n",
        "            for i in range(n):\n",
        "                linha = arquivo.readline()\n",
        "                mapa.append([])\n",
        "                for j in range(len(linha)):\n",
        "                    char = linha[j]\n",
        "                    if char == '\\n':\n",
        "                        continue\n",
        "                    if self.simbolos[char] == 'agent':\n",
        "                        self.agentPos.append((i, j))\n",
        "                        char = self.simbolosPadrao[\"path\"]\n",
        "                    mapa[-1].append(self.simbolosPadrao[self.simbolos[char]])\n",
        "        return mapa\n",
        "    \n",
        "    def mover(self, agent, acao):\n",
        "        \"\"\"\n",
        "        Dada uma acao, move o agente no mapa\n",
        "        a acao pode ser \"up\", \"down\", \"left\" ou \"right\"\n",
        "        \"\"\"\n",
        "        time.sleep(self.tempoEspera)\n",
        "        direcao = {\"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)}\n",
        "        posicaofinal = (agent.y+direcao[acao][0], agent.x+direcao[acao][1])\n",
        "        if self.mapa[posicaofinal[0]][posicaofinal[1]] != self.simbolosPadrao[\"wall\"]:\n",
        "            # seta a posicao atual como caminho\n",
        "            self.mapa[agent.y][agent.x] = self.mapaOriginal[agent.y][agent.x]\n",
        "            \n",
        "            # seta a posicao final como o agente\n",
        "            self.mapa[posicaofinal[0]][posicaofinal[1]] = self.simbolosPadrao[\"agent\"]\n",
        "            \n",
        "            # atualiza a posicao do agente\n",
        "            agent.setPos(x = posicaofinal[1], y = posicaofinal[0])\n",
        "        # retorna o reforco da posicao final \n",
        "        return self.reforcos[self.simbolos[self.mapaOriginal[agent.y][agent.x]]]\n",
        "\n",
        "    def util(self, pos, acao):\n",
        "        \"\"\"\n",
        "        Dada uma posicao e uma acao, retorna o que tem na posicao destino\n",
        "        \"\"\"\n",
        "        direcao = {\"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)}\n",
        "        posicaofinal = (pos[0]+direcao[acao][0], pos[1]+direcao[acao][1])\n",
        "        return self.simbolos[self.mapa[posicaofinal[0]][posicaofinal[1]]] != \"wall\"\n",
        "\n",
        "    def get_size(self):\n",
        "        return len(self.mapa), len(self.mapa[0])\n",
        "    \n",
        "    def setAgentPos(self, i, j):\n",
        "        if self.agent.y != None and self.agent.x != None:\n",
        "            self.mapa[self.agent.y][self.agent.x] = self.mapaOriginal[self.agent.y][self.agent.x]\n",
        "        self.agent.setPos(j, i)\n",
        "        self.mapa[i][j] = self.simbolosPadrao[\"agent\"]\n",
        "\n",
        "    def get_metrics(self):\n",
        "        pontuacao = 0\n",
        "        for pos in self.agentPos:\n",
        "            pontuacao += max(self.agent.livro_Q[pos[0]][pos[1]].values())\n",
        "        return pontuacao/len(self.agentPos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LearningStrategy():\n",
        "    def train(self, episodes):\n",
        "        pass\n",
        "\n",
        "    def setup(self, environment, agent):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearFunctionApproximation():\n",
        "    def __init__(self) -> None:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "class MonteCarlo(LearningStrategy):\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def setup(self, environment, agent):\n",
        "        self.environment = environment\n",
        "        self.agent = agent\n",
        "    \n",
        "    def train(self, episodes, politicaAleatoria = True, chanceExploracao = 0):\n",
        "        # Initialize\n",
        "        formato = self.environment.get_size()\n",
        "        self.agent.iniciaPolicy(formato, politicaAleatoria)\n",
        "        Renderer(self, self.agent.policy, \"Politica\", visual = False)\n",
        "        self.agent.iniciaQ(formato)\n",
        "        self.agent.iniciaRetorno(formato)\n",
        "        self.metricas = []\n",
        "        for ep in range(1,episodes+1):\n",
        "            if ep % (episodes//10) == 0:\n",
        "                print(f\"Episodio {ep} de {episodes} \", end = \"\\t\")\n",
        "\n",
        "            # escolhe posicao aleatoria valida para o agente\n",
        "            while True:\n",
        "                estado = (random.randrange(0, formato[0]), random.randrange(0, formato[1]))\n",
        "                if self.environment.mapaOriginal[estado[0]][estado[1]] in {self.environment.simbolosPadrao[\"path\"], self.environment.simbolosPadrao[\"goal\"]}:\n",
        "                    break\n",
        "            # escolhe uma acao diferente da dita pela politica atual\n",
        "            for _ in range(len(self.agent.acoes)*2):    # limite maximo de tentativas\n",
        "                acao = random.choice(self.agent.acoes)\n",
        "                if acao != self.agent.policy[estado[0]][estado[1]]:\n",
        "                    # se a acao nao te leva para uma parede\n",
        "                    if self.environment.util(estado, acao):\n",
        "                        break\n",
        "            else:\n",
        "                acao = self.agent.policy[estado[0]][estado[1]]\n",
        "            self.episode(estado, acao, max_steps= formato[1]*formato[0], chanceExploracao = chanceExploracao)\n",
        "            g = 0\n",
        "            for t in range(len(self.agent.lembrancas)-1, -1, -1): \n",
        "                memoria = self.agent.lembrancas[t]  # memoria = (estado, acao, reforco)\n",
        "                g = self.agent.gamma*g + memoria[2]\n",
        "                # verifica se o par estado acao ja foi inserido em returns\n",
        "                if self.agent.returns[memoria[0][0]][memoria[0][1]][memoria[1]][\"lastEpisode\"] != ep:\n",
        "                    self.agent.returns[memoria[0][0]][memoria[0][1]][memoria[1]][\"lastEpisode\"] = ep\n",
        "                    self.agent.returns[memoria[0][0]][memoria[0][1]][memoria[1]][\"value\"] += g\n",
        "                    self.agent.returns[memoria[0][0]][memoria[0][1]][memoria[1]][\"count\"] += 1\n",
        "                    media = self.agent.returns[memoria[0][0]][memoria[0][1]][memoria[1]][\"value\"]/self.agent.returns[memoria[0][0]][memoria[0][1]][memoria[1]][\"count\"]\n",
        "                    self.agent.livro_Q[memoria[0][0]][memoria[0][1]][memoria[1]] = media\n",
        "                    self.agent.policy[memoria[0][0]][memoria[0][1]] = max(self.agent.acoes, key = lambda acao: self.agent.livro_Q[memoria[0][0]][memoria[0][1]][acao])    # recebe a acao que maximiza o valor de Q\n",
        "            self.metricas.append(self.environment.get_metrics())\n",
        "            if ep % (episodes//10) == 0:\n",
        "                print(f\"Pontuacao total nos pontos destacados: {self.metricas[-1]}\")\n",
        "\n",
        "    def episode(self, estado, acao, max_steps, chanceExploracao = 0):\n",
        "        step_count = 0\n",
        "        self.agent.lembrancas = []\n",
        "        self.environment.setAgentPos(estado[0], estado[1])\n",
        "        while step_count < max_steps:  # enquanto nao estiver em um estado terminal\n",
        "            step_count +=1  # incrementa o numero de passos\n",
        "            posAnterior = (self.agent.y, self.agent.x)\n",
        "            reward = self.environment.mover(self.agent,acao) # realiza a acao e recebe a recompensa\n",
        "            self.agent.lembrancas.append((posAnterior, acao, reward)) # guarda o passo\n",
        "            if random.randint(0, 100)< (1-chanceExploracao)*100:\n",
        "                acao = self.agent.get_action() # escolhe uma acao de acordo com a politica\n",
        "            else:\n",
        "                for _ in range(len(self.agent.acoes)*2):    # limite maximo de tentativas\n",
        "                    acao = random.choice(self.agent.acoes)\n",
        "                    # se a acao nao te leva para uma parede\n",
        "                    if self.environment.util(estado, acao):\n",
        "                        break\n",
        "            if self.environment.in_terminal_state():\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QLearning():\n",
        "    def __init__(self) -> None:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SARSA():\n",
        "    def __init__(self) -> None:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Professor():\n",
        "    learningStrategys = {\"Monte Carlo\": MonteCarlo(), \"Q-Learning\": QLearning(), \"SARSA\": SARSA(), \"Linear Function Approximation\": LinearFunctionApproximation()}\n",
        "    \n",
        "    def __init__(self, caminhoSala = \"sala\", learningStrategy = None, nEpisodios = 1000, chanceExploracao = 0):\n",
        "        self.learningStrategy = self.learningStrategys[learningStrategy]\n",
        "        self.env = Environment(path = caminhoSala)\n",
        "        self.aluno = Agent(gamma = 0.9)\n",
        "        self.env.setAgent(self.aluno)\n",
        "        self.aluno.setEnvironment(self.env)\n",
        "        self.learningStrategy.setup(self.env, self.aluno)\n",
        "        self.chanceExploracao = chanceExploracao\n",
        "        self.ensinar(nEpisodios)\n",
        "\n",
        "    def ensinar(self, nEpisodios):\n",
        "        self.learningStrategy.train(nEpisodios, chanceExploracao = self.chanceExploracao)\n",
        "        Renderer(self, self.aluno.policy, \"Politica\", visual = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "##########\n",
            "#←→→↓↓↑#↓#\n",
            "#↑####←#←#\n",
            "#←↑↑↑#→↑→#\n",
            "####→#####\n",
            "#←←↓→←→→←#\n",
            "#↑↓###↓#←#\n",
            "#↑←#←→↑#↑#\n",
            "#→↑↑→#↓←←#\n",
            "##########\n",
            "Episodio 10000 de 100000 \tPontuacao total nos pontos destacados: 12.644875610974536\n",
            "Episodio 20000 de 100000 \tPontuacao total nos pontos destacados: 12.72440182021445\n",
            "Episodio 30000 de 100000 \tPontuacao total nos pontos destacados: 12.803759914497409\n",
            "Episodio 40000 de 100000 \tPontuacao total nos pontos destacados: 12.882370095207287\n",
            "Episodio 50000 de 100000 \tPontuacao total nos pontos destacados: 12.91912595496232\n",
            "Episodio 60000 de 100000 \tPontuacao total nos pontos destacados: 12.952370193690857\n",
            "Episodio 70000 de 100000 \tPontuacao total nos pontos destacados: 12.983027433294442\n",
            "Episodio 80000 de 100000 \tPontuacao total nos pontos destacados: 13.021535001683677\n",
            "Episodio 90000 de 100000 \tPontuacao total nos pontos destacados: 13.05089296570234\n",
            "Episodio 100000 de 100000 \tPontuacao total nos pontos destacados: 13.056423673776717\n",
            "##########\n",
            "#→→→→→↓#↓#\n",
            "#↑####↓#↑#\n",
            "#↑←←←#→→↑#\n",
            "####↑#####\n",
            "#→→→↑←←←←#\n",
            "#↑↑###↑#↑#\n",
            "#→↑#↑→↑#↑#\n",
            "#↑↑←↓#↓←↑#\n",
            "##########\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    sala = input(\"Digite o numero da sala: \")\n",
        "    professor = Professor(caminhoSala=f\"sala{sala}.txt\", learningStrategy=\"Monte Carlo\", nEpisodios=100000, chanceExploracao = 0.3)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
