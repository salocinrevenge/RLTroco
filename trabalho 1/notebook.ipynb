{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Projeto 1\n",
        "\n",
        "Este trabalho compara a efici√™ncia e efic√°cia de m√∫ltiplos algoritmos de Aprendizado por Refor√ßo (RL), sendo eles MonteCarlo, Sarsa(Lambda) e Q_learning, em diferentes cen√°rios e variando seus par√¢metros quando necess√°rio. O ambiente/problema que foi usado como teste s√£o labirintos, onde os algoritmos precisam encontrar uma politica √≥tima para que o agente consiga chegar at√© o final do labirinto. O algoritmo DQN (Deep Q Networking) tamb√©m foi implementado, por√©m utilizando outro ambiente, an√°logo ao nosso. Importante citar que o que chamamos de pol√≠tica √≥tima neste trabalho se refere a capacidade dessa pol√≠tica de levar o agente at√© o objetivo, na menor quantidade de passos.\n",
        "\n",
        "\n",
        "### Ambiente\n",
        "Como dito anteriormente, os ambientes s√£o labirintos. Eles s√£o descritos por meio de arquivos .txt, onde s√£o apontados as paredes, caminhos, estados terminais, formato e suas recompensas. Os arquivos t√™m essa cara:\n",
        "```\n",
        "4 4\n",
        ". path -1\n",
        "@ agent -1\n",
        "$ goal 100\n",
        "# wall -1000\n",
        "######\n",
        "#...$#\n",
        "#@...#\n",
        "######\n",
        "```\n",
        "\n",
        "A primeira linha cont√©m dois inteiros E e L, que representam o n√∫mero de elementos e o n√∫mero de linhas do labirinto. Em seguida s√£o descritos os s√≠mbolos que comp√µem os elementos. Os caminhos s√£o representados pelo caractere '.' e possuem recompensa de -1. O agente √© representado pelo ‚Äò@‚Äô. Por sua vez, o estado terminal, ou objetivo, √© representado por '$' e possui recompensa de 100.Por fim, as paredes s√£o representadas por ‚Äò#‚Äô. Ap√≥s a descri√ß√£o dos s√≠mbolos, h√° a descri√ß√£o do pr√≥prio formato do labirinto.\n",
        "\n",
        "Diferentes formatos de labirintos foram adotados para o trabalho, sendo alguns mais amplos, outros com diversos caminhos at√© o objetivo, e outros com caminhos extremamente estreitos.\n",
        "\n",
        "### C√≥digo\n",
        "As primeiras c√©lulas desse notebook cont√©m o c√≥digo usado para implementa√ß√£o e obten√ß√£o dos resultados. Por√©m, para facilitar, foram geradas imagens com os resultados obtidos (usando [pygame](https://www.pygame.org/)), e a an√°lise foi feita em cima delas. O c√≥digo est√° dispon√≠vel para execu√ß√£o, altera√ß√£o, e teste de outras configura√ß√µes de labirinto.\n",
        "\n",
        "### M√©tricas\n",
        "Para compara√ß√£o dos algoritmos entre si, foram extra√≠das as recompensas m√©dias e o tamanho do caminho percorrido a partir de um mesmo ponto do mapa definido na cria√ß√£o do mapa pelo valor 'agent', neste caso o '@'. Com isso √© poss√≠vel analisar o qu√£o eficiente foi o caminho encontrado pelo algoritmo, caso encontrado, e se ele conseguiu convergir. H√° tamb√©m a an√°lise de desempenho e custo computacional, o <b style=\"color:#770000\">n√∫mero m√©dio de epis√≥dios para converg√™ncia</b>, o <b style=\"color:#770000\">tempo de converg√™ncia</b>, os <b style=\"color:#770000\">hiperparametros</b> necess√°rios para ajustar o algoritmo, e o qu√£o <b style=\"color:#770000\">dif√≠cil</b> foi encontrar um valor √≥timo.\n",
        "\n",
        "Al√©m disso, um mapa de calor gerado a partir da fun√ß√£o valor ser√° usado para ilustrar o qu√£o bom √© estar em cada estado, ou seja, em cada posi√ß√£o do labirinto. Tamb√©m ser√° mostrado um gr√°fico contendo as recompensas do modelo ao longo do tempo, para que seja poss√≠vel analisar o qu√£o r√°pido o algoritmo convergiu. Esse gr√°fico utiliza o recurso da m√©dia m√≥vel para suavizar os dados e facilitar a visualiza√ß√£o e compreens√£o.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# C√≥digos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lftFAb-BqMew"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from itertools import product\n",
        "import random\n",
        "from threading import Thread\n",
        "import time\n",
        "from argparse import ArgumentParser\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import pygame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OwSgimmVxFIw"
      },
      "outputs": [],
      "source": [
        "class Renderer():\n",
        "    def __init__(self, chief, content, title=None, dimensions=(800, 800)):\n",
        "        self.chief = chief\n",
        "        self.content = content\n",
        "        self.contents = [content] # para trocar entre janelas\n",
        "        self.iConteudoAtual = 0 # para marcar qual o atual dentre os varios\n",
        "        self.title = title\n",
        "        self.dimensions = dimensions\n",
        "        self.running = True\n",
        "        if not self.title:\n",
        "            self.title = type(chief).__name__   # title √© o nome da classe\n",
        "        \n",
        "        \n",
        "        # redimensiona o ambiente para caber na tela\n",
        "        self.tamanhosprite = 64\n",
        "        self.escala = (self.dimensions[0]/len(self.content[0]), self.dimensions[1]/len(self.content))\n",
        "        while self.escala[0] < self.tamanhosprite//8 or self.escala[1] < self.tamanhosprite//8: # redimensiona pra pp\n",
        "            self.dimensions =(int(self.dimensions[0] *1.1), int(self.dimensions[1]*1.1))\n",
        "            self.escala = (self.dimensions[0]/len(self.content[0]), self.dimensions[1]/len(self.content))\n",
        "\n",
        "        self.load_sprites()\n",
        "\n",
        "        # cria uma thread que roda o pygame\n",
        "        displayer = Thread(target=self.show) # shower = mostrador\n",
        "\n",
        "        # Inicia a thread\n",
        "        displayer.start()\n",
        "\n",
        "    def addConteudo(self,conteudo):\n",
        "        self.contents.append(conteudo)\n",
        "\n",
        "    def desligar(self):\n",
        "        self.running = False\n",
        "\n",
        "    def load_sprites(self):\n",
        "        self.sprites = dict()\n",
        "        self.sprites[\"path\"] = pygame.transform.scale(pygame.image.load(\"imgs/path.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"wall\"] = pygame.transform.scale(pygame.image.load(\"imgs/wall.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"goal\"] = pygame.transform.scale(pygame.image.load(\"imgs/goal.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"cookie\"] = pygame.transform.scale(pygame.image.load(\"imgs/cookie.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"agent\"] = pygame.transform.scale(pygame.image.load(\"imgs/agent.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"right\"] = pygame.transform.scale(pygame.image.load(\"imgs/right.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"up\"] = pygame.transform.scale(pygame.image.load(\"imgs/up.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"left\"] = pygame.transform.scale(pygame.image.load(\"imgs/left.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"down\"] = pygame.transform.scale(pygame.image.load(\"imgs/down.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"acid\"] = pygame.transform.scale(pygame.image.load(\"imgs/acid.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"lava\"] = pygame.transform.scale(pygame.image.load(\"imgs/lava.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "\n",
        "\n",
        "        self.asciiSprites = dict()\n",
        "        self.asciiSprites[\"path\"]   = '‚¨õ'\n",
        "        self.asciiSprites[\"wall\"]   = 'üß±'\n",
        "        self.asciiSprites[\"goal\"]   = '‚öΩ'\n",
        "        self.asciiSprites[\"cookie\"]   = 'üç™'\n",
        "        self.asciiSprites[\"agent\"]  = 'üëæ' \n",
        "        self.asciiSprites[\"right\"]  = '->' #'‚û°Ô∏è'\n",
        "        self.asciiSprites[\"up\"]     = '‚¨ÜÔ∏è‚¨ÜÔ∏è'\n",
        "        self.asciiSprites[\"left\"]   = '<-' \n",
        "        self.asciiSprites[\"down\"]   = '‚¨áÔ∏è‚¨áÔ∏è'\n",
        "        self.asciiSprites[\"lava\"]   = 'üåã'\n",
        "        self.asciiSprites[\"acid\"]   = 'ü¶†'\n",
        "\n",
        "    def create_heatmap(data, cmap='viridis', title='Heatmap'):\n",
        "        \"\"\"\n",
        "        Create a heatmap from a list of lists of floats.\n",
        "\n",
        "        Parameters:\n",
        "        - data: List of lists of floats representing the heatmap data.\n",
        "        - cmap: Colormap for the heatmap (default is 'viridis').\n",
        "        - title: Title for the heatmap (default is 'Heatmap').\n",
        "        \"\"\"\n",
        "        data = np.array(data, dtype=float)\n",
        "\n",
        "        # Create a figure and axis\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        # Display the heatmap using imshow\n",
        "        im = ax.imshow(data, cmap=cmap)\n",
        "\n",
        "        # Add a colorbar to the right of the heatmap\n",
        "        cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "        # Set the title\n",
        "        ax.set_title(title)\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n",
        "\n",
        "    def showAscii(self):\n",
        "        for i in range(len(self.content)):\n",
        "            for j in range(len(self.content[0])):\n",
        "                cell = self.content[i][j]\n",
        "                # se o content de cell estiver no dicionario de sprites\n",
        "                if cell in self.asciiSprites:\n",
        "                    obj = cell\n",
        "                else:\n",
        "                    obj = self.chief.symbols[cell]\n",
        "                print(self.asciiSprites.get(obj,'‚ùå'),end='')\n",
        "            print('')\n",
        "\n",
        "    def show(self):\n",
        "        # renderiza o ambiente\n",
        "        pygame.init()\n",
        "        self.screen = pygame.display.set_mode(self.dimensions)\n",
        "        pygame.display.set_caption(self.title)\n",
        "        self.screen.fill((0, 0, 0))\n",
        "\n",
        "        try:\n",
        "            while self.running:\n",
        "                pygame.time.delay(10)  # delay de 10ms\n",
        "                # Botao de fechar\n",
        "                for event in pygame.event.get():\n",
        "                    if event.type == pygame.QUIT:\n",
        "                        pygame.quit()\n",
        "                        self.running = False\n",
        "                    # se apertar \"p\"\n",
        "                    if event.type == pygame.KEYDOWN:\n",
        "                        if event.key == pygame.K_p:\n",
        "                            self.iConteudoAtual = 1\n",
        "                            self.content = self.contents[self.iConteudoAtual]\n",
        "\n",
        "                    if event.type == pygame.KEYUP:\n",
        "                        if event.key == pygame.K_p:\n",
        "                            self.iConteudoAtual = 0\n",
        "                            self.content = self.contents[self.iConteudoAtual]\n",
        "\n",
        "                # limpa a tela\n",
        "                self.screen.fill((0,0,0))\n",
        "                \n",
        "                # desenha o conteudo\n",
        "                for k in range(self.iConteudoAtual+1):\n",
        "                    for i in range(len(self.contents[k])):\n",
        "                        for j in range(len(self.contents[k][0])):\n",
        "                            celula = self.contents[k][i][j]\n",
        "                            # se o conteudo de celula estiver no dicionario de sprites\n",
        "                            if celula in self.sprites:\n",
        "                                objeto = celula\n",
        "                            else:\n",
        "                                objeto = self.chief.symbols[celula]\n",
        "                            self.screen.blit(self.sprites[objeto], (j*self.escala[0], i*self.escala[1]))\n",
        "\n",
        "                # Atualizar a tela\n",
        "                pygame.display.update()\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "\n",
        "    def show_path(self, path):\n",
        "        for i in range(len(self.content)):\n",
        "            for j in range(len(self.content[0])):\n",
        "                cell = self.content[i][j]\n",
        "                if((i,j) in path.keys()):\n",
        "                    cell = path[(i,j)]\n",
        "                # se o content de cell estiver no dicionario de sprites\n",
        "                if cell in self.asciiSprites:\n",
        "                    obj = cell\n",
        "                else:\n",
        "                    obj = self.chief.symbols[cell]\n",
        "                print(self.asciiSprites.get(obj,'‚ùå'),end='')\n",
        "            print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    actions = ['up', 'down', 'left', 'right']\n",
        "    def __init__(self, x, y, environment, gamma = 0.9, display=True):\n",
        "        self.environment = environment\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.gamma = gamma\n",
        "        self.display = display\n",
        "\n",
        "    def action_idx(self, action: str):\n",
        "        return self.actions.index(action)\n",
        "\n",
        "    def startQ(self, shape, start_value = float(\"-inf\")):\n",
        "        \"\"\"\n",
        "        livroQ √© uma lista de listas de dicionarios,\n",
        "        ele armazena \n",
        "        \n",
        "        \"\"\"\n",
        "        self.livro_Q: list[list[dict]] = []\n",
        "        for i in range(shape[0]):\n",
        "            self.livro_Q.append([])\n",
        "            for _ in range(shape[1]):\n",
        "                self.livro_Q[i].append(dict())\n",
        "                for acao in self.actions:\n",
        "                    self.livro_Q[i][-1][acao] = start_value\n",
        "\n",
        "    def startV(self, shape):\n",
        "        \"\"\"\n",
        "        livroV √© uma lista de listas de dicionarios,\n",
        "        ele armazena \n",
        "        \n",
        "        \"\"\" \n",
        "        self.book_V: list[list] = []\n",
        "        for i in range(shape[0]):\n",
        "            self.book_V.append([])\n",
        "            for _ in range(shape[1]):\n",
        "                \n",
        "                self.book_V[i].append(float(\"-inf\"))\n",
        "    \n",
        "    def startPolicy(self, shape, randomPolicy):\n",
        "        \"\"\"\n",
        "        A policy √© uma matriz de caracteres que guarda a acao principal\n",
        "        a ser tomada ate o momento\n",
        "        \n",
        "        \"\"\"\n",
        "        self.policy: list[list[str]] = []\n",
        "        for i in range(shape[0]):\n",
        "            self.policy.append([])\n",
        "            for j in range(shape[1]):\n",
        "                if self.environment.symbols[self.environment.original_map[i][j]] == \"wall\":\n",
        "                    self.policy[i].append(\"wall\")\n",
        "                    continue\n",
        "                if randomPolicy:\n",
        "                    self.policy[i].append(random.choice(self.actions))\n",
        "                else:\n",
        "                    self.policy[i].append(self.actions[0])\n",
        "        \n",
        "        if self.display:\n",
        "            self.render = self.environment.render.addConteudo(self.policy)\n",
        "\n",
        "    def startReturns(self, shape):\n",
        "        \"\"\"\n",
        "        returns √© uma colecao de pares estado acao guardando um\n",
        "        dicionario para armazenar o valor maximo de reforcos obtidos,\n",
        "        o numero de vezes que o par estado acao foi visitado e o ultimo\n",
        "        episodio em que o par estado acao foi visitado\n",
        "        \"\"\"\n",
        "        self.returns: list[list[dict]] = []\n",
        "        for i in range(shape[0]):\n",
        "            self.returns.append([])\n",
        "            for j in range(shape[1]):\n",
        "                self.returns[i].append(dict())\n",
        "                for acao in self.actions:\n",
        "                    self.returns[i][j][acao] = {\"value\": 0, \"count\": 0, \"lastEpisode\": 0}\n",
        "\n",
        "    def setEnvironment(self, environment):\n",
        "        self.environment = environment\n",
        "    \n",
        "    def setPos(self, position):\n",
        "        self.x = position[1]\n",
        "        self.y = position[0]\n",
        "\n",
        "    def move(self, action):\n",
        "        return self.environment.move(self, action)\n",
        "    \n",
        "    def get_action(self):\n",
        "        return self.policy[self.y][self.x]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlGKK60NqTV4",
        "outputId": "d6167f45-bc5f-49b6-8732-de679f17aeec"
      },
      "outputs": [],
      "source": [
        "class LearningStrategy():\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.feature_lenght = 15\n",
        "        self.W = np.random.rand(self.feature_lenght)*2-1\n",
        "\n",
        "    def train(self, episodes):\n",
        "        pass\n",
        "\n",
        "    def get_Q(self, x, y, action, linear_approximation = False):\n",
        "        if not linear_approximation:\n",
        "            return self.Q[x,y,action]\n",
        "        \n",
        "        terms = self.data_to_features(x,y,action)\n",
        "        return np.dot(self.W, terms)\n",
        "\n",
        "    def setup(self, environment, agent):\n",
        "        self.environment: Environment = environment\n",
        "        self.agent: Agent = agent\n",
        "    \n",
        "    def data_to_features(self, x,y,action):\n",
        "        x= x/self.environment.get_size()[1]\n",
        "        y= y/self.environment.get_size()[0]\n",
        "        action = action/len(self.agent.actions)\n",
        "        return np.array([x,y,action, x*y, y*action, x*action, x*x, y*y, action*action, np.exp(x), np.exp(y), np.exp(action), np.sin(x), np.sin(y), np.sin(action)])\n",
        "        \n",
        "    def show_loss(self, rewards, window_size=None):\n",
        "        # se n√£o for numpy array, transformar em um\n",
        "        if not isinstance(rewards, np.ndarray):\n",
        "            rewards = np.asarray(rewards)\n",
        "\n",
        "        if window_size is not None:\n",
        "            # calcular a m√©dia m√≥vel\n",
        "            rewards = np.convolve(rewards, np.ones(window_size), 'valid') / window_size\n",
        "\n",
        "        # Criar o gr√°fico\n",
        "        plt.plot(rewards)\n",
        "        # Adicionar um t√≠tulo\n",
        "        plt.title(\"Gr√°fico de recompensas\")\n",
        "\n",
        "        # Adicionar um eixo x\n",
        "        plt.xlabel(\"Epis√≥dio\")\n",
        "\n",
        "        # Adicionar um eixo y\n",
        "        plt.ylabel(\"Recompensa\")\n",
        "\n",
        "        # Exibir o gr√°fico\n",
        "        plt.show()\n",
        "\n",
        "    def test(self, display = True):\n",
        "        shape = self.environment.get_size()\n",
        "        num_states = min(3000,shape[0]*shape[1])\n",
        "        success = 0\n",
        "        tries = len(self.environment.avaliations)\n",
        "        returns = []\n",
        "        steps = []\n",
        "        for avaliation in self.environment.avaliations:\n",
        "            reward = self.environment.setAgentPos(avaliation[0], avaliation[1])\n",
        "            step_count = 0\n",
        "            for _ in range(num_states):\n",
        "                step_count+=1\n",
        "                action = self.agent.policy[self.agent.y][self.agent.x]\n",
        "                R = self.environment.move(self.agent, action)\n",
        "                reward += R\n",
        "                if self.environment.in_terminal_state():\n",
        "                    success+=1\n",
        "                    break\n",
        "            steps.append(step_count)\n",
        "            returns.append(reward)\n",
        "        if display:\n",
        "            print(f\"Sucesso: {success}/{tries}: {success/tries*100}%\")\n",
        "            print(f\"Recompensa m√©dia: {np.asarray(returns).mean()}\")\n",
        "            print(f\"Steps m√©dio: {np.asarray(steps).mean()}\")\n",
        "        return {\"sucesso\": round(success/tries*100, 2), \"recompensa\": round(np.asarray(returns).mean(),2), \"steps\": round(np.asarray(steps).mean(),2)}\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "class MonteCarlo(LearningStrategy):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.episode_R = []\n",
        "        self.episode_length = []\n",
        "        self.Q = None\n",
        "        self.time = []\n",
        "    \n",
        "    def train(self, episodes, randomPolicy = True, exploration_chance = 0, appx = True, alpha = 0.003, display = True):\n",
        "        # Initialize\n",
        "        begin_training_time = time.time()\n",
        "        shape = self.environment.get_size()\n",
        "        self.agent.startPolicy(shape, randomPolicy)\n",
        "        self.agent.startReturns(shape)\n",
        "        self.Q = np.zeros((shape[0],shape[1], len(self.agent.actions)))\n",
        "        self.agent.startV(shape)\n",
        "        current_exploration_chance = exploration_chance\n",
        "        rewards = []\n",
        "        for ep in range(episodes):\n",
        "            start_time = time.time()\n",
        "            if display:\n",
        "                if ep % (episodes//10) == 0:\n",
        "                    print(f\"{ep=}\")\n",
        "                    path = self.path_from((1,1))\n",
        "                    print('Tamanho do epis√≥dio:', len(path))\n",
        "                    print('Recompensa', sum([i[2] for i in path]))\n",
        "                    path_dict = {}\n",
        "                    for s,a,r in path:\n",
        "                        path_dict[s] = a\n",
        "                    self.environment.render.show_path(path_dict)\n",
        "            # escolhe posicao aleatoria valida para o agente\n",
        "            while True:\n",
        "                state = (random.randrange(0, shape[0]), random.randrange(0, shape[1]))\n",
        "                if self.environment.original_map[state[0]][state[1]] in {self.environment.default_symbols[\"path\"], self.environment.default_symbols[\"goal\"]}:\n",
        "                    break\n",
        "            # escolhe uma acao diferente da dita pela politica atual\n",
        "            for _ in range(len(self.agent.actions)*2):    # limite maximo de tentativas\n",
        "                action = random.choice(self.agent.actions)\n",
        "                if action != self.agent.policy[state[0]][state[1]]:\n",
        "                    # se a acao nao te leva para uma parede\n",
        "                    if self.environment.util(state, action): \n",
        "                        break\n",
        "            else:\n",
        "                action = self.agent.policy[state[0]][state[1]]\n",
        "            self.episode(state, action, max_steps= min(3000, shape[1]*shape[0]), exploration_chance = exploration_chance)\n",
        "            g = 0\n",
        "            for t in range(len(self.agent.recalls)-1, -1, -1): \n",
        "                memory = self.agent.recalls[t]  # memoria = (estado, acao, reforco)\n",
        "                g = self.agent.gamma*g + memory[2]\n",
        "                # verifica se o par estado acao ja foi inserido em returns\n",
        "                if self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"lastEpisode\"] != ep:\n",
        "                    self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"lastEpisode\"] = ep\n",
        "                    self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"value\"] += g\n",
        "                    self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"count\"] += 1\n",
        "                    media = self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"value\"]/self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"count\"]\n",
        "\n",
        "                    if(not appx):\n",
        "                        self.Q[memory[0][0],memory[0][1],self.agent.action_idx(memory[1])] = media\n",
        "                    else:\n",
        "                        Q = self.get_Q(memory[0][0],memory[0][1],self.agent.action_idx(memory[1]),appx)\n",
        "                        # print(f\"Q: {Q}\")\n",
        "                        features = self.data_to_features(memory[0][0],memory[0][1],self.agent.action_idx(memory[1]))\n",
        "                        # print(f\"{features[0]=}, {g=}, {Q=}, {self.W[0]=}, {alpha=}, {features[0]*alpha*(g-Q)=}\")\n",
        "                        deltaW = alpha * (g - Q) * features\n",
        "                        # print(f\"deltaW: {deltaW}\")\n",
        "                        self.W += deltaW\n",
        "\n",
        "\n",
        "                    self.agent.book_V[memory[0][0]][memory[0][1]] = media\n",
        "                    self.agent.policy[memory[0][0]][memory[0][1]] = max(self.agent.actions, key = lambda action: self.get_Q(memory[0][0],memory[0][1],self.agent.action_idx(action),appx))    # recebe a action que maximiza o valor de Q\n",
        "            # atualiza a chance de exploracao\n",
        "            rewards.append(np.asarray(self.episode_R[ep]).sum())\n",
        "            current_exploration_chance *= 0.999\n",
        "            end_time = time.time()\n",
        "            time_difference_seconds = end_time - start_time\n",
        "            self.time.append(time_difference_seconds)\n",
        "\n",
        "        end_training_time = time.time()\n",
        "        if display:\n",
        "            print(f\"Tempo total de treinamento: {end_training_time - begin_training_time} segundos\")\n",
        "            self.show_loss(rewards, window_size=(len(rewards)//10))\n",
        "        return round(end_training_time - begin_training_time, 3)\n",
        "\n",
        "            \n",
        "    def episode(self, state, action, max_steps, exploration_chance=0):\n",
        "        step_count = 0\n",
        "        self.agent.recalls = []\n",
        "        reward = self.environment.setAgentPos(state[0], state[1])\n",
        "        episode_R = [reward]\n",
        "\n",
        "        while (not self.environment.in_terminal_state()) and (step_count < max_steps):  # enquanto nao estiver em um estado terminal\n",
        "            step_count +=1  # incrementa o numero de passos\n",
        "            last_pos = (self.agent.y, self.agent.x)\n",
        "            reward = self.environment.move(self.agent,action) # realiza a acao e recebe a recompensa\n",
        "            episode_R.append(reward)\n",
        "            self.agent.recalls.append((last_pos, action, reward)) # guarda o passo\n",
        "            if random.random() < exploration_chance:\n",
        "                for _ in range(len(self.agent.actions)*2):    # limite maximo de tentativas\n",
        "                    action = random.choice(self.agent.actions)\n",
        "                    # se a acao nao te leva para uma parede\n",
        "                    if self.environment.util(state, action): break\n",
        "            else:\n",
        "                action = self.agent.get_action() # escolhe uma acao de acordo com a politica\n",
        "        self.episode_R.append(episode_R)\n",
        "        self.episode_length.append(step_count)\n",
        "    \n",
        "    def path_from(self, starting_point):\n",
        "        shape = self.environment.get_size()\n",
        "        max_steps = shape[0] * shape[1]\n",
        "        step_count = 0\n",
        "        state = starting_point\n",
        "        self.environment.setAgentPos(state[0], state[1])\n",
        "        tuples = []\n",
        "        while (not self.environment.in_terminal_state()) and (step_count < max_steps):\n",
        "            action =  self.agent.policy[state[0]][state[1]]\n",
        "            R = self.environment.move(self.agent, action)\n",
        "            tuples.append((state,action,R))\n",
        "            state = (self.agent.y, self.agent.x)\n",
        "            step_count+=1\n",
        "        return tuples\n",
        "\n",
        "\n",
        "class SARSA(LearningStrategy):\n",
        "\n",
        "    def __init__(self, lam):\n",
        "        super().__init__()\n",
        "        self.lam = lam\n",
        "        self.episode_R = []\n",
        "        self.episode_length = []\n",
        "        self.Q = None\n",
        "        self.time = []\n",
        "        \n",
        "    def get_greedy_action(self,state, appx = False):\n",
        "        return max(self.agent.actions, key = lambda action: self.get_Q(state[0], state[1], self.agent.action_idx(action),appx))\n",
        "    \n",
        "    def get_epsilon_greedy(self, exploration_chance, state, appx = False):\n",
        "        if random.random() < exploration_chance:\n",
        "            return random.choice(self.agent.actions)\n",
        "        else:\n",
        "            return max(self.agent.actions, key = lambda action: self.get_Q(state[0], state[1], self.agent.action_idx(action), appx))\n",
        "                \n",
        "    def train(self, episodes, random_policy=True, exploration_chance=0.3, alpha=0.001, appx = False, display = True):\n",
        "        begin_training_time = time.time()\n",
        "        shape = self.environment.get_size()\n",
        "        ec = exploration_chance\n",
        "        num_states = shape[0]*shape[1]\n",
        "        linear_decay = exploration_chance/episodes\n",
        "        self.Q = np.zeros((shape[0],shape[1], len(self.agent.actions)))\n",
        "        rewards = []\n",
        "        E = dict()\n",
        "        for ep in range(episodes):\n",
        "            start_time = time.time()\n",
        "            episode_R = []\n",
        "            if display:\n",
        "                if ep % (episodes//10) == 0: \n",
        "                    print(f\"{ep=}\")\n",
        "                    path = self.path_from((1,1))\n",
        "                    print('Tamanho do epis√≥dio:', len(path))\n",
        "                    print('Recompensa', sum([i[2] for i in path]))\n",
        "                    path_dict = {}\n",
        "                    for s,a,r in path:\n",
        "                        path_dict[s] = a\n",
        "                    self.environment.render.show_path(path_dict)\n",
        "\n",
        "            E = dict()         \n",
        "            \n",
        "            # escolhe posicao aleatoria valida para o agente\n",
        "            while True:\n",
        "                S = (random.randrange(0, shape[0]), random.randrange(0, shape[1]))\n",
        "                if self.environment.original_map[S[0]][S[1]] in {self.environment.default_symbols[\"path\"], self.environment.default_symbols[\"goal\"]}:\n",
        "                    break\n",
        "            self.environment.setAgentPos(S[0], S[1])\n",
        "\n",
        "            A = self.get_epsilon_greedy(ec,S)\n",
        "            A_idx = self.agent.action_idx(A)\n",
        "\n",
        "            step_count = 0\n",
        "            while (not self.environment.in_terminal_state()) and (step_count < num_states):\n",
        "\n",
        "                R = self.environment.move(self.agent, A)\n",
        "                episode_R.append(R)\n",
        "                S_prime = (self.agent.y, self.agent.x)\n",
        "                A_prime = self.get_epsilon_greedy(ec, S_prime)\n",
        "                A_prime_idx = self.agent.action_idx(A)\n",
        "                \n",
        "                pair = (S, A)\n",
        "                E[pair] = E[pair] + 1 if pair in E.keys() else 1\n",
        "\n",
        "                delta = R + self.agent.gamma * self.get_Q(S_prime[0], S_prime[1], A_prime_idx, appx) - self.get_Q(S[0], S[1], A_idx, appx)\n",
        "                if (not appx):\n",
        "                    for (s, a) in E.keys():\n",
        "                        a_idx = self.agent.action_idx(a)\n",
        "                        self.Q[s[0],s[1], a_idx] += alpha * delta * E[(s,a)]\n",
        "                        E[(s,a)] *= self.agent.gamma * self.lam\n",
        "                else: \n",
        "                    for (s, a) in E.keys():\n",
        "                        a_idx = self.agent.action_idx(a)\n",
        "                        features = self.data_to_features(s[0],s[1],a_idx)\n",
        "                        deltaW = alpha * delta * E[(s,a)] * features\n",
        "                        self.W += deltaW\n",
        "                        E[(s,a)] *= self.agent.gamma * self.lam\n",
        "                \n",
        "                S = S_prime\n",
        "                A = A_prime\n",
        "                step_count += 1\n",
        "            self.episode_R.append(episode_R)\n",
        "            self.episode_length.append(step_count)\n",
        "            ec-=linear_decay\n",
        "            rewards.append(np.asarray(self.episode_R[ep]).sum())\n",
        "            end_time = time.time()\n",
        "            time_difference_seconds = end_time - start_time\n",
        "            self.time.append(time_difference_seconds)\n",
        "            \n",
        "\n",
        "        self.agent.startPolicy(shape, random_policy)\n",
        "        self.agent.startV(shape)\n",
        "        for i in range(shape[0]):\n",
        "            for j in range(shape[1]):\n",
        "                if(self.environment.original_map[i][j] == '#'): self.agent.policy[i][j] = \"wall\"\n",
        "                else: \n",
        "                    self.agent.policy[i][j] = max(self.agent.actions, key = lambda action: self.get_Q(i,j, self.agent.action_idx(action), appx))\n",
        "                    self.agent.book_V[i][j] = max(self.Q[i,j,:])\n",
        "                    \n",
        "        end_training_time = time.time()\n",
        "        if display:\n",
        "            print(f\"Tempo total de treinamento: {end_training_time - begin_training_time} segundos\")\n",
        "            self.show_loss(rewards, window_size=(len(rewards)//10))\n",
        "        return round(end_training_time - begin_training_time, 3)\n",
        "\n",
        "    def path_from(self, starting_point):\n",
        "        shape = self.environment.get_size()\n",
        "        max_steps = shape[0] * shape[1]\n",
        "        step_count = 0\n",
        "        S = starting_point\n",
        "        self.environment.setAgentPos(S[0], S[1])\n",
        "        tuples = []\n",
        "        while (not self.environment.in_terminal_state()) and (step_count < max_steps):\n",
        "            A = self.get_greedy_action(S)\n",
        "            R = self.environment.move(self.agent, A)\n",
        "            tuples.append((S,A,R))\n",
        "            S = (self.agent.y, self.agent.x)\n",
        "            step_count+=1\n",
        "        return tuples\n",
        "                \n",
        "\n",
        "\n",
        "        \n",
        "class QLearning(LearningStrategy):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.episode_R = []\n",
        "        self.episode_length = []\n",
        "        self.Q = None\n",
        "        \n",
        "        self.time = []\n",
        "\n",
        "    def get_greedy_action(self,state,appx = False):\n",
        "        return max(self.agent.actions, key = lambda action: self.get_Q(state[0], state[1], self.agent.action_idx(action),appx))\n",
        "    \n",
        "    def get_epsilon_greedy(self, exploration_chance, state, appx = False):\n",
        "        if random.random() < exploration_chance:\n",
        "            return random.choice(self.agent.actions)\n",
        "        else:\n",
        "            return max(self.agent.actions, key = lambda action: self.get_Q(state[0], state[1], self.agent.action_idx(action),appx))\n",
        "                \n",
        "    def train(self, episodes, random_policy=True, exploration_chance=0.3, alpha=0.003, appx = True, display = True):\n",
        "        begin_training_time = time.time()\n",
        "        shape = self.environment.get_size()\n",
        "        ec = exploration_chance\n",
        "        num_states = shape[0]*shape[1]\n",
        "        linear_decay = exploration_chance/episodes\n",
        "        self.Q = np.zeros((shape[0],shape[1], len(self.agent.actions)))\n",
        "\n",
        "        for ep in range(episodes):\n",
        "            start_time = time.time()\n",
        "            episode_R = []\n",
        "            if display:\n",
        "                if ep % (episodes//10) == 0: \n",
        "                    print(f\"{ep=}\")\n",
        "                    path = self.path_from((1,1))\n",
        "                    print('Tamanho do epis√≥dio:', len(path))\n",
        "                    print('Recompensa', sum([i[2] for i in path]))\n",
        "                    path_dict = {}\n",
        "                    for s,a,r in path:\n",
        "                        path_dict[s] = a\n",
        "                    self.environment.render.show_path(path_dict)\n",
        "\n",
        "            # escolhe posicao aleatoria valida para o agente\n",
        "            while True:\n",
        "                S = (random.randrange(0, shape[0]), random.randrange(0, shape[1]))\n",
        "                if self.environment.original_map[S[0]][S[1]] in {self.environment.default_symbols[\"path\"], self.environment.default_symbols[\"goal\"]}:\n",
        "                    break\n",
        "            self.environment.setAgentPos(S[0], S[1])\n",
        "\n",
        "            #A = random.choice(self.agent.actions)\n",
        "            step_count = 0\n",
        "            while (not self.environment.in_terminal_state()) and (step_count < num_states):\n",
        "                A = self.get_epsilon_greedy(ec,S)\n",
        "                A_idx = self.agent.action_idx(A)\n",
        "\n",
        "                R = self.environment.move(self.agent, A)\n",
        "\n",
        "                episode_R.append(R)\n",
        "\n",
        "                S_prime = (self.agent.y, self.agent.x)\n",
        "                A_prime = self.get_greedy_action(S_prime)\n",
        "                A_prime_idx = self.agent.action_idx(A_prime)\n",
        "\n",
        "                if(not appx):\n",
        "                    self.Q[S[0], S[1], A_idx] += alpha*(R + self.agent.gamma * self.get_Q(S_prime[0], S_prime[1], A_prime_idx) - self.get_Q(S[0], S[1], A_idx))\n",
        "                else:\n",
        "                    deltaW = alpha * (R + self.agent.gamma * self.get_Q(S_prime[0], S_prime[1], A_prime_idx, appx) - self.get_Q(S[0], S[1], A_idx, appx)) * self.data_to_features(S[0],S[1],A_idx)\n",
        "                    self.W += deltaW\n",
        "\n",
        "                \n",
        "                S = S_prime\n",
        "                step_count += 1\n",
        "                \n",
        "            self.episode_R.append(episode_R)\n",
        "            self.episode_length.append(step_count)\n",
        "            ec-=linear_decay\n",
        "            end_time = time.time()\n",
        "            time_difference_seconds = end_time - start_time\n",
        "            self.time.append(time_difference_seconds)\n",
        "            \n",
        "\n",
        "        self.agent.startPolicy(shape, random_policy)\n",
        "        self.agent.startV(shape)\n",
        "        for i in range(shape[0]):\n",
        "            for j in range(shape[1]):\n",
        "                if(self.environment.original_map[i][j] == '#'): self.agent.policy[i][j] = \"wall\"\n",
        "                else: \n",
        "                    self.agent.policy[i][j] = max(self.agent.actions, key = lambda action: self.get_Q(i,j, self.agent.action_idx(action),appx))\n",
        "                    self.agent.book_V[i][j] = max(self.Q[i,j,:])\n",
        "        end_training_time = time.time()\n",
        "        return round(end_training_time - begin_training_time, 3)\n",
        "\n",
        "    def path_from(self, starting_point):\n",
        "        shape = self.environment.get_size()\n",
        "        max_steps = shape[0] * shape[1]\n",
        "        step_count = 0\n",
        "        S = starting_point\n",
        "        self.environment.setAgentPos(S[0], S[1])\n",
        "        tuples = []\n",
        "        while (not self.environment.in_terminal_state()) and (step_count < max_steps):\n",
        "            A = self.get_greedy_action(S)\n",
        "            R = self.environment.move(self.agent, A)\n",
        "            tuples.append((S,A,R))\n",
        "            S = (self.agent.y, self.agent.x)\n",
        "            step_count+=1\n",
        "        return tuples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Environment:\n",
        "    default_symbols = {\"agent\": '@', \"wall\": '#', \"path\": '.', \"goal\":'$', \"lava\":'L', \"acid\":'A', \"cookie\":'C'}\n",
        "    def __init__(self, path, stochastic, display=True) -> None:\n",
        "        self.display = display\n",
        "        self.avaliations = []\n",
        "        self.original_map = self.load_map(path)\n",
        "        self.map = self.copy_map(self.original_map)\n",
        "        self.wait_time = 0\n",
        "        self.stochastic = stochastic\n",
        "        self.dists_goal = None\n",
        "\n",
        "        if self.display:\n",
        "            self.render = Renderer(self, self.map, \"Ambiente\")\n",
        "\n",
        "    def copy_map(self, mapa):\n",
        "        mapaCopia = []\n",
        "        for linha in mapa:\n",
        "            mapaCopia.append([])\n",
        "            for celula in linha:\n",
        "                mapaCopia[-1].append(celula)\n",
        "        return mapaCopia\n",
        "\n",
        "    def getAgent(self) -> Agent:\n",
        "        return self.agent\n",
        "\n",
        "    def in_terminal_state(self):\n",
        "        return self.original_map[self.agent.y][self.agent.x] in (self.default_symbols[\"goal\"], self.default_symbols[\"lava\"])\n",
        "    \n",
        "    def get_dist_goals(self):\n",
        "        self.dists_goal = np.full((len(self.map), len(self.map[0])), np.inf)\n",
        "        for i in range(len(self.map)):\n",
        "            for j in range(len(self.map[0])):\n",
        "                if self.original_map[i][j] == self.default_symbols[\"goal\"]:\n",
        "                    self.dists_goal[i][j] = 0\n",
        "        for i in range(1,len(self.map)-1):\n",
        "            for j in range(1,len(self.map[0])-1):\n",
        "                self.dists_goal[i][j] = min(self.dists_goal[i][j], self.dists_goal[i-1][j]+1, self.dists_goal[i+1][j]+1, self.dists_goal[i][j-1]+1, self.dists_goal[i][j+1]+1)\n",
        "\n",
        "    def get_dist_closest_goal(self, agent):\n",
        "        if self.dists_goal is None:\n",
        "            self.dists_goal = self.get_dist_goals()\n",
        "        return self.dists_goal[agent.y][agent.x]\n",
        "\n",
        "    \n",
        "    def get_index_object(self, object):\n",
        "        if object == self.default_symbols[\"agent\"]:\n",
        "            return 0\n",
        "        elif object == self.default_symbols[\"wall\"]:\n",
        "            return 1\n",
        "        elif object == self.default_symbols[\"path\"]:\n",
        "            return 2\n",
        "        elif object == self.default_symbols[\"goal\"]:\n",
        "            return 3\n",
        "        elif object == self.default_symbols[\"lava\"]:\n",
        "            return 4\n",
        "        elif object == self.default_symbols[\"acid\"]:\n",
        "            return 5\n",
        "        elif object == self.default_symbols[\"cookie\"]:\n",
        "            return 6\n",
        "        else:\n",
        "            return 7\n",
        "\n",
        "    def get_sensors(self, agent, num_sensors, sensors_type):\n",
        "        sensors = np.zeros(num_sensors)\n",
        "        s_i = 0\n",
        "        i = 0\n",
        "        while i < len(sensors_type):\n",
        "            match sensors_type[i]:\n",
        "                case \"radius\":\n",
        "                    i += 1\n",
        "                    radius = sensors_type[i]\n",
        "                    for j in range(-radius, radius+1):\n",
        "                        for k in range(-radius, radius+1):\n",
        "                            object = self.get_index_object(self.original_map[agent.y+j][agent.x+k])\n",
        "                            sensors[s_i] = object\n",
        "                            s_i += 1\n",
        "                    i += 1\n",
        "                case \"smell\":\n",
        "                    distance = self.get_dist_closest_goal(agent)\n",
        "                    sensors[s_i] = distance\n",
        "                    s_i += 1\n",
        "                    i += 1\n",
        "                \n",
        "                case _:\n",
        "                    i+=1\n",
        "\n",
        "\n",
        "    def load_map(self, path):\n",
        "        \"\"\"\n",
        "        Dado o caminho path, le um arquivo txt e retorna uma matriz\n",
        "        O txt consiste de uma linha contendo o numero n (n√∫mero de\n",
        "        linhas do mapa) e m (n√∫mero de caracteres diferentes no mapa),\n",
        "        seguido de m linhas explicando o que sao os caracteres no \n",
        "        arquivo e por fim n linhas contendo o mapa que sao caracteres\n",
        "        \"\"\"\n",
        "        grid = []\n",
        "        self.symbols = dict()\n",
        "        self.rewards = {\"agent\": 0, \"wall\": 0, \"path\": 0, \"goal\":0}\n",
        "        with open(path, 'r') as file:\n",
        "            m, n = map(int, file.readline().split())\n",
        "            for _ in range(m):\n",
        "                line = file.readline().split()\n",
        "                self.symbols[line[0]] = line[1]\n",
        "                self.rewards[line[1]] = int(line[2])\n",
        "            for i in range(n):\n",
        "                line = file.readline()\n",
        "                grid.append([])\n",
        "                for j in range(len(line)):\n",
        "                    char = line[j]\n",
        "                    if char == '\\n':\n",
        "                        continue\n",
        "                    if self.symbols[char] == 'agent':\n",
        "                        self.avaliations.append((i,j))\n",
        "                        char = self.default_symbols[\"path\"]\n",
        "                    grid[-1].append(self.default_symbols[self.symbols[char]])\n",
        "        return grid\n",
        "    \n",
        "    def move(self, agent, acao):\n",
        "        \"\"\"\n",
        "        Dada uma acao, move o agente no mapa\n",
        "        a acao pode ser \"up\", \"down\", \"left\" ou \"right\"\n",
        "        \"\"\"\n",
        "        if random.random() < self.stochastic:\n",
        "            acao = random.choice(agent.actions)\n",
        "        time.sleep(self.wait_time)\n",
        "        direction = {\"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)}\n",
        "        posicaofinal = (agent.y+direction[acao][0], agent.x+direction[acao][1])\n",
        "        if self.map[posicaofinal[0]][posicaofinal[1]] != self.default_symbols[\"wall\"]:\n",
        "            # seta a posicao atual como caminho\n",
        "            self.map[agent.y][agent.x] = self.original_map[agent.y][agent.x]\n",
        "            \n",
        "            # seta a posicao final como o agente\n",
        "            self.map[posicaofinal[0]][posicaofinal[1]] = self.default_symbols[\"agent\"]\n",
        "            \n",
        "            # atualiza a posicao do agente\n",
        "            agent.setPos(posicaofinal)\n",
        "        # retorna o reforco da posicao final \n",
        "        return self.rewards[self.symbols[self.original_map[agent.y][agent.x]]]\n",
        "\n",
        "    def util(self, pos, acao):\n",
        "        \"\"\"\n",
        "        Dada uma posicao e uma acao, retorna o que tem na posicao destino\n",
        "        \"\"\"\n",
        "        direcao = {\"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)}\n",
        "        posicaofinal = (pos[0]+direcao[acao][0], pos[1]+direcao[acao][1])\n",
        "        return self.symbols[self.map[posicaofinal[0]][posicaofinal[1]]] != \"wall\"\n",
        "\n",
        "    def get_size(self):\n",
        "        return len(self.map), len(self.map[0])\n",
        "    \n",
        "    def setAgentPos(self, i, j):\n",
        "        self.map[self.agent.y][self.agent.x] = self.original_map[self.agent.y][self.agent.x]\n",
        "        self.agent.setPos((i, j))\n",
        "        self.map[i][j] = self.default_symbols[\"agent\"]\n",
        "        return self.rewards[self.symbols[self.original_map[self.agent.y][self.agent.x]]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n",
            "15.270735025405884\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "learning_strategy = QLearning()\n",
        "environment = Environment('salas/salaA.txt', True, display=False)\n",
        "agent = Agent(x=0, y=0, environment=environment, display=environment.display)\n",
        "environment.agent = agent\n",
        "learning_strategy.setup(environment, agent)\n",
        "\n",
        "learning_strategy.train(10000, exploration_chance=0.5, display=False, appx=False)\n",
        "#agent.render.show()\n",
        "print(\"Done\")\n",
        "print(sum(learning_strategy.time))\n",
        "\n",
        "Renderer.create_heatmap(learning_strategy.agent.book_V, cmap='inferno', title='Sample Heatmap')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents import trajectories\n",
        "from tf_agents.utils import common\n",
        "import matplotlib.pyplot as pl\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "import gym\n",
        "import argparse\n",
        "\n",
        "\n",
        "# Define a larger map\n",
        "large_map = [\n",
        "    'SFFFFFFF',\n",
        "    'FFFFFFFF',\n",
        "    'FFFHFFFF',\n",
        "    'FFFFFFFF',\n",
        "    'HFFFFFFF',\n",
        "    'FFFFFFFF',\n",
        "    'FFFFFFFF',\n",
        "    'GFFFFFFF'\n",
        "]\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    \n",
        "    parser.add_argument('--iter', '-i', type=int, default=1000)\n",
        "    parser.add_argument('--slippery', '-s', type=bool ,action=argparse.BooleanOptionalAction)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    arg_num_iterations = args.iter\n",
        "    arg_is_slippery = args.slippery if args.slippery is not None else False\n",
        "\n",
        "    print('Number of iterations: ', arg_num_iterations, \"\\nIs slippery: \", arg_is_slippery)\n",
        "\n",
        "    train_env = make_env(arg_is_slippery)\n",
        "    tf_agent = make_tf_agent(train_env)\n",
        "    replay_buffer = make_replay_buffer(tf_agent, train_env)\n",
        "    collect_episodes_per_iteration = 1\n",
        "    batch_size = 64\n",
        "    dataset = replay_buffer.as_dataset(\n",
        "        num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2\n",
        "    ).prefetch(3)\n",
        "    iterator = iter(dataset)\n",
        "    collect_episode(\n",
        "        train_env,\n",
        "        tf_agent.collect_policy,\n",
        "        collect_episodes_per_iteration + batch_size,\n",
        "        replay_buffer,\n",
        "    )\n",
        "\n",
        "    returns = []\n",
        "    losses = []\n",
        "    weights = []\n",
        "    num_iterations = arg_num_iterations\n",
        "    eval_interval = 50\n",
        "    num_eval_episodes = 10\n",
        "    for _ in tqdm.tqdm(range(num_iterations)):\n",
        "        collect_episode(\n",
        "            train_env,\n",
        "            tf_agent.collect_policy,\n",
        "            collect_episodes_per_iteration,\n",
        "            replay_buffer,\n",
        "        )\n",
        "        experience, unused_info = next(iterator)\n",
        "        experience2 = trajectory.Trajectory(\n",
        "            experience.step_type,\n",
        "            experience.observation,\n",
        "            experience.action,\n",
        "            experience.policy_info,\n",
        "            experience.next_step_type,\n",
        "            experience.reward,\n",
        "            experience.discount * 0.95,\n",
        "        )\n",
        "        losses.append(tf_agent.train(experience2).loss)\n",
        "        step = tf_agent.train_step_counter.numpy()\n",
        "        s  = f\"_{arg_num_iterations}_{arg_is_slippery}\"\n",
        "        if step % eval_interval == 0:\n",
        "            avg_return = compute_avg_return(\n",
        "                train_env, tf_agent.policy, num_eval_episodes\n",
        "            )\n",
        "            returns.append(avg_return)\n",
        "            weights.append(tf_agent._q_network.layers[0].get_weights())\n",
        "            \n",
        "            plot_returns(eval_interval, returns,s)\n",
        "            plot_loss(losses,s)\n",
        "            plot_weights(eval_interval, weights,s)\n",
        "    #view_policy(train_env, tf_agent.policy)\n",
        "    show_policy_for_each_state(train_env, tf_agent.policy,s)\n",
        "\n",
        "\n",
        "def make_replay_buffer(tf_agent, train_env):\n",
        "    replay_buffer_capacity = 100000\n",
        "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "        data_spec=tf_agent.collect_data_spec,\n",
        "        batch_size=train_env.batch_size,\n",
        "        max_length=replay_buffer_capacity,\n",
        "    )\n",
        "    return replay_buffer\n",
        "\n",
        "\n",
        "def make_env(is_slippery):\n",
        "    env_name = \"FrozenLake-v1\"\n",
        "\n",
        "    train_gym_env = gym.make(env_name, desc=large_map, is_slippery=is_slippery)\n",
        "    train_py_env = suite_gym.wrap_env(train_gym_env)\n",
        "    train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "\n",
        "    print(\"Observation and action:\")\n",
        "    print(train_env.observation_spec())\n",
        "    print(train_env.action_spec())\n",
        "    return train_env\n",
        "\n",
        "\n",
        "def make_tf_agent(train_env):\n",
        "    dense_layers = [\n",
        "        tf.keras.layers.Embedding(64, 4),\n",
        "    ]\n",
        "    q_net = sequential.Sequential(dense_layers)\n",
        "    learning_rate = 1e-3\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    train_step_counter = tf.Variable(0)\n",
        "    tf_agent = dqn_agent.DqnAgent(\n",
        "        train_env.time_step_spec(),\n",
        "        train_env.action_spec(),\n",
        "        q_network=q_net,\n",
        "        optimizer=optimizer,\n",
        "        td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "        train_step_counter=train_step_counter,\n",
        "    )\n",
        "    tf_agent.initialize()\n",
        "    summarize_network(q_net)\n",
        "    tf_agent.train = common.function(tf_agent.train)\n",
        "    tf_agent.train_step_counter.assign(0)\n",
        "    return tf_agent\n",
        "\n",
        "\n",
        "def plot_returns(eval_interval, returns, s):\n",
        "    steps = np.arange(0, len(returns)) * eval_interval\n",
        "    pl.clf()\n",
        "    pl.gcf().set_size_inches((7, 7))\n",
        "    pl.plot(steps, returns, marker=\"o\")\n",
        "    pl.ylabel(\"Average Return\")\n",
        "    pl.xlabel(\"Step\")\n",
        "    pl.tight_layout()\n",
        "    pl.savefig(f\"training{s}.pdf\")\n",
        "    pl.savefig(f\"training{s}.png\", dpi=150)\n",
        "\n",
        "\n",
        "def plot_loss(losses,s):\n",
        "    steps = np.arange(1, len(losses) + 1)\n",
        "    pl.clf()\n",
        "    pl.gcf().set_size_inches((7, 7))\n",
        "    pl.semilogy(steps, losses, marker=\".\", linestyle=\"none\", alpha=0.5)\n",
        "    pl.ylabel(\"Loss\")\n",
        "    pl.xlabel(\"Steps\")\n",
        "    pl.tight_layout()\n",
        "    pl.savefig(f\"loss{s}.pdf\")\n",
        "    pl.savefig(f\"loss{s}.png\", dpi=150)\n",
        "\n",
        "\n",
        "def plot_weights(eval_interval, weights,s):\n",
        "    pl.clf()\n",
        "    pl.gcf().set_size_inches((18, 48))\n",
        "    all_weights = np.stack(weights)\n",
        "    print(all_weights.shape)\n",
        "    ylim = np.min(all_weights.flatten()), np.max(all_weights.flatten())\n",
        "    steps = np.arange(all_weights.shape[0]) * eval_interval\n",
        "    for row in range(all_weights.shape[2]):\n",
        "        for col in range(all_weights.shape[3]):\n",
        "            ax = pl.gcf().add_subplot(\n",
        "                all_weights.shape[2],\n",
        "                all_weights.shape[3],\n",
        "                row * all_weights.shape[3] + col + 1,\n",
        "            )\n",
        "            x = steps\n",
        "            y = all_weights[:, 0, row, col]\n",
        "            ax.plot(x, y, marker=\"o\")\n",
        "            ax.grid(True)\n",
        "            ax.set_ylim(ylim)\n",
        "    pl.tight_layout()\n",
        "    pl.savefig(f\"weights{s}.pdf\")\n",
        "    pl.savefig(f\"weights{s}.png\", dpi=150)\n",
        "\n",
        "\n",
        "def summarize_network(q_net):\n",
        "    q_net.summary()\n",
        "    for layer in q_net.layers:\n",
        "        print(layer)\n",
        "        print(layer.name)\n",
        "        if hasattr(layer, \"activation\"):\n",
        "            print(layer.activation)\n",
        "        if hasattr(layer, \"layers\"):\n",
        "            for layer in layer.layers[1:]:\n",
        "                print(layer.name)\n",
        "                print(layer.activation)\n",
        "\n",
        "\n",
        "def compute_avg_return(env, policy, num_episodes=10):\n",
        "    total_return = 0.0\n",
        "    for _ in range(num_episodes):\n",
        "        time_step = env.reset()\n",
        "        episode_return = 0.0\n",
        "\n",
        "        while not time_step.is_last():\n",
        "            action_step = policy.action(time_step)\n",
        "            time_step = env.step(action_step.action)\n",
        "            episode_return += time_step.reward\n",
        "        total_return += episode_return\n",
        "\n",
        "    avg_return = total_return / num_episodes\n",
        "    return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "def collect_episode(env, policy, num_episodes, replay_buffer):\n",
        "    episode_counter = 0\n",
        "    env.reset()\n",
        "\n",
        "    while episode_counter < num_episodes:\n",
        "        time_step = env.current_time_step()\n",
        "        action_step = policy.action(time_step)\n",
        "        next_time_step = env.step(action_step.action)\n",
        "        traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "        replay_buffer.add_batch(traj)\n",
        "\n",
        "        if traj.is_boundary():\n",
        "            episode_counter += 1\n",
        "    \n",
        "def view_policy(env, policy):\n",
        "    for _ in range(10):\n",
        "        time_step = env.reset()\n",
        "        while not time_step.is_last():\n",
        "            pl.clf()\n",
        "            env.render()\n",
        "            pl.gcf().canvas.draw()\n",
        "            action_step = policy.action(time_step)\n",
        "            time_step = env.step(action_step.action)\n",
        "            pl.pause(0.1)\n",
        "\n",
        "def show_policy_for_each_state(env, policy,s):\n",
        "    buffer = []\n",
        "    num_states = env.observation_spec().maximum + 1\n",
        "    for state in range(num_states):\n",
        "        time_step = trajectories.time_step.TimeStep(\n",
        "            step_type=np.array(0, dtype=np.int32),\n",
        "            reward=np.array(0.0, dtype=np.float32),\n",
        "            discount=np.array(1.0, dtype=np.float32),\n",
        "            observation=np.array(state, dtype=np.int32)\n",
        "        )\n",
        "        action_step = policy.action(time_step)\n",
        "        buffer.append(f\"State: {state}, Action: {action_step.action.numpy()}\")\n",
        "    open(f\"policy{s}.txt\", \"w\").write(\"\\n\".join(buffer))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <b style=\"color:#009900\">An√°lise do Monte Carlo em ambiente determin√≠stico</b>\n",
        "\n",
        "O algoritmo Monte Carlo utilizado foi colocado em diversas salas com diferentes par√¢metros. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##  <b style=\"color:#22BBBB\">Sala 1</b>\n",
        "A figura abaixo mostra a estrutura da primeira sala usada para avalia√ß√£o do modelo, sendo essa a sala mais simples que foi usada para os experimentos.\n",
        "\n",
        "![Sala 1](figs/sala%201.png)\n",
        "\n",
        "Na imagem o objetivo se mostra apresenta em forma de uma moeda e o agente na forma de um alien verde. Al√©m disso, √© poss√≠vel ver os caminhos v√°lidos em preto e as paredes em rosa. Cada caminho v√°lido aqui garante uma recompensa de -1, enquanto o objetivo garante uma recompensa de 100. Movimentos para cima ou para baixo nesse mapa s√£o sempre em dire√ß√£o a uma parede, o que n√£o move o agente do lugar. Assim √© esperado que esse tipo de comportamento seja evitado com o passar do tempo. A pol√≠tica √© inicializada de forma aleat√≥ria e pode ser verificada na forma de setas azuis, como na figura abaixo:\n",
        "\n",
        "![Politica inicial da sala 1](figs/politicaInicialSala1.png)\n",
        "\n",
        "Um primeiro obst√°culo foi verificado aqui: Como a pol√≠tica inicial pode iniciar direcionando o agente para um local inv√°lido, o mantendo preso para sempre, √© necess√°rio definir um valor m√°ximo de passos para cada epis√≥dio. Aqui esse valor foi de 10 passos. Ou seja, o epis√≥dio acaba quando o agente atinge o estado terminal ou ap√≥s 10 passos.\n",
        "Ap√≥s 10 epis√≥dios nesse cen√°rio, o algoritmo n√£o apresentou uma pol√≠tica satisfat√≥ria. Por√©m com 12 epis√≥dios houve converg√™ncia. O n√∫mero 12 aqui √© curioso pois √© m√∫ltiplo do n√∫mero de a√ß√µes dispon√≠veis (4). E nesse cen√°rio h√° 3 poss√≠veis locais onde o personagem pode surgir e tomar uma a√ß√£o v√°lida, j√° que caso ele surja no estado terminal a simula√ß√£o acaba o impedindo de tomar uma a√ß√£o. Assim, 12 √© um n√∫mero suficiente para ele convergir, dado uma probabilidade aleat√≥ria, chegando por fim na pol√≠tica √≥tima abaixo:\n",
        "\n",
        "![Politica final da sala 1](figs/politicaFinalSala1.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##  <b style=\"color:#22BBBB\">Sala 2</b>\n",
        "A figura abaixo mostra a estrutura da segunda sala usada para avalia√ß√£o do modelo, uma das salas mais simples presentes, mas que aumenta levemente o n√∫mero de caminhos v√°lidos dispon√≠veis e possibilita um movimento vertical limitado.\n",
        "\n",
        "![Sala 2](figs/sala%202.png)\n",
        "\n",
        "Diferentemente da primeira sala, aqui os movimentos s√£o menos limitados e h√° mais de um caminho √≥timo para o agente descobrir. A pol√≠tica inicial aleat√≥ria pode ser vista abaixo:\n",
        "\n",
        "![Politica inicial da sala 2](figs/politicaInicialSala2.png)\n",
        "\n",
        "Aqui os 12 epis√≥dios n√£o foram suficientes para obter alguma pol√≠tica √≥tima, e nem era esperado que isso acontecesse, pois o n√∫mero de movimentos v√°lidos e caminhos aumentou. Observando a imagem √© poss√≠vel verificar que h√° 7 estados n√£o terminais, nos quais 4 possuem apenas 1 movimento √≥timo e 3 com 2 movimentos √≥timos (direita ou cima). Assim, h√° um total de 28 op√ß√µes totais das quais 10 s√£o √≥timas. Isso indica que o agente precisa testar no m√≠nimo 18 op√ß√µes distintas para encontrar uma pol√≠tica √≥tima. Treinando o modelo com 18 epis√≥dios temos (no melhor resultado):\n",
        "\n",
        "![Politica da sala 2 com 18 epis√≥dios](figs/politicaFinalSala2ep18.png)\n",
        "\n",
        "Aqui podemos ver que o agente errou em apenas 1 estado a pol√≠tica √≥tima, por√©m nesse estado ele fica preso. Al√©m disso, esse resultado n√£o reflete o caso m√©dio, que √© bem mais desordenado, mostrando que esse caso foi originado por uma pol√≠tica inicial aleat√≥ria muito boa e 18 epis√≥dios n√£o s√£o o suficiente para obter uma pol√≠tica adequada para o agente. J√° com 36 epis√≥dios de treinamento, temos uma pol√≠tica mais est√°vel, mesmo que ainda n√£o √≥tima no caso m√©dio. √â poss√≠vel v√™-la na imagem abaixo:\n",
        "\n",
        "![Politica da sala 2 com 36 epis√≥dios](figs/politicaFinalSala2ep36.png)\n",
        "\n",
        " O modelo n√£o conseguiu convergir para a pol√≠tica √≥tima, por√©m consegue encontrar um caminho v√°lido. Isso pode ser corrigido com mais epis√≥dios ou por altera√ß√£o do valor do refor√ßo final, uma vez que a adi√ß√£o de um √∫nico caminho a mais (refor√ßo somado em -1) n√£o tem muito valor quando comparado com o total ganho no final (100).\n",
        "\n",
        "Ap√≥s um processo de busca bin√°ria no n√∫mero de epis√≥dios, foi poss√≠vel encontrar o valor de 130 como um valor em que todas as pol√≠ticas geradas para 20 amostras foram √≥timas.\n",
        "\n",
        "![Politica final da sala 2](figs/politicaFinalSala2.png)\n",
        "\n",
        "Em ambientes maiores o algoritmo demora mais para chegar em uma pol√≠tica onde o menor caminho √© alcan√ßado, ainda mais quando h√° diferentes possibilidades de se chegar no objetivo.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  <b style=\"color:#22BBBB\">Sala 4</b>\n",
        "\n",
        "Aqui a sala √© bem maior mas ainda completamente ampla\n",
        "\n",
        "![Sala 4](figs/sala%204.png)\n",
        "\n",
        "Nesse cen√°rio, foi necess√°rio a altera√ß√£o do limite de passos, uma vez que mesmo com a pol√≠tica √≥tima, seria imposs√≠vel que o agente iniciado no canto inferior esquerdo atingisse o estado terminal. Sendo assim ele passou a ser dependente da dimens√£o do mapa, valendo o n√∫mero total de quadrados da sala, neste caso 400. Com isso, ap√≥s 30000 epis√≥dios a pol√≠tica convergiu para uma pol√≠tica pr√≥xima da √≥tima:\n",
        "\n",
        "![Politica final da sala 4](figs/politicaFinalSala4.png)\n",
        "\n",
        "Aqui √© poss√≠vel verificar que n√£o foram todos os campos com a pol√≠tica boa, mas a adi√ß√£o de mais epis√≥dios ao treinamento faz com que o processo seja muito mais longo.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  <b style=\"color:#22BBBB\">Sala 3</b>\n",
        "\n",
        "Aqui a sala possui obst√°culos no meio do caminho, tornando mais complexa a identifica√ß√£o da pol√≠tica √≥tima. Al√©m disso, o final do caminho (pr√≥ximo ao objetivo) √© estreito, o que pode dificultar a aprendizagem.\n",
        "\n",
        "![Sala 3](figs/sala%203.png)\n",
        "\n",
        "Aqui um resultado peculiar ocorreu:\n",
        "\n",
        "![Politica final da sala 3](figs/politicaFinalSala3ep100.png)\n",
        "\n",
        "Foi verificado que a pol√≠tica √≥tima precisa evoluir a partir do objetivo. O que significa que salas grandes com corredores estreitos podem custar muito para o agente aprender o caminho. Para melhorar o treinamento do agente, foi adicionado a possibilidade dele n√£o seguir a pol√≠tica e experimentar uma nova possibilidade aleat√≥ria. Assim ele n√£o fica t√£o restrito √† posi√ß√£o inicial nem √† pol√≠tica aleat√≥ria inicial. Dessa forma foram necess√°rios 10000 epis√≥dios para converg√™ncia da pol√≠tica. Mas uma nova caracter√≠stica foi verificada: Com essa taxa de explora√ß√£o fixa, o agente esquecia da pol√≠tica aprendida em algumas regi√µes. Para resolver isso, a taxa de explora√ß√£o passou a decair de acordo com o n√∫mero de epis√≥dios, sendo multiplicada por 0.999 todo final de epis√≥dio. Ao final foi produzida a seguinte pol√≠tica:\n",
        "\n",
        "![Politica final da sala 3](figs/politicaFinalSala3.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  <b style=\"color:#22BBBB\">Salas 5 7 e 8</b>\n",
        "Mais salas foram adicionadas, aqui a sala 5:\n",
        "\n",
        "![Sala 5](figs/sala%205.png)\n",
        "\n",
        "Foram necess√°rios 30000 epis√≥dios para atingir a pol√≠tica √≥tima:\n",
        "\n",
        "![Politica final da sala 5](figs/politicaFinalSala5.png)\n",
        "\n",
        "Enquanto para a sala 7:\n",
        "\n",
        "![Sala 7](figs/sala%207.png)\n",
        "\n",
        "Nessa sala 30000 epis√≥dios geraram a seguinte pol√≠tica:\n",
        "\n",
        "![Politica final da sala 7](figs/politicaFinalSala7ep30000.png)\n",
        "\n",
        "Para a √∫ltima sala, a 8, temos:\n",
        "\n",
        "![Sala 8](figs/sala%208.png)\n",
        "\n",
        "O treinamento com 1000 epis√≥dios foi bem mais longo em compara√ß√£o com os demais mapas, isto era esperado j√° que o limite de passos para cada epis√≥dio cresce de acordo com as dimens√µes do mapa.\n",
        "\n",
        "![Politica final da sala 8](figs/politicaFinalSala8.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <b style=\"color:#009900\">An√°lise do Monte Carlo em ambiente estoc√°stico</b>\n",
        "A estoc√°sticidade do ambiente √© dado por um valor racional de 0 a 1 e simboliza a chance de determinada a√ß√£o trocar para uma outra a√ß√£o aleat√≥ria. Para s = 0.33 significa que h√° 33% de chance da a√ß√£o escolhida (ir para a direita por exemplo), trocar para outra a√ß√£o aleat√≥ria (ir para cima por exemplo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  <b style=\"color:#22BBBB\">Sala 1</b>\n",
        "\n",
        "Para uma estocasticidade de 0.1, nenhuma altera√ß√£o significativa foi percebida, o ambiente se comporta da mesma forma, levando o mesmo n√∫mero de epis√≥dios para convergir para a pol√≠tica √≥tima. Com o aumento do n√∫mero de √©pocas, nenhuma mudan√ßa na pol√≠tica foi verificada.\n",
        "\n",
        "Para um valor de 0.5, em metade das situa√ß√µes a pol√≠tica n√£o conseguiu convergir para a √≥tima:\n",
        "\n",
        "![Politica final da sala 1 estoc√°stica](figs/politicaFinalSala1Esto.png)\n",
        "\n",
        "Para corrigir esse comportamento, a quantidade de epis√≥dios aumentou para 100, ainda n√£o sendo o suficiente, e posteriormente para 1000, finalmente convergindo para a pol√≠tica √≥tima.\n",
        "\n",
        "Para uma estocasticidade de 0.75, 1000 epis√≥dios n√£o foram o suficiente para atingir a pol√≠tica √≥tima. Nesse ambiente, se em algum momento a pol√≠tica converge para uma dire√ß√£o inv√°lida, esse valor n√£o √© alterado, uma vez que a chance de explora√ß√£o somada a estocasticidade do ambiente fazem com que os movimentos sejam quase totalmente independentes da pol√≠tica, dificultando o aprendizado.\n",
        "\n",
        "Como esperado, para um valor igual a 1, nenhuma pol√≠tica √© derivada do aprendizado, uma vez que n√£o importa a decis√£o do agente, a a√ß√£o ser√° completamente aleat√≥ria.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  <b style=\"color:#22BBBB\">Sala 2</b>\n",
        "\n",
        "A sala 2 foi avaliada com a estocasticidade de 0.1, o que n√£o gerou nenhuma mudan√ßa significativa no treinamento do agente. J√° para um valor de 0.5, foram necess√°rios 1000 epis√≥dios para obter uma pol√≠tica aceit√°vel (com um estado errado ao m√°ximo), e para 0.75 foi necess√°rio 2000 epis√≥dios para convergir para a pol√≠tica √≥tima.\n",
        "\n",
        "Na sala 3 e estocasticidade 0.1, o modelo foi treinado com 10000 epis√≥dios gerando a pol√≠tica abaixo\n",
        "\n",
        "![Politica final da sala 3 estoc√°stica](figs/politicaFinalSala3sto01e1000.png)\n",
        "\n",
        "Aqui podemos ver que mesmo com uma estocasticidade baixa, o agente n√£o consegue aprender a pol√≠tica ideal, possivelmente por que para um caminho maior, esse valor se torna bem mais relevante. Para atingir uma qualidade aceit√°vel da pol√≠tica, foram necess√°rios 160000 epis√≥dios.\n",
        "Para estocasticidade de 0.5, o modelo deixou de convergir, encontrando pol√≠ticas sem sentido mesmo para 320000 epis√≥dios\n",
        "\n",
        "![Politica final da sala 3 estoc√°stica](figs/politicaFinalSala3sto05ep320000.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <b style=\"color:#009900\">Aproximador linear</b>\n",
        "\n",
        "Para conseguir representar os estados de nosso ambiente, diversas m√©tricas foram testadas, entre elas:\n",
        "1. Dist√¢ncia do eixo X e Y do agente at√© o objetivo;\n",
        "2. Presen√ßa de paredes na vizinhan√ßa-4\n",
        "3. Binariza√ß√£o dos estados de entrada\n",
        "\n",
        "Por√©m as caracter√≠sticas que mais deram certo em nosso ambiente foram a varia√ß√£o dos par√¢metros em fun√ß√µes como x¬≤, xy, y¬≤, y¬≥, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <b style=\"color:#009900\">M√©tricas</b>\n",
        "\n",
        "\n",
        "Nessa se√ß√£o ser√° exposto diversas tabelas com as seguintes m√©tricas: tempo, taxa, recompensa, passos. Tempo se refere a quantidade de tempo em segundos que demorou o treinamento. A taxa se refere √† quantidade de pontos iniciais marcados no mapa que obtiveram sucesso (alcan√ßaram o objetivo), dado a pol√≠tica treinada. A recompensa √© a m√©dia das recompensas finais de cada caminho marcado no mapa. Por fim temos a quantidade de passos do agente at√© o fim de cada epis√≥dio. Todas as m√©tricas foram extra√≠das a partir dos treinos e testes usando a seed 42. \n",
        "\n",
        "As salas usadas para obter as m√©tricas abaixo foram 3.1, 3.2, 5 e A. Sendo elas:\n",
        "\n",
        "Sala 3.1:\n",
        "\n",
        "![sala 3.1](figs/sala3.1.png)\n",
        "\n",
        "Sala 3.2:\n",
        "\n",
        "![sala 3.1](figs/sala3.2.png)\n",
        "\n",
        "Sala 5:\n",
        "\n",
        "![sala 3.1](figs/sala5.png)\n",
        "\n",
        "Sala A:\n",
        "\n",
        "![sala 3.1](figs/salaA.png)\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte Carlo sem aproximador linear e em ambiente determin√≠stico, 1000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo (s) |taxa (%) |recompensa|passos |\n",
        "|-----------------|------|-----|----------|-------|\n",
        "|3.1|0.111 |100.0|92.35     |7.65   |\n",
        "|3.2|0.076 |100.0|95.08     |4.92   |\n",
        "|5  |1.142 |66.66|-74.0    |140.36 |\n",
        "|A |0.79 |37.5|-221.0     |256.75  |\n",
        "\n",
        "Aqui √© poss√≠vel verificar que o modelo consegue aprender com 1000 epis√≥dios muito bem as salas pequenas ou muito simples, como as salas 1,2,3.1,3.2 4 e 6, al√©m de salas completamente vazias. Ao mesmo tempo, a sala 3 levou mais tempo para ser aprendida, como j√° discutido nas se√ß√µes anteriores.\n",
        "\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte Carlo sem aproximador linear e em ambiente determin√≠stico, 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo (s)  |taxa (%) |recompensa|passos |\n",
        "|-----------------|-------|-----|----------|-------|\n",
        "|3.1|0.823  |100.0|92.7      |7.3    |\n",
        "|3.2|0.521  |100.0|95.42     |4.58   |\n",
        "|5  |9.218  |35.71|-224.19   |259.26 |\n",
        "|A |4.353  |87.5|16.0     |66.875   |\n",
        "\n",
        "\n",
        "Aqui √© poss√≠vel verificar uma melhora consider√°vel com o aumento dos epis√≥dios, mas salas grandes ainda n√£o possuem uma taxa aceit√°vel. A sala 3 ainda n√£o convergiu para uma pol√≠tica boa, possivelmente por conta da taxa de explora√ß√£o zerada, o que obriga o caminho a iniciar com as escolhas em uma posi√ß√£o ao lado da pr√© calculada, o que diminui muito a velocidade de converg√™ncia.\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte Carlo sem aproximador linear e em ambiente determin√≠stico, 100000 epis√≥dios</b>\n",
        "\n",
        "|sala|tempo (s)   |taxa (%) |recompensa|passos |\n",
        "|----|--------|-----|----------|-------|\n",
        "|3.1|8.976   |100.0|92.7      |7.3    |\n",
        "|3.2|6.324   |100.0|95.42     |4.58   |\n",
        "|5  |39.406  |100.33|87.3      |12.61   |\n",
        "|A |38.189 |100.0|87.3     |12.61   |\n",
        "\n",
        "Aqui podemos verificar o ganho de efici√™ncia do algoritmo com o aumento do n√∫mero de epis√≥dios, por√©m houve o aumento do custo computacional. O valor de 10000 epis√≥dios apresenta um bom custo benef√≠cio e ser√° mantido para os outros testes.\n",
        "Para a sala 3, a taxa de sucesso est√° baixa, isso deve aumentar com a taxa de explora√ß√£o.\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte Carlo sem aproximador linear e em ambiente estoc√°stico, 0.1 , 10000 epis√≥dios</b>\n",
        "\n",
        "\n",
        "|sala             |tempo (s)   |taxa (%) |recompensa|passos |\n",
        "|-----------------|--------|-----|----------|-------|\n",
        "|3.1|0.778   |100.0|92.26     |7.74   |\n",
        "|3.2|0.565   |100.0|94.29     |5.71   |\n",
        "|5  |7.895   |100.0|87.3   |174.31 |\n",
        "|A |7.04   |100.0|79.27     |322.25 |\n",
        "\n",
        "O ambiente estoc√°stico de 0.1 n√£o foi suficiente para diminuir a qualidade do modelo nas salas 9, 10, 11 e 12.\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte Carlo sem aproximador linear e em ambiente estoc√°stico, 0.5 , 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo (s)   |taxa (%) |recompensa|passos |\n",
        "|-----------------|--------|-----|----------|-------|\n",
        "|3.1|4.015   |100.0|80.61     |19.39  |\n",
        "|3.2|1.061   |100.0|91.25     |8.75   |\n",
        "|5  |7.088   |100.0|79.26    |162.43 |\n",
        "|A |8.63 |83.33|77.57     |352.88   |\n",
        "\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte Carlo sem aproximador e em ambiente estoc√°stico, 0.7 , 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo (s)   |taxa (%) |recompensa|passos |\n",
        "|-----------------|--------|-----|----------|-------|\n",
        "|3.1|2.992   |78.26|30.04     |48.0   |\n",
        "|3.2|1.649   |100.0|79.54     |20.46  |\n",
        "|5  |11.378  |57.14|-168.4    |225.12 |\n",
        "|A |8.764 |37.5|-289.25     |276.12   |\n",
        "\n",
        "\n",
        "Para simplificar os experimentos, vamos apenas mostrar os resultados que destoam do padr√£o apresentado aqui, que valham a pena serem analisados.\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte Carlo sem aproximador linear e em ambiente determin√≠stico, com chance de explora√ß√£o 0.3, 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo (s) |taxa (%) |recompensa|passos |\n",
        "|-----------------|------|-----|----------|-------|\n",
        "|3.1|0.749 |100.0|92.7      |7.3    |\n",
        "|3.2|0.536 |100.0|95.42     |4.58   |\n",
        "|5  |1.404 |100.0|87.29     |12.71  |\n",
        "|A |2.505 |75.0|-42.0     |113.38   |\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte Carlo sem aproximador linear e em ambiente determin√≠stico, com chance de explora√ß√£o 0.5, 10000 epis√≥dios</b>\n",
        "\n",
        "\n",
        "|sala             |tempo (s) |taxa (%) |recompensa|passos |\n",
        "|-----------------|------|-----|----------|-------|\n",
        "|3.1|0.99  |100.0|92.7      |7.3    |\n",
        "|3.2|0.681 |100.0|95.42     |4.58   |\n",
        "|5  |2.118 |95.24|64.5      |30.69  |\n",
        "|A |2.934 |50.0|-159.88     |208.25   |\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte Carlo com aproximador linear e em ambiente determin√≠stico, com chance de explora√ß√£o 0.3, 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo (s) |taxa (%) |recompensa|passos |\n",
        "|-----------------|------|-----|----------|-------|\n",
        "|3.1|7.881 |100.0|92.7      |7.3    |\n",
        "|3.2|11.568|8.33 |-84.5     |91.92  |\n",
        "|5  |32.211|2.38 |-389.1    |390.5  |\n",
        "|A |5.649 |12.5|-339.75     |351.38   |\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Sarsa sem aproximador linear e em ambiente estoc√°stico 0.5, com chance de explora√ß√£o 0.3, 10000 epis√≥dios</b>\n",
        "\n",
        "\n",
        "\n",
        "|sala             |tempo (s) |taxa (%) |recompensa|passos |\n",
        "|-----------------|------|-----|----------|-------|\n",
        "|3.1|4.352 |100.0|83.91     |16.09  |\n",
        "|3.2|1.699 |100.0|91.21     |8.79   |\n",
        "|5  |109.771|76.19|-65.76    |141.71 |\n",
        "|A |91.132 |50.0|-259.62     |256.88   |\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Sarsa com aproximador linear e em ambiente estoc√°stico 0.5, com chance de explora√ß√£o 0.3, 1000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo (s) |taxa (%) |recompensa|passos |\n",
        "|-----------------|------|-----|----------|-------|\n",
        "|3.1|8.192 |39.13|-36.43    |74.96  |\n",
        "|3.2|11.044|45.83|-23.04    |68.33  |\n",
        "|5  |116.541|9.52 |-353.88   |362.5  |\n",
        "|A |68.112 |50.0|-379.0     |328.5   |\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Q-Learning sem aproximador linear e em ambiente estoc√°stico 0.5, com chance de explora√ß√£o 0.3, 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo (s)|taxa (%) |recompensa|passos|\n",
        "|-----------------|-----|-----|----------|------|\n",
        "|3.1|1.9  |100.0|83.78     |16.22 |\n",
        "|3.2|1.025|100.0|90.17     |9.83  |\n",
        "|5  |11.829|64.29|-128.6    |192.52|\n",
        "|A |10.448 |75.0|-167.88     |188.12   |\n",
        "\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Q-Learning com aproximador linear e em ambiente estoc√°stico 0.5, com chance de explora√ß√£o 0.3, 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo (s)|taxa (%) |recompensa|passos|\n",
        "|-----------------|-----|-----|----------|------|\n",
        "|3.1|1.545|65.22|12.39     |52.48 |\n",
        "|3.2|2.067|37.5 |-39.29    |76.17 |\n",
        "|5  |9.706|7.14 |-365.52   |371.74|\n",
        "|A |62.505 |12.5|-375.0     |361.62   |\n",
        "\n",
        "√â importante ressaltar que nenhum modelo performou bem na sala 8, a maior sala dispon√≠vel, uma vez que n√£o foi poss√≠vel apresentar todos os estados para os modelos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como forma de verificar a recompensa dos algoritmos com o passar do tempo, foi constru√≠da uma sala gen√©rica com diversos refor√ßos positivos e negativos, v√°rios estados finais e um tamanho grande o suficiente para explorar o \"medo\" do agente e estruturas de corredores.\n",
        "\n",
        "O treinamento ocorreu com uma chance de explora√ß√£o 0.3, estrat√©gia do Monte Carlo em um ambiente com estocasticidade 0.1. Vale lembrar que o custo da lava √© -100 sendo um estado terminal e o √°cido com custo de -10, n√£o sendo um estado terminal.\n",
        "\n",
        "![Sala A](figs/salaA.png)\n",
        "\n",
        "Ap√≥s o treinamento, temos a pol√≠tica final:\n",
        "\n",
        "![Politica final da sala A estoc√°stica](figs/politicaFinalSalaAsto01ep50000.png)\n",
        "\n",
        "O treinamento levou 14 segundos e a recompensa m√©dia foi de 70, com m√©dia de 24 passos.\n",
        "\n",
        "O gr√°fico abaixo apresenta a curva de aprendizagem do algoritmo dado a configura√ß√£o descrita acima:\n",
        "\n",
        "![Recompensa no tempo Monte Carlo](figs/reward/rewardSalaAep50000.png)\n",
        "\n",
        "Aqui √© poss√≠vel verificar que o modelo convergiu rapidamente para a pol√≠tica √≥tima. Vale ressaltar que o gr√°fico foi constru√≠do atrav√©s de uma m√©dia m√≥vel de 10% das amostras, ou seja, 5000 amostras.\n",
        "\n",
        "\n",
        "Um resultado estranho foi encontrado para o Sarsa: O treinamento atinge um m√°ximo e decai rapidamente perdendo informa√ß√£o:\n",
        "\n",
        "![Recompensa no tempoSarsa ](figs/reward/rewardSalaAep50000Sarsa.png)\n",
        "\n",
        "\n",
        "A pol√≠tica final nesse caso √© \n",
        "\n",
        "![Recompensa no tempoSarsa ](figs/Sarsa50000.png)\n",
        "\n",
        "\n",
        "Treinando por 10000 epis√≥dios temos:\n",
        "\n",
        "\n",
        "![Recompensa no tempoSarsa ](figs/reward/rewardSalaAep10000Sarsa.png)\n",
        "\n",
        "E o resultado da pol√≠tica:\n",
        "\n",
        "![Recompensa no tempoSarsa ](figs/Sarsa10000.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analisando agora o parametro lambda para o Sarsa, ele ser√° variado de 0.01, 0.1 e 0.5, com 10000 epis√≥dios cada.\n",
        "Essa quantidade de epis√≥dios foi escolhida por que o modelo sofre de overflow caso o n√∫mero de epis√≥dios seja muito grande para lambda = 0.5.\n",
        "\n",
        "para lambda = 0.01, temos:\n",
        "![Recompensa no tempoSarsa ](figs/reward/Sarsa001.png)\n",
        "\n",
        "para lambda = 0.1, temos:\n",
        "![Recompensa no tempoSarsa ](figs/reward/Sarsa01.png)\n",
        "\n",
        "para lambda = 0.5, temos:\n",
        "![Recompensa no tempoSarsa ](figs/reward/Sarsa05.png)\n",
        "\n",
        "Podemos verficar que quanto maior o valor de lambda, mais rapidamente o modelo converge para a pol√≠tica √≥tima, mas √© necess√°rio balancear esse valor com o n√∫mero de epis√≥dios, uma vez que o modelo pode sofrer overflow e convergir para uma pol√≠tica em que todos os estados tem valores infinitos.\n",
        "\n",
        "|lambda             |tempo |taxa |recompensa|passos |\n",
        "|-----------------|------|-----|----------|-------|\n",
        "|0.01  |213.78  |37.5|-333.62     |295.5    |\n",
        "|0.1  |209.73 |100.0|-370.37     |295.5   |\n",
        "|0.5  |126.73 |87.5|-118.25    |119.625   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analisando agora os valores dos estados para 6 configura√ß√µes de algoritmos, temos:\n",
        "\n",
        "Monte Carlo sem aproximador linear, 10000 epis√≥dios, chance de explora√ß√£o 0.5\n",
        "\n",
        "![Valores dos estados para o Monte Carlo](figs/AmonteCarlo.png)\n",
        "\n",
        "Monte Carlo com aproximador linear, 10000 epis√≥dios, chance de explora√ß√£o 0.5\n",
        "\n",
        "![Valores dos estados para o Monte Carlo](figs/AmonteCarloAprox.png)\n",
        "\n",
        "Sarsa sem aproximador linear, 10000 epis√≥dios, chance de explora√ß√£o 0.5, lambda 0.5\n",
        "\n",
        "![Valores dos estados para o Sarsa](figs/ASarsa.png)\n",
        "\n",
        "Sarsa com aproximador linear, 10000 epis√≥dios, chance de explora√ß√£o 0.5, lambda 0.5\n",
        "\n",
        "![Valores dos estados para o Sarsa](figs/ASarsaAprox.png)\n",
        "\n",
        "Q-Learning sem aproximador linear, 10000 epis√≥dios, chance de explora√ß√£o 0.5\n",
        "\n",
        "![Valores dos estados para o Q-Learning](figs/AQLearning.png)\n",
        "\n",
        "Q-Learning com aproximador linear, 10000 epis√≥dios, chance de explora√ß√£o 0.5\n",
        "\n",
        "![Valores dos estados para o Q-Learning](figs/AQLearningAprox.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <b style=\"color:#009900\">DQN</b>\n",
        "\n",
        "A DQN substitui a necessidade de usar tabelas para calcular o valor de Q e V, e diferentemente do aproximador linear, consegue aprender fun√ß√µes n√£o-lineares para representar seu conhecimento. Infelizmente n√£o houve tempo h√°bil para reimplementar ou adaptar o nosso ambiente usado at√© ent√£o em nossos testes. Por isso foi buscado um ambiente no [gymlibrary](https://www.gymlibrary.dev) mais an√°logo poss√≠vel ao nosso. \n",
        "\n",
        "Optamos pelos [Frozen Lake](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/). Nele o agente pode se mover nas quatros dire√ß√µes cardeais. H√° dois estados terminais, os buracos com recompensa 0, e o objetivo final com recompensa 1. A estocasticidade pode ser habilitada para que haja ‚Öì de chance do agente n√£o se mover na dire√ß√£o escolhida pela pol√≠tica, e ‚Äúdeslizar‚Äù para outro estado. A escolha desse ambiente foi interessante, pois assim tamb√©m mudamos a fun√ß√£o de recompensa, podendo ser observado nos resultado sua influ√™ncia na hora de resolver o problema.\n",
        "\n",
        "A DQN foi treinada em um mapa 8x8, variando a quantidade de epis√≥dios [100, 1000, 10000], com e sem estocasticidade. Os gr√°ficos abaixo representam a curva de aprendizado dos algoritmos com 1000 e 10000 epis√≥dios. Com apenas 100 epis√≥dios a DQN n√£o foi capaz de aprender nada em nenhum dos casos. J√° com 1000 epis√≥dios a curva come√ßa a ficar interessante para o caso estoc√°stico, por√©m ainda muito provavelmente influenciado pela natureza aleat√≥ria do ambiente. Com 10000 epis√≥dios √© poss√≠vel ver que o algoritmo conseguiu convergir muito antes. Para a mesma quantidade de epis√≥dios no mundo estoc√°stico, o algoritmo convergiu cedo, por√©m devido a aleatoriedade, ele oscilou bastante em aprender e desaprender a sua pol√≠tica.\n",
        "\n",
        "Gr√°fico de aprendizagem DQN com 1000 epis√≥dios e sem estocasticidade:\n",
        "\n",
        "![Gr√°fico de aprendizagem DQN com 1000 epis√≥dios e sem estocasticidade](dqn_results/training_1000_False.png)\n",
        "Gr√°fico de aprendizagem DQN com 1000 epis√≥dios e sem estocasticidade\n",
        "\n",
        "\n",
        "Gr√°fico de aprendizagem DQN com 1000 epis√≥dios e com estocasticidade:\n",
        "![Gr√°fico de aprendizagem DQN com 1000 epis√≥dios e com estocasticidade](dqn_results/training_1000_True.png)\n",
        "\n",
        "\n",
        "Gr√°fico de aprendizagem DQN com 10000 epis√≥dios e sem estocasticidade:\n",
        "![Gr√°fico de aprendizagem DQN com 10000 epis√≥dios e sem estocasticidade](dqn_results/training_10000_False.png)\n",
        "\n",
        "\n",
        "Gr√°fico de aprendizagem DQN com 10000 epis√≥dios e com estocasticidade:\n",
        "![Gr√°fico de aprendizagem DQN com 10000 epis√≥dios e com estocasticidade](dqn_results/training_10000_True.png)\n",
        "\n",
        "Por fim temos a ilustra√ß√£o da pol√≠tica final obtida durante o treinamento. Primeiramente a pol√≠tica do ambiente determin√≠stico, onde temos que todos os caminhos levam para o objetivo final. Por√©m, diferentemente da pol√≠tica derivada pelos demais algoritmos no ambiente anterior, a falta de um desconto temporal fez com que n√£o fosse uma preocupa√ß√£o encontrar o menor caminho at√© o objetivo.\n",
        "\n",
        "![Gr√°fico de aprendizagem DQN com 10000 epis√≥dios e com estocasticidade](dqn_results/10000_politica_False.jpeg)\n",
        "\n",
        "J√° no caso estoc√°stico, observamos um comportamento parecido com os demais algoritmos, onde o agente aprender a ter ‚Äúmedo‚Äù de passar perto dos estados terminais que n√£o d√£o uma recompensa negativa.\n",
        "\n",
        "![Gr√°fico de aprendizagem DQN com 10000 epis√≥dios e com estocasticidade](dqn_results/10000_politica_True.jpeg)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
