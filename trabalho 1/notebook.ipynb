{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Projeto 1\n",
        "\n",
        "Este trabalho compara a efici√™ncia e efic√°cia de m√∫ltiplos algoritmos de Aprendizado por Refor√ßo (RL), sendo eles MonteCarlo, Sarsa(Lambda) e Q_learning, em diferentes cen√°rios e variando seus par√¢metros quando necess√°rio. O ambiente/problema que foi usado como teste s√£o labirintos, onde os algoritmos precisam encontrar uma politica √≥tima para que o agente consiga chegar at√© o final do labirinto.\n",
        "\n",
        "### Ambiente\n",
        "Como dito anteriormente, os ambientes s√£o labirintos. Eles s√£o descritos por meio de arquivos .txt, onde s√£o apontados as paredes, caminhos, estados terminais, formato e suas recompensas. Os arquivos t√™m essa cara:\n",
        "```\n",
        "4 4\n",
        ". path -1\n",
        "@ agent -1\n",
        "$ goal 100\n",
        "# wall -1000\n",
        "######\n",
        "#...$#\n",
        "#@...#\n",
        "######\n",
        "```\n",
        "\n",
        "A primeira linha cont√©m dois inteiros E e L, que representam o n√∫mero de elementos e o n√∫mero de linhas do labirinto. Aqui s√£o descritos 4 elementos e ser√° assado um labirinto de 4 linhas. Para os elementos tem-se que os caminhos s√£o '.' e possuem recompensa de -1. Por sua vez, o estado terminal, ou objetivo, √© representado por '$' e possui recompensa de 100. Por fim, ap√≥s a descri√ß√£o dos simbolos, h√° a descri√ß√£o do pr√≥prio formato do labirinto.\n",
        "\n",
        "Diferentes formatos de labirintos foram adotados para o trabalho, sendo alguns bem amplos, outros com diversos caminhos, e outros extremamente estreitos. As recompensas tamb√©m foram alteradas para avaliar seu impacto, j√° que a escolha de uma fun√ß√£o de recompensa boa tamb√©m faz parte do processo de treinamento quando se trabalha com RL.\n",
        "\n",
        "### C√≥digo\n",
        "As primeiras c√©lulas desse notebook cont√©m o c√≥digo usado para implementa√ß√£o e obten√ß√£o dos resultados. Por√©m para facilitar, foram geradas imagens com os resultados obtidos (usando pygame), e a an√°lise foi feita em cima delas. O c√≥digo est√° dispon√≠vel para execu√ß√£o, altera√ß√£o, e teste de outras configura√ß√µes de labirinto.\n",
        "\n",
        "### M√©tricas\n",
        "Para compara√ß√£o dos algoritmos entre si, foram extra√≠das as recompensas m√©dias e o tamanho do caminho percorrido a partir de um mesmo ponto do mapa definidos na cria√ß√£o do mapa pelo valor 'agent', nesse caso '@'. Com isso √© poss√≠vel analisar o qu√£o eficiente foi o caminho encontrado pelo algoritmo, caso encontrado, e se ele conseguiu convergir. H√° tamb√©m a analise de desempenho e custo computacional, o <b style=\"color:#770000\">n√∫mero m√©dio de epis√≥dios para converg√™ncia</b>, o <b style=\"color:#770000\">tempo de converg√™ncia</b>, os <b style=\"color:#770000\">hiperparametros</b> necess√°rios para ajustar o algoritmo, e o qu√£o <b style=\"color:#770000\">dificil</b> foi encontrar um valor √≥timo.\n",
        "\n",
        "Al√©m disso, um mapa de calor gerado a partir da fun√ß√£o valor ser√° usado para ilustrar o qu√£o bom √© estar em cada estado, ou seja, em cada posi√ß√£o do labirinto. tamb√©m ser√° mostrado um gr√°fico contendo as recompensas do modelo ao longo do tempo, para que seja poss√≠vel analisar o qu√£o r√°pido o algoritmo convergiu. Esse gr√°fico utiliza do recurso da m√©dia m√≥vel para suavizar os dados e facilitar a visualiza√ß√£o e compreens√£o.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# C√≥digos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "lftFAb-BqMew"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from itertools import product\n",
        "import random\n",
        "from threading import Thread\n",
        "import time\n",
        "from argparse import ArgumentParser\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import pygame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OwSgimmVxFIw"
      },
      "outputs": [],
      "source": [
        "class Renderer():\n",
        "    def __init__(self, chief, content, title=None, dimensions=(800, 800)):\n",
        "        self.chief = chief\n",
        "        self.content = content\n",
        "        self.contents = [content] # para trocar entre janelas\n",
        "        self.iConteudoAtual = 0 # para marcar qual o atual dentre os varios\n",
        "        self.title = title\n",
        "        self.dimensions = dimensions\n",
        "        self.running = True\n",
        "        if not self.title:\n",
        "            self.title = type(chief).__name__   # title √© o nome da classe\n",
        "        \n",
        "        \n",
        "        # redimensiona o ambiente para caber na tela\n",
        "        self.tamanhosprite = 64\n",
        "        self.escala = (self.dimensions[0]/len(self.content[0]), self.dimensions[1]/len(self.content))\n",
        "        while self.escala[0] < self.tamanhosprite//8 or self.escala[1] < self.tamanhosprite//8: # redimensiona pra pp\n",
        "            self.dimensions =(int(self.dimensions[0] *1.1), int(self.dimensions[1]*1.1))\n",
        "            self.escala = (self.dimensions[0]/len(self.content[0]), self.dimensions[1]/len(self.content))\n",
        "\n",
        "        self.load_sprites()\n",
        "\n",
        "        # cria uma thread que roda o pygame\n",
        "        displayer = Thread(target=self.show) # shower = mostrador\n",
        "\n",
        "        # Inicia a thread\n",
        "        displayer.start()\n",
        "\n",
        "    def addConteudo(self,conteudo):\n",
        "        self.contents.append(conteudo)\n",
        "\n",
        "    def desligar(self):\n",
        "        self.running = False\n",
        "\n",
        "    def load_sprites(self):\n",
        "        self.sprites = dict()\n",
        "        self.sprites[\"path\"] = pygame.transform.scale(pygame.image.load(\"imgs/path.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"wall\"] = pygame.transform.scale(pygame.image.load(\"imgs/wall.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"goal\"] = pygame.transform.scale(pygame.image.load(\"imgs/goal.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"cookie\"] = pygame.transform.scale(pygame.image.load(\"imgs/cookie.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"agent\"] = pygame.transform.scale(pygame.image.load(\"imgs/agent.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"right\"] = pygame.transform.scale(pygame.image.load(\"imgs/right.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"up\"] = pygame.transform.scale(pygame.image.load(\"imgs/up.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"left\"] = pygame.transform.scale(pygame.image.load(\"imgs/left.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"down\"] = pygame.transform.scale(pygame.image.load(\"imgs/down.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"acid\"] = pygame.transform.scale(pygame.image.load(\"imgs/acid.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "        self.sprites[\"lava\"] = pygame.transform.scale(pygame.image.load(\"imgs/lava.png\"), (int(self.escala[0]), int(self.escala[1])))\n",
        "\n",
        "\n",
        "        self.asciiSprites = dict()\n",
        "        self.asciiSprites[\"path\"]   = '‚¨õ'\n",
        "        self.asciiSprites[\"wall\"]   = 'üß±'\n",
        "        self.asciiSprites[\"goal\"]   = '‚öΩ'\n",
        "        self.asciiSprites[\"cookie\"]   = 'üç™'\n",
        "        self.asciiSprites[\"agent\"]  = 'üëæ' \n",
        "        self.asciiSprites[\"right\"]  = '->' #'‚û°Ô∏è'\n",
        "        self.asciiSprites[\"up\"]     = '‚¨ÜÔ∏è‚¨ÜÔ∏è'\n",
        "        self.asciiSprites[\"left\"]   = '<-' \n",
        "        self.asciiSprites[\"down\"]   = '‚¨áÔ∏è‚¨áÔ∏è'\n",
        "        self.asciiSprites[\"lava\"]   = 'üåã'\n",
        "        self.asciiSprites[\"acid\"]   = 'ü¶†'\n",
        "\n",
        "    def create_heatmap(data, cmap='viridis', title='Heatmap'):\n",
        "        \"\"\"\n",
        "        Create a heatmap from a list of lists of floats.\n",
        "\n",
        "        Parameters:\n",
        "        - data: List of lists of floats representing the heatmap data.\n",
        "        - cmap: Colormap for the heatmap (default is 'viridis').\n",
        "        - title: Title for the heatmap (default is 'Heatmap').\n",
        "        \"\"\"\n",
        "        data = np.array(data, dtype=float)\n",
        "\n",
        "        # Create a figure and axis\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        # Display the heatmap using imshow\n",
        "        im = ax.imshow(data, cmap=cmap)\n",
        "\n",
        "        # Add a colorbar to the right of the heatmap\n",
        "        cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "        # Set the title\n",
        "        ax.set_title(title)\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n",
        "\n",
        "    def showAscii(self):\n",
        "        for i in range(len(self.content)):\n",
        "            for j in range(len(self.content[0])):\n",
        "                cell = self.content[i][j]\n",
        "                # se o content de cell estiver no dicionario de sprites\n",
        "                if cell in self.asciiSprites:\n",
        "                    obj = cell\n",
        "                else:\n",
        "                    obj = self.chief.symbols[cell]\n",
        "                print(self.asciiSprites.get(obj,'‚ùå'),end='')\n",
        "            print('')\n",
        "\n",
        "    def show(self):\n",
        "        # renderiza o ambiente\n",
        "        pygame.init()\n",
        "        self.screen = pygame.display.set_mode(self.dimensions)\n",
        "        pygame.display.set_caption(self.title)\n",
        "        self.screen.fill((0, 0, 0))\n",
        "\n",
        "        try:\n",
        "            while self.running:\n",
        "                pygame.time.delay(10)  # delay de 10ms\n",
        "                # Botao de fechar\n",
        "                for event in pygame.event.get():\n",
        "                    if event.type == pygame.QUIT:\n",
        "                        pygame.quit()\n",
        "                        self.running = False\n",
        "                    # se apertar \"p\"\n",
        "                    if event.type == pygame.KEYDOWN:\n",
        "                        if event.key == pygame.K_p:\n",
        "                            self.iConteudoAtual = 1\n",
        "                            self.content = self.contents[self.iConteudoAtual]\n",
        "\n",
        "                    if event.type == pygame.KEYUP:\n",
        "                        if event.key == pygame.K_p:\n",
        "                            self.iConteudoAtual = 0\n",
        "                            self.content = self.contents[self.iConteudoAtual]\n",
        "\n",
        "                # limpa a tela\n",
        "                self.screen.fill((0,0,0))\n",
        "                \n",
        "                # desenha o conteudo\n",
        "                for k in range(self.iConteudoAtual+1):\n",
        "                    for i in range(len(self.contents[k])):\n",
        "                        for j in range(len(self.contents[k][0])):\n",
        "                            celula = self.contents[k][i][j]\n",
        "                            # se o conteudo de celula estiver no dicionario de sprites\n",
        "                            if celula in self.sprites:\n",
        "                                objeto = celula\n",
        "                            else:\n",
        "                                objeto = self.chief.symbols[celula]\n",
        "                            self.screen.blit(self.sprites[objeto], (j*self.escala[0], i*self.escala[1]))\n",
        "\n",
        "                # Atualizar a tela\n",
        "                pygame.display.update()\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "\n",
        "    def show_path(self, path):\n",
        "        for i in range(len(self.content)):\n",
        "            for j in range(len(self.content[0])):\n",
        "                cell = self.content[i][j]\n",
        "                if((i,j) in path.keys()):\n",
        "                    cell = path[(i,j)]\n",
        "                # se o content de cell estiver no dicionario de sprites\n",
        "                if cell in self.asciiSprites:\n",
        "                    obj = cell\n",
        "                else:\n",
        "                    obj = self.chief.symbols[cell]\n",
        "                print(self.asciiSprites.get(obj,'‚ùå'),end='')\n",
        "            print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    actions = ['up', 'down', 'left', 'right']\n",
        "    def __init__(self, x, y, environment, gamma = 0.9, display=True):\n",
        "        self.environment = environment\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.gamma = gamma\n",
        "        self.display = display\n",
        "\n",
        "    def action_idx(self, action: str):\n",
        "        return self.actions.index(action)\n",
        "\n",
        "    def startQ(self, shape, start_value = float(\"-inf\")):\n",
        "        \"\"\"\n",
        "        livroQ √© uma lista de listas de dicionarios,\n",
        "        ele armazena \n",
        "        \n",
        "        \"\"\"\n",
        "        self.livro_Q: list[list[dict]] = []\n",
        "        for i in range(shape[0]):\n",
        "            self.livro_Q.append([])\n",
        "            for _ in range(shape[1]):\n",
        "                self.livro_Q[i].append(dict())\n",
        "                for acao in self.actions:\n",
        "                    self.livro_Q[i][-1][acao] = start_value\n",
        "\n",
        "    def startV(self, shape):\n",
        "        \"\"\"\n",
        "        livroV √© uma lista de listas de dicionarios,\n",
        "        ele armazena \n",
        "        \n",
        "        \"\"\" \n",
        "        self.book_V: list[list] = []\n",
        "        for i in range(shape[0]):\n",
        "            self.book_V.append([])\n",
        "            for _ in range(shape[1]):\n",
        "                \n",
        "                self.book_V[i].append(float(\"-inf\"))\n",
        "    \n",
        "    def startPolicy(self, shape, randomPolicy):\n",
        "        \"\"\"\n",
        "        A policy √© uma matriz de caracteres que guarda a acao principal\n",
        "        a ser tomada ate o momento\n",
        "        \n",
        "        \"\"\"\n",
        "        self.policy: list[list[str]] = []\n",
        "        for i in range(shape[0]):\n",
        "            self.policy.append([])\n",
        "            for j in range(shape[1]):\n",
        "                if self.environment.symbols[self.environment.original_map[i][j]] == \"wall\":\n",
        "                    self.policy[i].append(\"wall\")\n",
        "                    continue\n",
        "                if randomPolicy:\n",
        "                    self.policy[i].append(random.choice(self.actions))\n",
        "                else:\n",
        "                    self.policy[i].append(self.actions[0])\n",
        "        \n",
        "        if self.display:\n",
        "            self.render = self.environment.render.addConteudo(self.policy)\n",
        "\n",
        "    def startReturns(self, shape):\n",
        "        \"\"\"\n",
        "        returns √© uma colecao de pares estado acao guardando um\n",
        "        dicionario para armazenar o valor maximo de reforcos obtidos,\n",
        "        o numero de vezes que o par estado acao foi visitado e o ultimo\n",
        "        episodio em que o par estado acao foi visitado\n",
        "        \"\"\"\n",
        "        self.returns: list[list[dict]] = []\n",
        "        for i in range(shape[0]):\n",
        "            self.returns.append([])\n",
        "            for j in range(shape[1]):\n",
        "                self.returns[i].append(dict())\n",
        "                for acao in self.actions:\n",
        "                    self.returns[i][j][acao] = {\"value\": 0, \"count\": 0, \"lastEpisode\": 0}\n",
        "\n",
        "    def setEnvironment(self, environment):\n",
        "        self.environment = environment\n",
        "    \n",
        "    def setPos(self, position):\n",
        "        self.x = position[1]\n",
        "        self.y = position[0]\n",
        "\n",
        "    def move(self, action):\n",
        "        return self.environment.move(self, action)\n",
        "    \n",
        "    def get_action(self):\n",
        "        return self.policy[self.y][self.x]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlGKK60NqTV4",
        "outputId": "d6167f45-bc5f-49b6-8732-de679f17aeec"
      },
      "outputs": [],
      "source": [
        "class LearningStrategy():\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.feature_lenght = 15\n",
        "        self.W = np.random.rand(self.feature_lenght)*2-1\n",
        "\n",
        "    def train(self, episodes):\n",
        "        pass\n",
        "\n",
        "    def get_Q(self, x, y, action, linear_approximation = False):\n",
        "        if not linear_approximation:\n",
        "            return self.Q[x,y,action]\n",
        "        \n",
        "        terms = self.data_to_features(x,y,action)\n",
        "        return np.dot(self.W, terms)\n",
        "\n",
        "    def setup(self, environment, agent):\n",
        "        self.environment: Environment = environment\n",
        "        self.agent: Agent = agent\n",
        "    \n",
        "    def data_to_features(self, x,y,action):\n",
        "        x= x/self.environment.get_size()[1]\n",
        "        y= y/self.environment.get_size()[0]\n",
        "        action = action/len(self.agent.actions)\n",
        "        return np.array([x,y,action, x*y, y*action, x*action, x*x, y*y, action*action, np.exp(x), np.exp(y), np.exp(action), np.sin(x), np.sin(y), np.sin(action)])\n",
        "        \n",
        "    def show_loss(self, rewards, window_size=None):\n",
        "        # se n√£o for numpy array, transformar em um\n",
        "        if not isinstance(rewards, np.ndarray):\n",
        "            rewards = np.asarray(rewards)\n",
        "\n",
        "        if window_size is not None:\n",
        "            # calcular a m√©dia m√≥vel\n",
        "            rewards = np.convolve(rewards, np.ones(window_size), 'valid') / window_size\n",
        "\n",
        "        # Criar o gr√°fico\n",
        "        plt.plot(rewards)\n",
        "        # Adicionar um t√≠tulo\n",
        "        plt.title(\"Gr√°fico de recompensas\")\n",
        "\n",
        "        # Adicionar um eixo x\n",
        "        plt.xlabel(\"Epis√≥dio\")\n",
        "\n",
        "        # Adicionar um eixo y\n",
        "        plt.ylabel(\"Recompensa\")\n",
        "\n",
        "        # Exibir o gr√°fico\n",
        "        plt.show()\n",
        "\n",
        "    def test(self, display = True):\n",
        "        shape = self.environment.get_size()\n",
        "        num_states = min(3000,shape[0]*shape[1])\n",
        "        success = 0\n",
        "        tries = len(self.environment.avaliations)\n",
        "        returns = []\n",
        "        steps = []\n",
        "        for avaliation in self.environment.avaliations:\n",
        "            reward = self.environment.setAgentPos(avaliation[0], avaliation[1])\n",
        "            step_count = 0\n",
        "            for _ in range(num_states):\n",
        "                step_count+=1\n",
        "                action = self.agent.policy[self.agent.y][self.agent.x]\n",
        "                R = self.environment.move(self.agent, action)\n",
        "                reward += R\n",
        "                if self.environment.in_terminal_state():\n",
        "                    success+=1\n",
        "                    break\n",
        "            steps.append(step_count)\n",
        "            returns.append(reward)\n",
        "        if display:\n",
        "            print(f\"Sucesso: {success}/{tries}: {success/tries*100}%\")\n",
        "            print(f\"Recompensa m√©dia: {np.asarray(returns).mean()}\")\n",
        "            print(f\"Steps m√©dio: {np.asarray(steps).mean()}\")\n",
        "        return {\"sucesso\": round(success/tries*100, 2), \"recompensa\": round(np.asarray(returns).mean(),2), \"steps\": round(np.asarray(steps).mean(),2)}\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "class MonteCarlo(LearningStrategy):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.episode_R = []\n",
        "        self.episode_length = []\n",
        "        self.Q = None\n",
        "        self.time = []\n",
        "    \n",
        "    def train(self, episodes, randomPolicy = True, exploration_chance = 0, appx = True, alpha = 0.003, display = True):\n",
        "        # Initialize\n",
        "        begin_training_time = time.time()\n",
        "        shape = self.environment.get_size()\n",
        "        self.agent.startPolicy(shape, randomPolicy)\n",
        "        self.agent.startReturns(shape)\n",
        "        self.Q = np.zeros((shape[0],shape[1], len(self.agent.actions)))\n",
        "        self.agent.startV(shape)\n",
        "        current_exploration_chance = exploration_chance\n",
        "        rewards = []\n",
        "        for ep in range(episodes):\n",
        "            start_time = time.time()\n",
        "            if display:\n",
        "                if ep % (episodes//10) == 0:\n",
        "                    print(f\"{ep=}\")\n",
        "                    path = self.path_from((1,1))\n",
        "                    print('Tamanho do epis√≥dio:', len(path))\n",
        "                    print('Recompensa', sum([i[2] for i in path]))\n",
        "                    path_dict = {}\n",
        "                    for s,a,r in path:\n",
        "                        path_dict[s] = a\n",
        "                    self.environment.render.show_path(path_dict)\n",
        "            # escolhe posicao aleatoria valida para o agente\n",
        "            while True:\n",
        "                state = (random.randrange(0, shape[0]), random.randrange(0, shape[1]))\n",
        "                if self.environment.original_map[state[0]][state[1]] in {self.environment.default_symbols[\"path\"], self.environment.default_symbols[\"goal\"]}:\n",
        "                    break\n",
        "            # escolhe uma acao diferente da dita pela politica atual\n",
        "            for _ in range(len(self.agent.actions)*2):    # limite maximo de tentativas\n",
        "                action = random.choice(self.agent.actions)\n",
        "                if action != self.agent.policy[state[0]][state[1]]:\n",
        "                    # se a acao nao te leva para uma parede\n",
        "                    if self.environment.util(state, action): \n",
        "                        break\n",
        "            else:\n",
        "                action = self.agent.policy[state[0]][state[1]]\n",
        "            self.episode(state, action, max_steps= min(3000, shape[1]*shape[0]), exploration_chance = exploration_chance)\n",
        "            g = 0\n",
        "            for t in range(len(self.agent.recalls)-1, -1, -1): \n",
        "                memory = self.agent.recalls[t]  # memoria = (estado, acao, reforco)\n",
        "                g = self.agent.gamma*g + memory[2]\n",
        "                # verifica se o par estado acao ja foi inserido em returns\n",
        "                if self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"lastEpisode\"] != ep:\n",
        "                    self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"lastEpisode\"] = ep\n",
        "                    self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"value\"] += g\n",
        "                    self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"count\"] += 1\n",
        "                    media = self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"value\"]/self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"count\"]\n",
        "\n",
        "                    if(not appx):\n",
        "                        self.Q[memory[0][0],memory[0][1],self.agent.action_idx(memory[1])] = media\n",
        "                    else:\n",
        "                        Q = self.get_Q(memory[0][0],memory[0][1],self.agent.action_idx(memory[1]),appx)\n",
        "                        # print(f\"Q: {Q}\")\n",
        "                        features = self.data_to_features(memory[0][0],memory[0][1],self.agent.action_idx(memory[1]))\n",
        "                        # print(f\"{features[0]=}, {g=}, {Q=}, {self.W[0]=}, {alpha=}, {features[0]*alpha*(g-Q)=}\")\n",
        "                        deltaW = alpha * (g - Q) * features\n",
        "                        # print(f\"deltaW: {deltaW}\")\n",
        "                        self.W += deltaW\n",
        "\n",
        "\n",
        "                    self.agent.book_V[memory[0][0]][memory[0][1]] = media\n",
        "                    self.agent.policy[memory[0][0]][memory[0][1]] = max(self.agent.actions, key = lambda action: self.get_Q(memory[0][0],memory[0][1],self.agent.action_idx(action),appx))    # recebe a action que maximiza o valor de Q\n",
        "            # atualiza a chance de exploracao\n",
        "            rewards.append(np.asarray(self.episode_R[ep]).sum())\n",
        "            current_exploration_chance *= 0.999\n",
        "            end_time = time.time()\n",
        "            time_difference_seconds = end_time - start_time\n",
        "            self.time.append(time_difference_seconds)\n",
        "\n",
        "        end_training_time = time.time()\n",
        "        if display:\n",
        "            print(f\"Tempo total de treinamento: {end_training_time - begin_training_time} segundos\")\n",
        "            self.show_loss(rewards, window_size=(len(rewards)//10))\n",
        "        return round(end_training_time - begin_training_time, 3)\n",
        "\n",
        "            \n",
        "    def episode(self, state, action, max_steps, exploration_chance=0):\n",
        "        step_count = 0\n",
        "        self.agent.recalls = []\n",
        "        reward = self.environment.setAgentPos(state[0], state[1])\n",
        "        episode_R = [reward]\n",
        "\n",
        "        while (not self.environment.in_terminal_state()) and (step_count < max_steps):  # enquanto nao estiver em um estado terminal\n",
        "            step_count +=1  # incrementa o numero de passos\n",
        "            last_pos = (self.agent.y, self.agent.x)\n",
        "            reward = self.environment.move(self.agent,action) # realiza a acao e recebe a recompensa\n",
        "            episode_R.append(reward)\n",
        "            self.agent.recalls.append((last_pos, action, reward)) # guarda o passo\n",
        "            if random.random() < exploration_chance:\n",
        "                for _ in range(len(self.agent.actions)*2):    # limite maximo de tentativas\n",
        "                    action = random.choice(self.agent.actions)\n",
        "                    # se a acao nao te leva para uma parede\n",
        "                    if self.environment.util(state, action): break\n",
        "            else:\n",
        "                action = self.agent.get_action() # escolhe uma acao de acordo com a politica\n",
        "        self.episode_R.append(episode_R)\n",
        "        self.episode_length.append(step_count)\n",
        "    \n",
        "    def path_from(self, starting_point):\n",
        "        shape = self.environment.get_size()\n",
        "        max_steps = shape[0] * shape[1]\n",
        "        step_count = 0\n",
        "        state = starting_point\n",
        "        self.environment.setAgentPos(state[0], state[1])\n",
        "        tuples = []\n",
        "        while (not self.environment.in_terminal_state()) and (step_count < max_steps):\n",
        "            action =  self.agent.policy[state[0]][state[1]]\n",
        "            R = self.environment.move(self.agent, action)\n",
        "            tuples.append((state,action,R))\n",
        "            state = (self.agent.y, self.agent.x)\n",
        "            step_count+=1\n",
        "        return tuples\n",
        "\n",
        "\n",
        "class SARSA(LearningStrategy):\n",
        "\n",
        "    def __init__(self, lam):\n",
        "        super().__init__()\n",
        "        self.lam = lam\n",
        "        self.episode_R = []\n",
        "        self.episode_length = []\n",
        "        self.Q = None\n",
        "        self.time = []\n",
        "        \n",
        "    def get_greedy_action(self,state, appx = False):\n",
        "        return max(self.agent.actions, key = lambda action: self.get_Q(state[0], state[1], self.agent.action_idx(action),appx))\n",
        "    \n",
        "    def get_epsilon_greedy(self, exploration_chance, state, appx = False):\n",
        "        if random.random() < exploration_chance:\n",
        "            return random.choice(self.agent.actions)\n",
        "        else:\n",
        "            return max(self.agent.actions, key = lambda action: self.get_Q(state[0], state[1], self.agent.action_idx(action), appx))\n",
        "                \n",
        "    def train(self, episodes, random_policy=True, exploration_chance=0.3, alpha=0.001, appx = False, display = True):\n",
        "        begin_training_time = time.time()\n",
        "        shape = self.environment.get_size()\n",
        "        ec = exploration_chance\n",
        "        num_states = shape[0]*shape[1]\n",
        "        linear_decay = exploration_chance/episodes\n",
        "        self.Q = np.zeros((shape[0],shape[1], len(self.agent.actions)))\n",
        "        rewards = []\n",
        "        E = dict()\n",
        "        for ep in range(episodes):\n",
        "            start_time = time.time()\n",
        "            episode_R = []\n",
        "            if display:\n",
        "                if ep % (episodes//10) == 0: \n",
        "                    print(f\"{ep=}\")\n",
        "                    path = self.path_from((1,1))\n",
        "                    print('Tamanho do epis√≥dio:', len(path))\n",
        "                    print('Recompensa', sum([i[2] for i in path]))\n",
        "                    path_dict = {}\n",
        "                    for s,a,r in path:\n",
        "                        path_dict[s] = a\n",
        "                    self.environment.render.show_path(path_dict)\n",
        "\n",
        "            E = dict()         \n",
        "            \n",
        "            # escolhe posicao aleatoria valida para o agente\n",
        "            while True:\n",
        "                S = (random.randrange(0, shape[0]), random.randrange(0, shape[1]))\n",
        "                if self.environment.original_map[S[0]][S[1]] in {self.environment.default_symbols[\"path\"], self.environment.default_symbols[\"goal\"]}:\n",
        "                    break\n",
        "            self.environment.setAgentPos(S[0], S[1])\n",
        "\n",
        "            A = self.get_epsilon_greedy(ec,S)\n",
        "            A_idx = self.agent.action_idx(A)\n",
        "\n",
        "            step_count = 0\n",
        "            while (not self.environment.in_terminal_state()) and (step_count < num_states):\n",
        "\n",
        "                R = self.environment.move(self.agent, A)\n",
        "                episode_R.append(R)\n",
        "                S_prime = (self.agent.y, self.agent.x)\n",
        "                A_prime = self.get_epsilon_greedy(ec, S_prime)\n",
        "                A_prime_idx = self.agent.action_idx(A)\n",
        "                \n",
        "                pair = (S, A)\n",
        "                E[pair] = E[pair] + 1 if pair in E.keys() else 1\n",
        "\n",
        "                delta = R + self.agent.gamma * self.get_Q(S_prime[0], S_prime[1], A_prime_idx, appx) - self.get_Q(S[0], S[1], A_idx, appx)\n",
        "                if (not appx):\n",
        "                    for (s, a) in E.keys():\n",
        "                        a_idx = self.agent.action_idx(a)\n",
        "                        self.Q[s[0],s[1], a_idx] += alpha * delta * E[(s,a)]\n",
        "                        E[(s,a)] *= self.agent.gamma * self.lam\n",
        "                else: \n",
        "                    for (s, a) in E.keys():\n",
        "                        a_idx = self.agent.action_idx(a)\n",
        "                        features = self.data_to_features(s[0],s[1],a_idx)\n",
        "                        deltaW = alpha * delta * E[(s,a)] * features\n",
        "                        self.W += deltaW\n",
        "                        E[(s,a)] *= self.agent.gamma * self.lam\n",
        "                \n",
        "                S = S_prime\n",
        "                A = A_prime\n",
        "                step_count += 1\n",
        "            self.episode_R.append(episode_R)\n",
        "            self.episode_length.append(step_count)\n",
        "            ec-=linear_decay\n",
        "            rewards.append(np.asarray(self.episode_R[ep]).sum())\n",
        "            end_time = time.time()\n",
        "            time_difference_seconds = end_time - start_time\n",
        "            self.time.append(time_difference_seconds)\n",
        "            \n",
        "\n",
        "        self.agent.startPolicy(shape, random_policy)\n",
        "        self.agent.startV(shape)\n",
        "        for i in range(shape[0]):\n",
        "            for j in range(shape[1]):\n",
        "                if(self.environment.original_map[i][j] == '#'): self.agent.policy[i][j] = \"wall\"\n",
        "                else: \n",
        "                    self.agent.policy[i][j] = max(self.agent.actions, key = lambda action: self.get_Q(i,j, self.agent.action_idx(action), appx))\n",
        "                    self.agent.book_V[i][j] = max(self.Q[i,j,:])\n",
        "                    \n",
        "        end_training_time = time.time()\n",
        "        if display:\n",
        "            print(f\"Tempo total de treinamento: {end_training_time - begin_training_time} segundos\")\n",
        "            self.show_loss(rewards, window_size=(len(rewards)//10))\n",
        "        return round(end_training_time - begin_training_time, 3)\n",
        "\n",
        "    def path_from(self, starting_point):\n",
        "        shape = self.environment.get_size()\n",
        "        max_steps = shape[0] * shape[1]\n",
        "        step_count = 0\n",
        "        S = starting_point\n",
        "        self.environment.setAgentPos(S[0], S[1])\n",
        "        tuples = []\n",
        "        while (not self.environment.in_terminal_state()) and (step_count < max_steps):\n",
        "            A = self.get_greedy_action(S)\n",
        "            R = self.environment.move(self.agent, A)\n",
        "            tuples.append((S,A,R))\n",
        "            S = (self.agent.y, self.agent.x)\n",
        "            step_count+=1\n",
        "        return tuples\n",
        "                \n",
        "\n",
        "\n",
        "        \n",
        "class QLearning(LearningStrategy):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.episode_R = []\n",
        "        self.episode_length = []\n",
        "        self.Q = None\n",
        "        \n",
        "        self.time = []\n",
        "\n",
        "    def get_greedy_action(self,state,appx = False):\n",
        "        return max(self.agent.actions, key = lambda action: self.get_Q(state[0], state[1], self.agent.action_idx(action),appx))\n",
        "    \n",
        "    def get_epsilon_greedy(self, exploration_chance, state, appx = False):\n",
        "        if random.random() < exploration_chance:\n",
        "            return random.choice(self.agent.actions)\n",
        "        else:\n",
        "            return max(self.agent.actions, key = lambda action: self.get_Q(state[0], state[1], self.agent.action_idx(action),appx))\n",
        "                \n",
        "    def train(self, episodes, random_policy=True, exploration_chance=0.3, alpha=0.003, appx = True, display = True):\n",
        "        begin_training_time = time.time()\n",
        "        shape = self.environment.get_size()\n",
        "        ec = exploration_chance\n",
        "        num_states = shape[0]*shape[1]\n",
        "        linear_decay = exploration_chance/episodes\n",
        "        self.Q = np.zeros((shape[0],shape[1], len(self.agent.actions)))\n",
        "\n",
        "        for ep in range(episodes):\n",
        "            start_time = time.time()\n",
        "            episode_R = []\n",
        "            if display:\n",
        "                if ep % (episodes//10) == 0: \n",
        "                    print(f\"{ep=}\")\n",
        "                    path = self.path_from((1,1))\n",
        "                    print('Tamanho do epis√≥dio:', len(path))\n",
        "                    print('Recompensa', sum([i[2] for i in path]))\n",
        "                    path_dict = {}\n",
        "                    for s,a,r in path:\n",
        "                        path_dict[s] = a\n",
        "                    self.environment.render.show_path(path_dict)\n",
        "\n",
        "            # escolhe posicao aleatoria valida para o agente\n",
        "            while True:\n",
        "                S = (random.randrange(0, shape[0]), random.randrange(0, shape[1]))\n",
        "                if self.environment.original_map[S[0]][S[1]] in {self.environment.default_symbols[\"path\"], self.environment.default_symbols[\"goal\"]}:\n",
        "                    break\n",
        "            self.environment.setAgentPos(S[0], S[1])\n",
        "\n",
        "            #A = random.choice(self.agent.actions)\n",
        "            step_count = 0\n",
        "            while (not self.environment.in_terminal_state()) and (step_count < num_states):\n",
        "                A = self.get_epsilon_greedy(ec,S)\n",
        "                A_idx = self.agent.action_idx(A)\n",
        "\n",
        "                R = self.environment.move(self.agent, A)\n",
        "\n",
        "                episode_R.append(R)\n",
        "\n",
        "                S_prime = (self.agent.y, self.agent.x)\n",
        "                A_prime = self.get_greedy_action(S_prime)\n",
        "                A_prime_idx = self.agent.action_idx(A_prime)\n",
        "\n",
        "                if(not appx):\n",
        "                    self.Q[S[0], S[1], A_idx] += alpha*(R + self.agent.gamma * self.get_Q(S_prime[0], S_prime[1], A_prime_idx) - self.get_Q(S[0], S[1], A_idx))\n",
        "                else:\n",
        "                    deltaW = alpha * (R + self.agent.gamma * self.get_Q(S_prime[0], S_prime[1], A_prime_idx, appx) - self.get_Q(S[0], S[1], A_idx, appx)) * self.data_to_features(S[0],S[1],A_idx)\n",
        "                    self.W += deltaW\n",
        "\n",
        "                \n",
        "                S = S_prime\n",
        "                step_count += 1\n",
        "                \n",
        "            self.episode_R.append(episode_R)\n",
        "            self.episode_length.append(step_count)\n",
        "            ec-=linear_decay\n",
        "            end_time = time.time()\n",
        "            time_difference_seconds = end_time - start_time\n",
        "            self.time.append(time_difference_seconds)\n",
        "            \n",
        "\n",
        "        self.agent.startPolicy(shape, random_policy)\n",
        "        self.agent.startV(shape)\n",
        "        for i in range(shape[0]):\n",
        "            for j in range(shape[1]):\n",
        "                if(self.environment.original_map[i][j] == '#'): self.agent.policy[i][j] = \"wall\"\n",
        "                else: \n",
        "                    self.agent.policy[i][j] = max(self.agent.actions, key = lambda action: self.get_Q(i,j, self.agent.action_idx(action),appx))\n",
        "                    self.agent.book_V[i][j] = max(self.Q[i,j,:])\n",
        "        end_training_time = time.time()\n",
        "        return round(end_training_time - begin_training_time, 3)\n",
        "\n",
        "    def path_from(self, starting_point):\n",
        "        shape = self.environment.get_size()\n",
        "        max_steps = shape[0] * shape[1]\n",
        "        step_count = 0\n",
        "        S = starting_point\n",
        "        self.environment.setAgentPos(S[0], S[1])\n",
        "        tuples = []\n",
        "        while (not self.environment.in_terminal_state()) and (step_count < max_steps):\n",
        "            A = self.get_greedy_action(S)\n",
        "            R = self.environment.move(self.agent, A)\n",
        "            tuples.append((S,A,R))\n",
        "            S = (self.agent.y, self.agent.x)\n",
        "            step_count+=1\n",
        "        return tuples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Environment:\n",
        "    default_symbols = {\"agent\": '@', \"wall\": '#', \"path\": '.', \"goal\":'$', \"lava\":'L', \"acid\":'A', \"cookie\":'C'}\n",
        "    def __init__(self, path, stochastic, display=True) -> None:\n",
        "        self.display = display\n",
        "        self.avaliations = []\n",
        "        self.original_map = self.load_map(path)\n",
        "        self.map = self.copy_map(self.original_map)\n",
        "        self.wait_time = 0\n",
        "        self.stochastic = stochastic\n",
        "        self.dists_goal = None\n",
        "\n",
        "        if self.display:\n",
        "            self.render = Renderer(self, self.map, \"Ambiente\")\n",
        "\n",
        "    def copy_map(self, mapa):\n",
        "        mapaCopia = []\n",
        "        for linha in mapa:\n",
        "            mapaCopia.append([])\n",
        "            for celula in linha:\n",
        "                mapaCopia[-1].append(celula)\n",
        "        return mapaCopia\n",
        "\n",
        "    def getAgent(self) -> Agent:\n",
        "        return self.agent\n",
        "\n",
        "    def in_terminal_state(self):\n",
        "        return self.original_map[self.agent.y][self.agent.x] in (self.default_symbols[\"goal\"], self.default_symbols[\"lava\"])\n",
        "    \n",
        "    def get_dist_goals(self):\n",
        "        self.dists_goal = np.full((len(self.map), len(self.map[0])), np.inf)\n",
        "        for i in range(len(self.map)):\n",
        "            for j in range(len(self.map[0])):\n",
        "                if self.original_map[i][j] == self.default_symbols[\"goal\"]:\n",
        "                    self.dists_goal[i][j] = 0\n",
        "        for i in range(1,len(self.map)-1):\n",
        "            for j in range(1,len(self.map[0])-1):\n",
        "                self.dists_goal[i][j] = min(self.dists_goal[i][j], self.dists_goal[i-1][j]+1, self.dists_goal[i+1][j]+1, self.dists_goal[i][j-1]+1, self.dists_goal[i][j+1]+1)\n",
        "\n",
        "    def get_dist_closest_goal(self, agent):\n",
        "        if self.dists_goal is None:\n",
        "            self.dists_goal = self.get_dist_goals()\n",
        "        return self.dists_goal[agent.y][agent.x]\n",
        "\n",
        "    \n",
        "    def get_index_object(self, object):\n",
        "        if object == self.default_symbols[\"agent\"]:\n",
        "            return 0\n",
        "        elif object == self.default_symbols[\"wall\"]:\n",
        "            return 1\n",
        "        elif object == self.default_symbols[\"path\"]:\n",
        "            return 2\n",
        "        elif object == self.default_symbols[\"goal\"]:\n",
        "            return 3\n",
        "        elif object == self.default_symbols[\"lava\"]:\n",
        "            return 4\n",
        "        elif object == self.default_symbols[\"acid\"]:\n",
        "            return 5\n",
        "        elif object == self.default_symbols[\"cookie\"]:\n",
        "            return 6\n",
        "        else:\n",
        "            return 7\n",
        "\n",
        "    def get_sensors(self, agent, num_sensors, sensors_type):\n",
        "        sensors = np.zeros(num_sensors)\n",
        "        s_i = 0\n",
        "        i = 0\n",
        "        while i < len(sensors_type):\n",
        "            match sensors_type[i]:\n",
        "                case \"radius\":\n",
        "                    i += 1\n",
        "                    radius = sensors_type[i]\n",
        "                    for j in range(-radius, radius+1):\n",
        "                        for k in range(-radius, radius+1):\n",
        "                            object = self.get_index_object(self.original_map[agent.y+j][agent.x+k])\n",
        "                            sensors[s_i] = object\n",
        "                            s_i += 1\n",
        "                    i += 1\n",
        "                case \"smell\":\n",
        "                    distance = self.get_dist_closest_goal(agent)\n",
        "                    sensors[s_i] = distance\n",
        "                    s_i += 1\n",
        "                    i += 1\n",
        "                \n",
        "                case _:\n",
        "                    i+=1\n",
        "\n",
        "\n",
        "    def load_map(self, path):\n",
        "        \"\"\"\n",
        "        Dado o caminho path, le um arquivo txt e retorna uma matriz\n",
        "        O txt consiste de uma linha contendo o numero n (n√∫mero de\n",
        "        linhas do mapa) e m (n√∫mero de caracteres diferentes no mapa),\n",
        "        seguido de m linhas explicando o que sao os caracteres no \n",
        "        arquivo e por fim n linhas contendo o mapa que sao caracteres\n",
        "        \"\"\"\n",
        "        grid = []\n",
        "        self.symbols = dict()\n",
        "        self.rewards = {\"agent\": 0, \"wall\": 0, \"path\": 0, \"goal\":0}\n",
        "        with open(path, 'r') as file:\n",
        "            m, n = map(int, file.readline().split())\n",
        "            for _ in range(m):\n",
        "                line = file.readline().split()\n",
        "                self.symbols[line[0]] = line[1]\n",
        "                self.rewards[line[1]] = int(line[2])\n",
        "            for i in range(n):\n",
        "                line = file.readline()\n",
        "                grid.append([])\n",
        "                for j in range(len(line)):\n",
        "                    char = line[j]\n",
        "                    if char == '\\n':\n",
        "                        continue\n",
        "                    if self.symbols[char] == 'agent':\n",
        "                        self.avaliations.append((i,j))\n",
        "                        char = self.default_symbols[\"path\"]\n",
        "                    grid[-1].append(self.default_symbols[self.symbols[char]])\n",
        "        return grid\n",
        "    \n",
        "    def move(self, agent, acao):\n",
        "        \"\"\"\n",
        "        Dada uma acao, move o agente no mapa\n",
        "        a acao pode ser \"up\", \"down\", \"left\" ou \"right\"\n",
        "        \"\"\"\n",
        "        if random.random() < self.stochastic:\n",
        "            acao = random.choice(agent.actions)\n",
        "        time.sleep(self.wait_time)\n",
        "        direction = {\"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)}\n",
        "        posicaofinal = (agent.y+direction[acao][0], agent.x+direction[acao][1])\n",
        "        if self.map[posicaofinal[0]][posicaofinal[1]] != self.default_symbols[\"wall\"]:\n",
        "            # seta a posicao atual como caminho\n",
        "            self.map[agent.y][agent.x] = self.original_map[agent.y][agent.x]\n",
        "            \n",
        "            # seta a posicao final como o agente\n",
        "            self.map[posicaofinal[0]][posicaofinal[1]] = self.default_symbols[\"agent\"]\n",
        "            \n",
        "            # atualiza a posicao do agente\n",
        "            agent.setPos(posicaofinal)\n",
        "        # retorna o reforco da posicao final \n",
        "        return self.rewards[self.symbols[self.original_map[agent.y][agent.x]]]\n",
        "\n",
        "    def util(self, pos, acao):\n",
        "        \"\"\"\n",
        "        Dada uma posicao e uma acao, retorna o que tem na posicao destino\n",
        "        \"\"\"\n",
        "        direcao = {\"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)}\n",
        "        posicaofinal = (pos[0]+direcao[acao][0], pos[1]+direcao[acao][1])\n",
        "        return self.symbols[self.map[posicaofinal[0]][posicaofinal[1]]] != \"wall\"\n",
        "\n",
        "    def get_size(self):\n",
        "        return len(self.map), len(self.map[0])\n",
        "    \n",
        "    def setAgentPos(self, i, j):\n",
        "        self.map[self.agent.y][self.agent.x] = self.original_map[self.agent.y][self.agent.x]\n",
        "        self.agent.setPos((i, j))\n",
        "        self.map[i][j] = self.default_symbols[\"agent\"]\n",
        "        return self.rewards[self.symbols[self.original_map[self.agent.y][self.agent.x]]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n",
            "253.19389867782593\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "learning_strategy = SARSA(0.1)\n",
        "environment = Environment('salas/salaA.txt', True, display=False)\n",
        "agent = Agent(x=0, y=0, environment=environment, display=environment.display)\n",
        "environment.agent = agent\n",
        "learning_strategy.setup(environment, agent)\n",
        "\n",
        "learning_strategy.train(10000, exploration_chance=0.5, display=False, appx=False)\n",
        "#agent.render.show()\n",
        "print(\"Done\")\n",
        "print(sum(learning_strategy.time))\n",
        "\n",
        "Renderer.create_heatmap(learning_strategy.agent.book_V, cmap='inferno', title='Sample Heatmap')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <b style=\"color:#009900\">An√°lise do Monte Carlo em ambiente determin√≠stico</b>\n",
        "\n",
        "O algoritmo monte carlo utilizado foi colocado em diversas salas com diversos par√¢metros diferentes e algumas varia√ß√µes no seu algoritmo. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##  <b style=\"color:#22BBBB\">Sala 1</b>\n",
        "A figura abaixo mostra a estrutura da primeira sala usada para avalia√ß√£o do modelo, a sala mais simples presente.\n",
        "\n",
        "![Sala 1](figs/sala%201.png)\n",
        "\n",
        "Aqui √© poss√≠vel visualizar o objetivo na forma de uma moeda e o agente na forma de um alien verde. Al√©m disso √© poss√≠vel ver os caminhos v√°lidos em preto e as paredes em roza. Cada caminho v√°lido aqui garante uma recompensa de -1 ao chegar, enquanto o objetivo garante uma recompensa de 100. Movimentos para cima ou para baixo nesse mapa s√£o sempre em dire√ß√£o a uma parede, o que n√£o move o agente de lugar mas garante uma recompensa de -1. Assim √© esperado que esse tipo de atitude seja evitada com o passar do tempo. A pol√≠tica √© inicializada de forma aleat√≥ria e pode ser verificada na forma de setas azuis, como na figura abaixo:\n",
        "\n",
        "![Politica inicial da sala 1](figs/politicaInicialSala1.png)\n",
        "\n",
        "Um primeiro obst√°culo foi verificado aqui: Como a pol√≠tica inicial pode iniciar direcionando o agente para um local inv√°lido, o mantendo preso para sempre, √© necess√°rio definir um valor m√°ximo de passos para cada epis√≥dio. Aqui esse valor foi de 10 passos. Ou seja, o epis√≥dio acaba, ou quando o agente atinge o estado terminal, ou ap√≥s 10 passos.\n",
        "Ap√≥s 10 epis√≥dios nesse cen√°rio o algoritmo n√£o apresentou uma converg√™ncia satisfat√≥ria. Por√©m com 12 epis√≥dios a converg√™ncia j√° foi quase absoluta. O n√∫mero 12 aqui √© curioso pois √© m√∫ltiplo to n√∫mero de a√ß√µes dispon√≠veis (4). E nesse cen√°rio h√° 3 poss√≠veis locais onde o personagem pode surgir e tomar uma a√ß√£o v√°lida (j√° que se ele surgir no estado terminal a simula√ß√£o acaba o impedindo de tomar uma a√ß√£o). Assim, 12 √© um n√∫mero suficiente para ele testar, com uma boa probabilidade, todas as possibilidades desse ambiente, chegando por fim na pol√≠tica √≥tima abaixo:\n",
        "\n",
        "![Politica final da sala 1](figs/politicaFinalSala1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##  <b style=\"color:#22BBBB\">Sala 2</b>\n",
        "A figura abaixo mostra a estrutura da segunda sala usada para avalia√ß√£o do modelo, uma das salas mais simples presentes, mas que aumenta levemente o n√∫mero de caminhos v√°lidos dispon√≠veis e possibilita um movimento vertical limitado.\n",
        "\n",
        "![Sala 2](figs/sala%202.png)\n",
        "\n",
        "Diferente da primeira sala, aqui os movimentos s√£o menos limitados e h√° mais de um caminho √≥timo para o agente descobrir. A pol√≠tica inicial aleat√≥ria pode ser vista como abaixo:\n",
        "\n",
        "![Politica inicial da sala 2](figs/politicaInicialSala2.png)\n",
        "\n",
        "Aqui os 12 epis√≥dios n√£o foram suficientes para obter alguma pol√≠tica √≥tima, e nem era esperado que isso acontece, pois o n√∫mero de movimentos v√°lidos e caminhos aumentou. Observando A imagem √© poss√≠vel veerificar que h√° 7 estados n√£o terminais, nos quais 4 possuem apenas 1 movimento √≥timo e 3 com 2 movimentos √≥timos (direita ou cima). Assim, h√° um total de 28 op√ß√µes totais das quais 10 s√£o √≥timas. Isso indica que o agente precisa testar no m√≠nimo 18 op√ß√µes distintas para encontrar uma pol√≠tica √≥tima. Treinando o modelo com 18 epis√≥dios temos (no melhor resultado):\n",
        "\n",
        "![Politica da sala 2 com 18 epis√≥dios](figs/politicaFinalSala2ep18.png)\n",
        "\n",
        "Aqui podemos ver que o agente errou em apenas 1 estado a pol√≠tica √≥tima, por√©m nesse estado ele fica preso. Al√©m disso, esse resultado n√£o reflete o caso m√©dio, que √© bem mais desordenado, mostrando que esse caso foi originado por uma pol√≠tica inicial aleat√≥ria muito boa e 18 epis√≥dios n√£o √© o suficiente para treinar o agente de forma adequada. Colocando 36 epis√≥dios de treinamento temos uma pol√≠tica mais est√°vel mas n√£o √≥tima no caso m√©dio, como √© poss√≠vel ver na imagem abaixo:\n",
        "\n",
        "![Politica da sala 2 com 36 epis√≥dios](figs/politicaFinalSala2ep36.png)\n",
        "\n",
        "Aqui √© poss√≠vel ver que o modelo n√£o est√° conseguindo convergir para a pol√≠tica √≥tima, mas est√° encontrando um caminho v√°lido. Isso pode ser corrigido com mais epis√≥dios ou por altera√ß√£o do valor do refor√ßo final, uma vez que a adi√ß√£o de um √∫nico caminho a mais (refor√ßo somado em -1) n√£o tem muito valor quando comparado com o total ganho no final (100).\n",
        "\n",
        "Ap√≥s um processo de busca bin√°ria no n√∫mero de epis√≥dios, foi poss√≠vel encontrar o valor de 130 como um valor em que todas as pol√≠ticas geradas para 20 amostras foram √≥timas.\n",
        "\n",
        "![Politica final da sala 2](figs/politicaFinalSala2.png)\n",
        "\n",
        "Aqui podemos ver que o valor foi muito superior a 18, evidenciando uma depend√™ncia bem mais complexa que a inferida anteriormente, sendo necess√°rio um teste mais preciso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  <b style=\"color:#22BBBB\">Sala 4</b>\n",
        "\n",
        "Aqui a sala √© bem maior mas ainda completamente ampla\n",
        "\n",
        "![Sala 4](figs/sala%202.png)\n",
        "\n",
        "Nesse cen√°rio, foi necess√°rio a altera√ß√£o doo limite de passos, uma vez que mesmo com a pol√≠tica √≥tima, seria imposs√≠vel que o agente iniciado no canto inferior esquerdo atingisse o estado terminal. Sendo assim ele passou a ser dependente da dimens√£o do mapa, valendo o n√∫mero total de quadrados da sala, nesse caso 400. Com isso, ap√≥s 30000 epis√≥dios a pol√≠tica convergiu para uma pol√≠tica pr√≥xima da √≥tima:\n",
        "\n",
        "![Politica final da sala 4](figs/politicaFinalSala2.png)\n",
        "\n",
        "Aqui √© poss√≠vel verificar que n√£o foram todos os campos com a pol√≠tica perfeita, mas a adic√£o de mais epis√≥dios toma muito tempo para atingir a pol√≠tica perfeita, logo a partir de agora ser√° dada a pol√≠tica ideal com alguma toler√¢ncia em rela√ß√£o a pol√≠tica √≥tima."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  <b style=\"color:#22BBBB\">Sala 3</b>\n",
        "\n",
        "Aqui a sala possui obst√°culos no meio do caminho, tornando mais complexa a identifica√ß√£o da pol√≠tica √≥tima. Al√©m disso o final do caminho (pr√≥ximo ao objetivo) √© muito estreito, o que pode gerar resultados inesperados\n",
        "\n",
        "![Sala 3](figs/sala%203.png)\n",
        "\n",
        "Aqui um resultado peculiar ocorreu:\n",
        "\n",
        "![Politica final da sala 3](figs/politicaFinalSala3ep100.png)\n",
        "\n",
        "Foi verificado que a pol√≠tica √≥tima precisa evoluir a partir do objetivo. O que significa que salas grandes com corredores estreitos podem custar muito para o agente aprender a resolver. Com 100 √©pocas apenas uma regi√£o do corredor estava dispon√≠vel, enquanto que para 200, uma regi√£o maior ficou dispon√≠vel, semelhante ao porcesso de uma busca em largura em um grafo. Assim, foi considerado uma altera√ß√£o no algoritmo principal. Agora o agente possui uma chance n√£o seguir a pol√≠tica e experimentar uma nova possibilidade. Assim ele n√£o fica t√£o restrito a posi√ß√£o inicial nem a pol√≠tica aleat√≥ria inicial. Dessa forma foram necess√°rios 10000 epis√≥dios para converg√™ncia da pol√≠tica. Mas uma nova caracter√≠stica foi verificada: Com essa taxa de explora√ß√£o fixa, o agente esquecia da pol√≠tica aprendida em algumas regi√µes. Para resolver isso, a taxa de explora√ß√£o agora decaia com o n√∫mero de epis√≥dios, sendo multiplicada por 0.999 todo final de epis√≥dio. Ao final foi produzida a seguinte pol√≠tica:\n",
        "\n",
        "![Politica final da sala 3](figs/politicaFinalSala3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  <b style=\"color:#22BBBB\">Salas 5 7 e 8</b>\n",
        "Mais salas foram adicionadas, para a sala 5:\n",
        "\n",
        "![Sala 5](figs/sala%205.png)\n",
        "\n",
        "foram necess√°rios 30000 epis√≥dios para atingir a pol√≠tica ideal:\n",
        "\n",
        "![Politica final da sala 5](figs/politicaFinalSala5.png)\n",
        "\n",
        "enquanto para a sala 7:\n",
        "\n",
        "![Sala 7](figs/sala%207.png)\n",
        "\n",
        "O valor de 30000 levou muito mais tempo para finalizar, gerando a pol√≠tica:\n",
        "\n",
        "![Politica final da sala 7](figs/politicaFinalSala7ep30000.png)\n",
        "\n",
        "Aqui podemos ver que apenas o corredor inicial foi analisado, mas o tempo de treinamento foi de 3 minutos, o que torna o processo muito custoso.\n",
        "\n",
        "Para a √∫ltima sala, a 8 temos:\n",
        "\n",
        "![Sala 8](figs/sala%208.png)\n",
        "\n",
        "O treinamento com 1000 √©pocas custou 45 segundos, indicando um custo computacional elevado para computar todos epis√≥dios. Assim √© poss√≠vel verificar que o Monte Carlo possui muitas dificuldades quando o ambiente possui muitos estados diferentes.\n",
        "\n",
        "![Politica final da sala 7](figs/politicaFinalSala8.png)\n",
        "\n",
        "Com essas 1000 √©pocas o agente n foi capaz de aprender nem mesmo o estado ao lado do terminal, logo nada foi aprendido nesse ambiente com tantos cen√°rios.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <b style=\"color:#009900\">An√°lise do Monte Carlo em ambiente estoc√°stico</b>\n",
        "A estoc√°sticidade do ambiente √© dado por um valor racional de 0 a 1 e simboliza a chance de determinada a√ß√£o trocar para uma outra a√ß√£o aleat√≥ria. Para s = 0.33 significa que h√° 33% de chance da a√ß√£o escolhida (ir para a direita por exemplo), trocar para outra a√ß√£o aleat√≥ria (ir para cima por exemplo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  <b style=\"color:#22BBBB\">Sala 1</b>\n",
        "\n",
        "Para uma estocasticidade de 0.1, nenhuma altera√ß√£o significativa foi percebida, o ambiente se comporta da mesma forma, levando o mesmo n√∫mero de epis√≥dios para convergir ara a pol√≠tica √≥tima. Com o aumento do n√∫mero de √©pocas, nenhuma mudan√ßa na pol√≠tica foi verificada.\n",
        "\n",
        "Para s = 0.5, em metade dos casos a pol√≠tica n√£o convergiu para a √≥tica, assumindo valores como:\n",
        "\n",
        "![Politica final da sala 1 estoc√°stica](figs/politicaFinalSala1Esto.png)\n",
        "\n",
        "Para corrigir esse erro, a quantidade de epis√≥dios aumentou para 100 (que ainda apresentou erro), e posteriormente para 1000, ficando completamente √≥timo.\n",
        "\n",
        "Para s = 0.75, 1000 epis√≥dios n√£o foi o suficiente, enquanto 2000 parece melhorar ao limite, mas n√£o atingindo a pol√≠tica √≥tima. Nesse ambiente, se em algum momento a pol√≠tica atinge um valor para uma dire√ß√£o inv√°lida, esse valor n√£o √© alterado, uma vez que a chance de explora√ß√£o somada a estocasticidade do ambiente fazem com que os movimentos sejam quase totalmente independentes da pol√≠tica, dificultando o aprendizado.\n",
        "\n",
        "Como esperado, para s = 1, nenhuma pol√≠tica √© derivada do aprendizado, uma vez que n√£o importa a decis√£o do agente, a a√ß√£o ser√° completamente aleat√≥ria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  <b style=\"color:#22BBBB\">Sala 2</b>\n",
        "\n",
        "A sala 2 foi avaliada com a estocasticidade de 0.1, o que n√£o gerou nenhuma mudan√ßa significativa no treinamento do agente, 0.5, precisando de 1000 epis√≥dios para obter uma pol√≠tica aceit√°vel (com um estado erro no m√°ximo), e 0.75 foi necess√°rio 2000 epis√≥dios para convergir.\n",
        "\n",
        "Na sala 3 e estocasticidade 0.1, o modelo foi treinado com 10000 epis√≥dios gerando a pol√≠tica abaixo\n",
        "\n",
        "![Politica final da sala 3 estoc√°stica](figs/politicaFinalSala3sto01e1000.png)\n",
        "\n",
        "Aqui podemos ver que mesmo com uma estocasticidade baixa, o agente n√£o consegue aprender a pol√≠tica ideal, possivelmente por que para um caminho maior, esse valor se torna bem mais relevante. Para atingir uma qualidade aceit√°vel da pol√≠tica, foram necess√°rios 160000 epis√≥dios.\n",
        "Para estocasticidade de 0.5, o modelo deixou de convergir, encontrando pol√≠ticas sem sentido mesmo para 320000 epis√≥dios\n",
        "\n",
        "![Politica final da sala 3 estoc√°stica](figs/politicaFinalSala3sto05ep320000.png)\n",
        "\n",
        "Para a sala 4 \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <b style=\"color:#009900\">Metricas</b>\n",
        "\n",
        "Para as m√©tricas abaixo, foi calculado para treinos e testes com a seed 42.\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte carlo sem aproximador e em ambiente determin√≠stico, 1000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo |taxa |recompensa|passos |\n",
        "|-----------------|------|-----|----------|-------|\n",
        "|1  |0.04  |100.0|98.0      |2.0    |\n",
        "|2  |0.046 |100.0|97.71     |2.29   |\n",
        "|3  |0.264 |22.22|-57.06    |78.5   |\n",
        "|3.1|0.111 |100.0|92.35     |7.65   |\n",
        "|3.2|0.076 |100.0|95.08     |4.92   |\n",
        "|4  |0.632 |52.94|-143.03   |195.5  |\n",
        "|5  |1.142 |19.05|-306.1    |324.33 |\n",
        "|6  |8.198 |78.69|-472.26   |550.75 |\n",
        "|7  |12.497|8.51 |-2279.85  |2287.45|\n",
        "|8  |17.299|0.0  |-3001.0   |3000.0 |\n",
        "|9  |1.45  |25.64|-283.72   |298.64 |\n",
        "|10 |0.27  |47.83|-24.83    |72.13  |\n",
        "|11 |0.253 |63.16|15.79     |47.0   |\n",
        "|12 |0.249 |82.86|54.86     |27.83  |\n",
        "|A |0.79 |0.0|-401.0     |400.0  |\n",
        "\n",
        "Aqui √© poss√≠vel verificar que o modelo consegue aprender com 1000 epis√≥dios muito bem as salas pequenas ou muito simples, como as salas 1,2,3.1,3.2 4 e 6, salas completamente vazias. Ao mesmo tempo a sala 3 levou mais tempo para ser aprendida, como ja discutido nas se√ß√µes acima.\n",
        "Para a sala 8, nenhum caminho levou a o objetivo, provavelmente por conta de que em nenhum epis√≥dio foi selecionado \n",
        "\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte carlo sem aproximador e em ambiente determin√≠stico, 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo  |taxa |recompensa|passos |\n",
        "|-----------------|-------|-----|----------|-------|\n",
        "|1  |0.336  |100.0|98.0      |2.0    |\n",
        "|2  |0.433  |100.0|97.71     |2.29   |\n",
        "|3  |2.338  |38.89|-25.78    |64.06  |\n",
        "|3.1|0.823  |100.0|92.7      |7.3    |\n",
        "|3.2|0.521  |100.0|95.42     |4.58   |\n",
        "|4  |1.792  |100.0|87.94     |12.06  |\n",
        "|5  |9.218  |35.71|-224.19   |259.26 |\n",
        "|6  |70.496 |96.08|-23.51    |119.5  |\n",
        "|7  |82.09  |12.77|-2169.4   |2181.3 |\n",
        "|8  |102.726|11.11|-2656.81  |2667.03|\n",
        "|9  |11.838 |43.59|-188.26   |228.97 |\n",
        "|10 |2.162  |86.96|62.26     |24.57  |\n",
        "|11 |1.22   |100.0|92.79     |7.21   |\n",
        "|12 |1.445  |100.0|91.26     |8.74   |\n",
        "|A |6.542  |12.5|-339.75     |351.38   |\n",
        "\n",
        "\n",
        "Aqui √© poss√≠vel verificar uma melhora consider√°vel com o aumento dos epis√≥dios, mas salas grandes ainda n√£o possuem uma taxa aceit√°vel. A sala 3 ainda n√£o convergiu para uma pol√≠tica boa, possivelmente por conta da taxa de explora√ß√£o zerada, o que obriga o caminho a iniciar com as escolhas em uma posi√ß√£o ao lado da pr√© calculada, o que diminui muito a velocidade de converg√™ncia.\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte carlo sem aproximador e em ambiente determin√≠stico, 100000 epis√≥dios</b>\n",
        "\n",
        "|sala|tempo   |taxa |recompensa|passos |\n",
        "|----|--------|-----|----------|-------|\n",
        "|1  |5.119   |100.0|98.0      |2.0    |\n",
        "|2  |4.104   |100.0|97.71     |2.29   |\n",
        "|3  |20.927  |44.44|-15.61    |59.5   |\n",
        "|3.1|8.976   |100.0|92.7      |7.3    |\n",
        "|3.2|6.324   |100.0|95.42     |4.58   |\n",
        "|4  |11.079  |100.0|88.41     |11.59  |\n",
        "|5  |39.406  |83.33|7.26      |75.9   |\n",
        "|6  |711.838 |98.69|43.81     |54.86  |\n",
        "|7  |912.406 |17.02|-2059.04  |2075.23|\n",
        "|8  |1366.857|36.11|-1883.43  |1918.9 |\n",
        "|9  |74.364  |66.67|-76.97    |140.31 |\n",
        "|10 |21.304  |100.0|90.83     |9.17   |\n",
        "|11 |14.014  |100.0|92.79     |7.21   |\n",
        "|12 |15.336  |100.0|91.26     |8.74   |\n",
        "|A |38.189 |50.0|-159.12     |207.5   |\n",
        "\n",
        "Assim podemos verificar o ganho de efici√™ncia do algoritmo com o aumento do n√∫mero de epis√≥dios mas como o custo computacional aumenta muito, 1e4 apresenta um bom custo benef√≠cio e ser√° mantido para os outros testes.\n",
        "Para a sala 3, a taxa de sucesso est√° baixa, isso deve aumentar com a taxa de explora√ß√£o.\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte carlo sem aproximador e em ambiente estoc√°stico, 0.1 , 10000 epis√≥dios</b>\n",
        "\n",
        "\n",
        "|sala             |tempo   |taxa |recompensa|passos |\n",
        "|-----------------|--------|-----|----------|-------|\n",
        "|1  |0.439   |100.0|98.0      |2.0    |\n",
        "|2  |0.431   |100.0|97.57     |2.43   |\n",
        "|3  |2.296   |44.44|-19.44    |63.33  |\n",
        "|3.1|0.778   |100.0|92.26     |7.74   |\n",
        "|3.2|0.565   |100.0|94.29     |5.71   |\n",
        "|4  |1.995   |100.0|84.44     |15.56  |\n",
        "|5  |7.895   |73.81|-100.76   |174.31 |\n",
        "|6  |60.119  |56.52|-1221.26  |1277.35|\n",
        "|7  |85.739  |17.02|-2085.98  |2102.17|\n",
        "|8  |94.089  |33.33|-2174.92  |2207.58|\n",
        "|9  |7.157   |82.05|-50.38    |122.51 |\n",
        "|10 |1.953   |82.61|12.83     |43.52  |\n",
        "|11 |1.776   |100.0|82.32     |7.16   |\n",
        "|12 |1.266   |100.0|87.91     |12.09  |\n",
        "|A |7.04   |25.0|-300.25     |322.25 |\n",
        "\n",
        "O ambiente estoc√°stico de 0.1 n√£o foi suficiente para diminuir a qualidade do modelo nas salas 9, 10, 11 e 12, mas facilitou o aprendizado dos modelos em diversas salas, simulando uma taxa de explora√ß√£o\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte carlo sem aproximador e em ambiente estoc√°stico, 0.5 , 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo   |taxa |recompensa|passos |\n",
        "|-----------------|--------|-----|----------|-------|\n",
        "|1  |0.531   |100.0|92.0      |8.0    |\n",
        "|2  |0.731   |100.0|94.29     |5.71   |\n",
        "|3  |3.615   |22.22|-61.61    |83.06  |\n",
        "|3.1|4.015   |100.0|80.61     |19.39  |\n",
        "|3.2|1.061   |100.0|91.25     |8.75   |\n",
        "|4  |3.456   |100.0|62.26     |37.74  |\n",
        "|5  |7.088   |83.33|-79.26    |162.43 |\n",
        "|6  |56.169  |66.52|-1017.18  |1083.37|\n",
        "|7  |109.078 |21.28|-2051.02  |2071.51|\n",
        "|8  |104.403 |40.28|-1856.5   |1896.18|\n",
        "|9  |5.61    |87.18|-62.36    |115.51 |\n",
        "|10 |3.261   |52.17|-38.48    |69.13  |\n",
        "|11 |3.617   |63.16|-11.32    |53.05  |\n",
        "|12 |2.02    |97.14|57.14     |28.54  |\n",
        "|A |8.63 |25.0|-328.62     |352.88   |\n",
        "\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte carlo sem aproximador e em ambiente estoc√°stico, 0.7 , 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo   |taxa |recompensa|passos |\n",
        "|-----------------|--------|-----|----------|-------|\n",
        "|1  |0.563   |100.0|95.33     |4.67   |\n",
        "|2  |3.551   |100.0|90.57     |9.43   |\n",
        "|3  |3.477   |22.22|-60.33    |81.78  |\n",
        "|3.1|2.992   |78.26|30.04     |48.0   |\n",
        "|3.2|1.649   |100.0|79.54     |20.46  |\n",
        "|4  |7.842   |94.12|-30.68    |124.74 |\n",
        "|5  |11.378  |57.14|-168.4    |225.12 |\n",
        "|6  |105.498 |23.48|-1982.65  |2005.37|\n",
        "|7  |116.68  |17.02|-2150.62  |2166.81|\n",
        "|8  |112.991 |54.17|-1585.78  |1639.49|\n",
        "|9  |8.319   |64.1 |-188.23   |211.92 |\n",
        "|10 |2.864   |56.52|-56.26    |61.91  |\n",
        "|11 |3.286   |73.68|-28.68    |38.95  |\n",
        "|12 |3.196   |65.71|-56.49    |64.71  |\n",
        "|A |8.764 |37.5|-289.25     |276.12   |\n",
        "\n",
        "\n",
        "Para simplificar os experimentos, vamos apenas mostrar os resutados que destoam do padr√£o apresentado aqui, que valham a pena serem analisados, uma vez que h√° muitas salas e o tempo de treinamento pode ser muito extenso.\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte carlo sem aproximador e em ambiente determin√≠stico, com chance de explora√ß√£o 0.3, 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo |taxa |recompensa|passos |\n",
        "|-----------------|------|-----|----------|-------|\n",
        "|1  |0.316 |100.0|98.0      |2.0    |\n",
        "|2  |0.357 |100.0|97.71     |2.29   |\n",
        "|3  |1.748 |72.22|33.78     |38.17  |\n",
        "|3.1|0.749 |100.0|92.7      |7.3    |\n",
        "|3.2|0.536 |100.0|95.42     |4.58   |\n",
        "|4  |1.003 |100.0|88.35     |11.65  |\n",
        "|5  |1.404 |100.0|87.29     |12.71  |\n",
        "|6  |6.216 |87.39|-247.26   |334.52 |\n",
        "|7  |76.564|17.02|-2059.04  |2075.23|\n",
        "|9  |1.615 |89.74|31.79     |51.62  |\n",
        "|10 |2.349 |47.83|-24.83    |72.13  |\n",
        "|11 |1.127 |100.0|89.42     |10.58  |\n",
        "|12 |0.871 |100.0|90.34     |9.66   |\n",
        "|A |2.505 |75.0|-42.0     |113.38   |\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte carlo sem aproximador e em ambiente determin√≠stico, com chance de explora√ß√£o 0.5, 10000 epis√≥dios</b>\n",
        "\n",
        "\n",
        "|sala             |tempo |taxa |recompensa|passos |\n",
        "|-----------------|------|-----|----------|-------|\n",
        "|1  |0.342 |100.0|98.0      |2.0    |\n",
        "|2  |0.408 |100.0|97.71     |2.29   |\n",
        "|3  |2.483 |33.33|-36.06    |68.72  |\n",
        "|3.1|0.99  |100.0|92.7      |7.3    |\n",
        "|3.2|0.681 |100.0|95.42     |4.58   |\n",
        "|4  |1.297 |100.0|88.41     |11.59  |\n",
        "|5  |2.118 |95.24|64.5      |30.69  |\n",
        "|6  |4.177 |94.35|-67.93    |162.23 |\n",
        "|7  |92.047|23.4 |-1894.28  |1916.91|\n",
        "|9  |2.787 |74.36|-42.0     |111.95 |\n",
        "|10 |2.361 |43.48|-34.74    |77.65  |\n",
        "|11 |1.372 |100.0|89.42     |10.58  |\n",
        "|12 |1.168 |100.0|90.06     |9.94   |\n",
        "|A |2.934 |50.0|-159.88     |208.25   |\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte carlo com aproximador e em ambiente determin√≠stico, com chance de explora√ß√£o 0.7, 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo |taxa |recompensa|passos |\n",
        "|-----------------|------|-----|----------|-------|\n",
        "|1  |1.386 |100.0|98.0      |2.0    |\n",
        "|2  |2.151 |100.0|97.71     |2.29   |\n",
        "|3  |9.276 |11.11|-78.83    |89.06  |\n",
        "|3.1|7.881 |100.0|92.7      |7.3    |\n",
        "|3.2|11.568|8.33 |-84.5     |91.92  |\n",
        "|4  |48.806|0.0  |-401.0    |400.0  |\n",
        "|5  |32.211|2.38 |-389.1    |390.5  |\n",
        "|6  |199.533|2.17 |-2444.61  |2445.8 |\n",
        "|7  |155.405|0.0  |-2501.0   |2500.0 |\n",
        "|9  |31.694|10.26|-370.44   |359.28 |\n",
        "|10 |11.786|39.13|-192.26   |82.96  |\n",
        "|11 |9.795 |5.26 |-109.42   |113.74 |\n",
        "|12 |10.624|14.29|-106.83   |103.11 |\n",
        "|A |5.649 |12.5|-339.75     |351.38   |\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Sarsa sem aproximador e em ambiente estoc√°stico 0.5, com chance de explora√ß√£o 0.3, 10000 epis√≥dios</b>\n",
        "\n",
        "\n",
        "\n",
        "|sala             |tempo |taxa |recompensa|passos |\n",
        "|-----------------|------|-----|----------|-------|\n",
        "|1  |0.325 |100.0|97.33     |2.67   |\n",
        "|2  |0.528 |100.0|96.0      |4.0    |\n",
        "|3  |16.95 |44.44|-20.06    |63.94  |\n",
        "|3.1|4.352 |100.0|83.91     |16.09  |\n",
        "|3.2|1.699 |100.0|91.21     |8.79   |\n",
        "|4  |36.036|100.0|50.03     |49.97  |\n",
        "|5  |109.771|76.19|-65.76    |141.71 |\n",
        "|9  |96.33 |84.62|-124.03   |143.54 |\n",
        "|10 |16.164|78.26|-54.74    |44.78  |\n",
        "|11 |16.412|68.42|-25.84    |41.32  |\n",
        "|12 |9.702 |100.0|50.29     |32.57  |\n",
        "|A |91.132 |50.0|-259.62     |256.88   |\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Sarsa com aproximador e em ambiente estoc√°stico 0.5, com chance de explora√ß√£o 0.3, 1000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo |taxa |recompensa|passos |\n",
        "|-----------------|------|-----|----------|-------|\n",
        "|1  |0.375 |100.0|94.33     |5.67   |\n",
        "|2  |0.623 |85.71|72.86     |12.71  |\n",
        "|3  |9.175 |27.78|-56.33    |83.39  |\n",
        "|3.1|8.192 |39.13|-36.43    |74.96  |\n",
        "|3.2|11.044|45.83|-23.04    |68.33  |\n",
        "|4  |134.282|2.94 |-386.76   |388.74 |\n",
        "|5  |116.541|9.52 |-353.88   |362.5  |\n",
        "|9  |84.766|23.08|-325.21   |308.62 |\n",
        "|10 |3.683 |95.65|-41.22    |22.91  |\n",
        "|11 |3.948 |84.21|-36.21    |36.05  |\n",
        "|12 |15.872|80.0 |-29.74    |52.4   |\n",
        "|A |68.112 |50.0|-379.0     |328.5   |\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Q-Learning sem aproximador e em ambiente estoc√°stico 0.5, com chance de explora√ß√£o 0.3, 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo|taxa |recompensa|passos|\n",
        "|-----------------|-----|-----|----------|------|\n",
        "|1  |0.285|100.0|98.0      |2.0   |\n",
        "|2  |0.371|100.0|96.43     |3.57  |\n",
        "|3  |5.04 |38.89|-27.39    |65.67 |\n",
        "|3.1|1.9  |100.0|83.78     |16.22 |\n",
        "|3.2|1.025|100.0|90.17     |9.83  |\n",
        "|4  |5.896|100.0|33.26     |66.74 |\n",
        "|5  |11.829|64.29|-128.6    |192.52|\n",
        "|9  |11.097|76.92|-118.69   |156.31|\n",
        "|10 |3.837|86.96|-58.65    |41.3  |\n",
        "|11 |3.79 |68.42|-7.32     |43.84 |\n",
        "|12 |2.778|100.0|29.23     |19.34 |\n",
        "|A |10.448 |75.0|-167.88     |188.12   |\n",
        "\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Q-Learning com aproximador e em ambiente estoc√°stico 0.5, com chance de explora√ß√£o 0.3, 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo|taxa |recompensa|passos|\n",
        "|-----------------|-----|-----|----------|------|\n",
        "|1  |0.248|100.0|94.0      |6.0   |\n",
        "|2  |0.332|100.0|94.57     |5.43  |\n",
        "|3  |2.451|22.22|-63.06    |84.5  |\n",
        "|3.1|1.545|65.22|12.39     |52.48 |\n",
        "|3.2|2.067|37.5 |-39.29    |76.17 |\n",
        "|4  |9.483|0.0  |-401.0    |400.0 |\n",
        "|5  |9.706|7.14 |-365.52   |371.74|\n",
        "|9  |7.057|23.08|-319.82   |308.59|\n",
        "|10 |0.91 |95.65|-106.57   |36.61 |\n",
        "|11 |1.114|89.47|-120.11   |41.05 |\n",
        "|12 |2.714|77.14|-28.26    |48.03 |\n",
        "|A |62.505 |12.5|-375.0     |361.62   |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "√â importante ressaltar que nenhum modelo performou bem na sala 8, a maior sala dispon√≠vel, uma vez que n√£o foi poss√≠vel apresentar todos os estados para os modelos.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como forma de verificar a recompensa dos algoritmos com o passar do tempo, foi constru√≠da uma sala gen√©rica com diversos refor√ßos posiivos e negativos, varios estador finais e um tamanho grande o suficinete para explorar o \"medo\" do agente e estruturas de corredores e abertas.\n",
        "\n",
        "O treinamento ocorreu com o agente com chance de explora√ß√£o 0.3, estrat√©gia do monte carlo em um ambiente com estocasticidade 0.1. Vale lembrar que o custo da lava √© -100 sendo estado terminal e o custo do √°cido √© -10.\n",
        "\n",
        "![Sala A](figs/salaA.png)\n",
        "\n",
        "Ap√≥s o treinamento, temos a pol√≠tica final:\n",
        "\n",
        "![Politica final da sala A estoc√°stica](figs/politicaFinalSalaAsto01ep50000.png)\n",
        "\n",
        "O treinamento levou 14 segundos e a recompensa m√©dia foi de 70, com m√©dia de 24 steps.\n",
        "\n",
        "O treinamento teve as seguintes recompensas:\n",
        "\n",
        "![Recompensa no tempo Monte Carlo](figs/reward/rewardSalaAep50000.png)\n",
        "\n",
        "Aqui √© poss√≠vel verificar que o modelo convergiu rapidamente para a pol√≠tica √≥tima. Vale ressaltar que o gr√°fico foi contru√≠do atrave≈õ da m√©dia m√≥vel de 10% das amostras, ous eja, 5000 amostras.\n",
        "\n",
        "\n",
        "Um resutado estranho foi encontrado para o Sarsa: O treinamento atinge um m√°ximo e decai rapidamente perdendo informa√ß√£o:\n",
        "\n",
        "![Recompensa no tempoSarsa ](figs/reward/rewardSalaAep50000Sarsa.png)\n",
        "\n",
        "\n",
        "A pol√≠tica final nesse caso √© \n",
        "\n",
        "![Recompensa no tempoSarsa ](figs/Sarsa50000.png)\n",
        "\n",
        "\n",
        "Treinando por 10000 temos:\n",
        "\n",
        "\n",
        "![Recompensa no tempoSarsa ](figs/reward/rewardSalaAep10000Sarsa.png)\n",
        "\n",
        "E o resultado da pol√≠tica:\n",
        "\n",
        "![Recompensa no tempoSarsa ](figs/Sarsa10000.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analisando agora o parametro lambda para o Sarsa, ele ser√° variado de 0.01, 0.1 e 0.5, com 10000 epis√≥dios cada.\n",
        "Essa quantidade de epis√≥dios foi escolhida por que o modelo sofre de overflow caso o n√∫mero de epis√≥dios seja muito grande para lambda = 0.5.\n",
        "\n",
        "para lambda = 0.01, temos:\n",
        "![Recompensa no tempoSarsa ](figs/reward/Sarsa001.png)\n",
        "\n",
        "para lambda = 0.1, temos:\n",
        "![Recompensa no tempoSarsa ](figs/reward/Sarsa01.png)\n",
        "\n",
        "para lambda = 0.5, temos:\n",
        "![Recompensa no tempoSarsa ](figs/reward/Sarsa05.png)\n",
        "\n",
        "Podemos verficar que quanto maior o valor de lambda, mais rapidamente o modelo converge para a pol√≠tica √≥tima, mas √© necess√°rio balancear esse valor com o n√∫mero de epis√≥dios, uma vez que o modelo pode sofrer overflow e convergir para uma pol√≠tica em que todos os estados tem valores infinitos.\n",
        "\n",
        "|lambda             |tempo |taxa |recompensa|passos |\n",
        "|-----------------|------|-----|----------|-------|\n",
        "|0.01  |213.78  |37.5|-333.62     |295.5    |\n",
        "|0.1  |209.73 |100.0|-370.37     |295.5   |\n",
        "|0.5  |126.73 |87.5|-118.25    |119.625   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analisando agora os valores dos estados para todas as 6 possibilidades de algoritmos, temos:\n",
        "\n",
        "Monte Carlo sem aproximador linear, 10000 epis√≥dios, chance de explora√ß√£o 0.5\n",
        "![Valores dos estados para o Monte Carlo](figs/AmonteCarlo.png)\n",
        "\n",
        "Monte Carlo com aproximador linear, 10000 epis√≥dios, chance de explora√ß√£o 0.5\n",
        "![Valores dos estados para o Monte Carlo](figs/AmonteCarloAprox.png)\n",
        "\n",
        "Sarsa sem aproximador linear, 10000 epis√≥dios, chance de explora√ß√£o 0.5, lambda 0.5\n",
        "![Valores dos estados para o Sarsa](figs/Asarsa.png)\n",
        "\n",
        "Sarsa com aproximador linear, 10000 epis√≥dios, chance de explora√ß√£o 0.5, lambda 0.5\n",
        "![Valores dos estados para o Sarsa](figs/AsarsaAprox.png)\n",
        "\n",
        "Q-Learning sem aproximador linear, 10000 epis√≥dios, chance de explora√ß√£o 0.5\n",
        "![Valores dos estados para o Q-Learning](figs/AQLearning.png)\n",
        "\n",
        "Q-Learning com aproximador linear, 10000 epis√≥dios, chance de explora√ß√£o 0.5\n",
        "![Valores dos estados para o Q-Learning](figs/AQLearningAprox.png)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
