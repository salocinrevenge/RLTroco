{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Projeto 1\n",
        "\n",
        "Este trabalho compara a efici√™ncia e efic√°cia de m√∫ltiplos algoritmos de Aprendizado por Refor√ßo (RL), sendo eles MonteCarlo, Sarsa(Lambda) e Q_learning, em diferentes cen√°rios e variando seus par√¢metros quando necess√°rio. O ambiente/problema que foi usado como teste s√£o labirintos, onde os algoritmos precisam encontrar uma politica √≥tima para que o agente consiga chegar at√© o final do labirinto.\n",
        "\n",
        "### Ambiente\n",
        "Como dito anteriormente, os ambientes s√£o labirintos. Eles s√£o descritos por meio de arquivos .txt, onde s√£o apontados as paredes, caminhos, estados terminais, formato e suas recompensas. Os arquivos t√™m essa cara:\n",
        "```\n",
        "4 4\n",
        ". path -1\n",
        "@ agent -1\n",
        "$ goal 100\n",
        "# wall -1000\n",
        "######\n",
        "#...$#\n",
        "#@...#\n",
        "######\n",
        "```\n",
        "\n",
        "A primeira linha cont√©m dois inteiros E e L, que representam o n√∫mero de elementos e o n√∫mero de linhas do labirinto. Aqui s√£o descritos 4 elementos e ser√° assado um labirinto de 4 linhas. Para os elementos tem-se que os caminhos s√£o '.' e possuem recompensa de -1. Por sua vez, o estado terminal, ou objetivo, √© representado por '$' e possui recompensa de 100. Por fim, ap√≥s a descri√ß√£o dos simbolos, h√° a descri√ß√£o do pr√≥prio formato do labirinto.\n",
        "\n",
        "Diferentes formatos de labirintos foram adotados para o trabalho, sendo alguns bem amplos, outros com diversos caminhos, e outros extremamente estreitos. As recompensas tamb√©m foram alteradas para avaliar seu impacto, j√° que a escolha de uma fun√ß√£o de recompensa boa tamb√©m faz parte do processo de treinamento quando se trabalha com RL.\n",
        "\n",
        "### C√≥digo\n",
        "As primeiras c√©lulas desse notebook cont√©m o c√≥digo usado para implementa√ß√£o e obten√ß√£o dos resultados. Por√©m para facilitar, foram geradas imagens com os resultados obtidos (usando pygame), e a an√°lise foi feita em cima delas. O c√≥digo est√° dispon√≠vel para execu√ß√£o, altera√ß√£o, e teste de outras configura√ß√µes de labirinto.\n",
        "\n",
        "### M√©tricas\n",
        "Para compara√ß√£o dos algoritmos entre si, foram extra√≠das as recompensas m√©dias e o tamanho do caminho percorrido a partir de um mesmo ponto do mapa definidos na cria√ß√£o do mapa pelo valor 'agent', nesse caso '@'. Com isso √© poss√≠vel analisar o qu√£o eficiente foi o caminho encontrado pelo algoritmo, caso encontrado, e se ele conseguiu convergir. H√° tamb√©m a analise de desempenho e custo computacional, o <b style=\"color:#770000\">n√∫mero m√©dio de epis√≥dios para converg√™ncia</b>, o <b style=\"color:#770000\">tempo de converg√™ncia</b>, os <b style=\"color:#770000\">hiperparametros</b> necess√°rios para ajustar o algoritmo, e o qu√£o <b style=\"color:#770000\">dificil</b> foi encontrar um valor √≥timo.\n",
        "\n",
        "Al√©m disso, um mapa de calor gerado a partir da fun√ß√£o valor ser√° usado para ilustrar o qu√£o bom √© estar em cada estado, ou seja, em cada posi√ß√£o do labirinto. tamb√©m ser√° mostrado um gr√°fico contendo as recompensas do modelo ao longo do tempo, para que seja poss√≠vel analisar o qu√£o r√°pido o algoritmo convergiu. Esse gr√°fico utiliza do recurso da m√©dia m√≥vel para suavizar os dados e facilitar a visualiza√ß√£o e compreens√£o.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lftFAb-BqMew"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from itertools import product\n",
        "import random\n",
        "from threading import Thread\n",
        "import time\n",
        "from argparse import ArgumentParser\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OwSgimmVxFIw"
      },
      "outputs": [],
      "source": [
        "class Renderer():\n",
        "    def __init__(self, chief, content, title=None, dimensions=(800, 800)):\n",
        "        self.chief = chief\n",
        "        self.content = content\n",
        "        self.contents = [content] # para trocar entre janelas\n",
        "        self.iConteudoAtual = 0 # para marcar qual o atual dentre os varios\n",
        "        self.title = title\n",
        "        self.dimensions = dimensions\n",
        "        self.running = True\n",
        "        if not self.title:\n",
        "            self.title = type(chief).__name__   # title √© o nome da classe\n",
        "        \n",
        "        self.load_sprites()\n",
        "\n",
        "    def add_content(self,content):\n",
        "        self.contents.append(content)\n",
        "\n",
        "    def load_sprites(self):\n",
        "        self.sprites = dict()\n",
        "        self.sprites[\"path\"] = '‚¨õ'\n",
        "        self.sprites[\"wall\"] = 'üß±'\n",
        "        self.sprites[\"goal\"] = '‚öΩ'\n",
        "        self.sprites[\"agent\"] = 'üëæ' \n",
        "        self.sprites[\"right\"] = '‚û°Ô∏è'\n",
        "        self.sprites[\"up\"] = '‚¨ÜÔ∏è'\n",
        "        self.sprites[\"left\"] = '‚¨ÖÔ∏è' \n",
        "        self.sprites[\"down\"] = '‚¨áÔ∏è'\n",
        "\n",
        "\n",
        "    def show(self):\n",
        "        for i in range(len(self.content)):\n",
        "            for j in range(len(self.content[0])):\n",
        "                cell = self.content[i][j]\n",
        "                # se o content de cell estiver no dicionario de sprites\n",
        "                if cell in self.sprites:\n",
        "                    obj = cell\n",
        "                else:\n",
        "                    obj = self.chief.symbols[cell]\n",
        "                print(self.sprites.get(obj,'‚ùå'),end='')\n",
        "            print('')\n",
        "\n",
        "    def create_heatmap(data, cmap='viridis', title='Heatmap'):\n",
        "        \"\"\"\n",
        "        Create a heatmap from a list of lists of floats.\n",
        "\n",
        "        Parameters:\n",
        "        - data: List of lists of floats representing the heatmap data.\n",
        "        - cmap: Colormap for the heatmap (default is 'viridis').\n",
        "        - title: Title for the heatmap (default is 'Heatmap').\n",
        "        \"\"\"\n",
        "        data = np.array(data, dtype=float)\n",
        "\n",
        "        # Create a figure and axis\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        # Display the heatmap using imshow\n",
        "        im = ax.imshow(data, cmap=cmap)\n",
        "\n",
        "        # Add a colorbar to the right of the heatmap\n",
        "        cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "        # Set the title\n",
        "        ax.set_title(title)\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n",
        "\n",
        "    def show_path(self, path):\n",
        "        for i in range(len(self.content)):\n",
        "            for j in range(len(self.content[0])):\n",
        "                cell = self.content[i][j]\n",
        "                if((i,j) in path.keys()):\n",
        "                    cell = path[(i,j)]\n",
        "                # se o content de cell estiver no dicionario de sprites\n",
        "                if cell in self.sprites:\n",
        "                    obj = cell\n",
        "                else:\n",
        "                    obj = self.chief.symbols[cell]\n",
        "                print(self.sprites.get(obj,'‚ùå'),end='')\n",
        "            print('')\n",
        "\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    actions = ['up', 'down', 'left', 'right']\n",
        "    def __init__(self, x, y, environment, gamma = 0.9, display=True):\n",
        "        self.environment = environment\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.gamma = gamma\n",
        "        self.display = display\n",
        "\n",
        "    def action_idx(self, action: str):\n",
        "        return self.actions.index(action)\n",
        "\n",
        "\n",
        "    def startQ(self, shape, start_value = float(\"-inf\")):\n",
        "        \"\"\"\n",
        "        livroQ √© uma lista de listas de dicionarios,\n",
        "        ele armazena \n",
        "        \n",
        "        \"\"\"\n",
        "        self.book_Q: list[list[dict]] = []\n",
        "        for i in range(shape[0]):\n",
        "            self.book_Q.append([])\n",
        "            for _ in range(shape[1]):\n",
        "                self.book_Q[i].append(dict())\n",
        "                for action in self.actions:\n",
        "                    self.book_Q[i][-1][action] = start_value\n",
        "\n",
        "    def startV(self, shape):\n",
        "        \"\"\"\n",
        "        livroV √© uma lista de listas de dicionarios,\n",
        "        ele armazena \n",
        "        \n",
        "        \"\"\" \n",
        "        self.book_V: list[list] = []\n",
        "        for i in range(shape[0]):\n",
        "            self.book_V.append([])\n",
        "            for _ in range(shape[1]):\n",
        "                \n",
        "                self.book_V[i].append(float(\"-inf\"))\n",
        "    \n",
        "    def startPolicy(self, shape, randomPolicy):\n",
        "        \"\"\"\n",
        "        A policy √© uma matriz de caracteres que guarda a action principal\n",
        "        a ser tomada ate o momento\n",
        "        \n",
        "        \"\"\"\n",
        "        self.policy: list[list[str]] = []\n",
        "        for i in range(shape[0]):\n",
        "            self.policy.append([])\n",
        "            for j in range(shape[1]):\n",
        "                if self.environment.symbols[self.environment.original_map[i][j]] == \"wall\":\n",
        "                    self.policy[i].append(\"wall\")\n",
        "                    continue\n",
        "                if randomPolicy:\n",
        "                    self.policy[i].append(random.choice(self.actions))\n",
        "                else:\n",
        "                    self.policy[i].append(self.actions[0])\n",
        "        if self.display:\n",
        "            self.render = Renderer(self, self.policy, \"Agente\")\n",
        "\n",
        "    def startReturns(self, shape):\n",
        "        \"\"\"\n",
        "        returns √© uma colecao de pares state action guardando um\n",
        "        dicionario para armazenar o valor maximo de rewards obtidos,\n",
        "        o numero de vezes que o par state action foi visitado e o ultimo\n",
        "        episodio em que o par state action foi visitado\n",
        "        \"\"\"\n",
        "        self.returns: list[list[dict]] = []\n",
        "        for i in range(shape[0]):\n",
        "            self.returns.append([])\n",
        "            for j in range(shape[1]):\n",
        "                self.returns[i].append(dict())\n",
        "                for action in self.actions:\n",
        "                    self.returns[i][j][action] = {\"value\": 0, \"count\": 0, \"lastEpisode\": 0}\n",
        "\n",
        "    def setEnvironment(self, environment):\n",
        "        self.environment = environment\n",
        "    \n",
        "    def setPos(self, position):\n",
        "        self.x = position[1]\n",
        "        self.y = position[0]\n",
        "\n",
        "    def move(self, action):\n",
        "        return self.environment.move(self, action)\n",
        "    \n",
        "    def get_action(self):\n",
        "        return self.policy[self.y][self.x]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlGKK60NqTV4",
        "outputId": "d6167f45-bc5f-49b6-8732-de679f17aeec"
      },
      "outputs": [],
      "source": [
        "class LearningStrategy():\n",
        "    def train(self, episodes):\n",
        "        pass\n",
        "\n",
        "    def setup(self, environment, agent):\n",
        "        self.environment = environment\n",
        "        self.agent = agent\n",
        "\n",
        "class MonteCarlo(LearningStrategy):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.episode_R = []\n",
        "        self.episode_length = []\n",
        "        self.Q = None\n",
        "        self.W = np.random.rand(3)\n",
        "        self.time = []\n",
        "\n",
        "\n",
        "    def get_Q(self, x, y, action, linear_approximation = False):\n",
        "        if not linear_approximation:\n",
        "            return self.Q[x,y,action]\n",
        "        \n",
        "        terms = np.array([x,y,action])\n",
        "        return np.dot(self.W, terms).astype(int)\n",
        "    \n",
        "    def train(self, episodes, randomPolicy = True, exploration_chance = 0):\n",
        "        # Initialize\n",
        "        shape = self.environment.get_size()\n",
        "        self.agent.startPolicy(shape, randomPolicy)\n",
        "        self.agent.startReturns(shape)\n",
        "        self.Q = np.zeros((shape[0],shape[1], len(self.agent.actions)))\n",
        "        self.agent.startV(shape)\n",
        "\n",
        "        for ep in range(episodes):\n",
        "            start_time = time.time()\n",
        "            if ep % (episodes//10) == 0:\n",
        "                print(f\"{ep=}\")\n",
        "                path = self.path_from((1,1))\n",
        "                print('Tamanho do epis√≥dio:', len(path))\n",
        "                print('Recompensa', sum([i[2] for i in path]))\n",
        "                path_dict = {}\n",
        "                for s,a,r in path:\n",
        "                    path_dict[s] = a\n",
        "                self.environment.render.show_path(path_dict)\n",
        "            # escolhe posicao aleatoria valida para o agente\n",
        "            while True:\n",
        "                state = (random.randrange(0, shape[0]), random.randrange(0, shape[1]))\n",
        "                if self.environment.original_map[state[0]][state[1]] in {self.environment.default_symbols[\"path\"], self.environment.default_symbols[\"goal\"]}:\n",
        "                    break\n",
        "            # escolhe uma action diferente da dita pela politica atual\n",
        "            for _ in range(len(self.agent.actions)*2):    # limite maximo de tentativas\n",
        "                action = random.choice(self.agent.actions)\n",
        "                if action != self.agent.policy[state[0]][state[1]]:\n",
        "                    # se a action nao te leva para uma parede\n",
        "                    if self.environment.util(state, action): \n",
        "                        break\n",
        "            else:\n",
        "                action = self.agent.policy[state[0]][state[1]]\n",
        "            self.episode(state, action, max_steps= shape[1]*shape[0], exploration_chance = exploration_chance)\n",
        "            g = 0\n",
        "            for t in range(len(self.agent.recalls)-1, -1, -1): \n",
        "                memory = self.agent.recalls[t]  # memory = (state, action, reforco)\n",
        "                g = self.agent.gamma*g + memory[2]\n",
        "                # verifica se o par state action ja foi inserido em returns\n",
        "                if self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"lastEpisode\"] != ep:\n",
        "                    self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"lastEpisode\"] = ep\n",
        "                    self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"value\"] += g\n",
        "                    self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"count\"] += 1\n",
        "                    media = self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"value\"]/self.agent.returns[memory[0][0]][memory[0][1]][memory[1]][\"count\"]\n",
        "                    self.Q[memory[0][0],memory[0][1],self.agent.action_idx(memory[1])] = media\n",
        "                    self.agent.book_V[memory[0][0]][memory[0][1]] = media\n",
        "                    self.agent.policy[memory[0][0]][memory[0][1]] = max(self.agent.actions, key = lambda action: self.get_Q(memory[0][0],memory[0][1],self.agent.action_idx(action)))    # recebe a action que maximiza o valor de Q\n",
        "            end_time = time.time()\n",
        "            time_difference_seconds = end_time - start_time\n",
        "            self.time.append(time_difference_seconds)\n",
        "\n",
        "    def episode(self, state, action, max_steps, exploration_chance=0):\n",
        "        step_count = 0\n",
        "        self.agent.recalls = []\n",
        "        self.environment.setAgentPos(state[0], state[1])\n",
        "        episode_R = []\n",
        "        while (not self.environment.in_terminal_state()) and (step_count < max_steps):  # enquanto nao estiver em um state terminal\n",
        "            step_count +=1  # incrementa o numero de passos\n",
        "            lastPos = (self.agent.y, self.agent.x)\n",
        "            reward = self.environment.move(self.agent,action) # realiza a action e recebe a recompensa\n",
        "            episode_R.append(reward)\n",
        "            self.agent.recalls.append((lastPos, action, reward)) # guarda o passo\n",
        "            if random.random() < exploration_chance:\n",
        "                for _ in range(len(self.agent.actions)*2):    # limite maximo de tentativas\n",
        "                    action = random.choice(self.agent.actions)\n",
        "                    # se a action nao te leva para uma parede\n",
        "                    if self.environment.util(state, action): break\n",
        "            else:\n",
        "                action = self.agent.get_action() # escolhe uma action de acordo com a politica\n",
        "        self.episode_R.append(episode_R)\n",
        "        self.episode_length.append(step_count)\n",
        "\n",
        "    def path_from(self, starting_point):\n",
        "        shape = self.environment.get_size()\n",
        "        max_steps = shape[0] * shape[1]\n",
        "        step_count = 0\n",
        "        state = starting_point\n",
        "        self.environment.setAgentPos(state[0], state[1])\n",
        "        tuples = []\n",
        "        while (not self.environment.in_terminal_state()) and (step_count < max_steps):\n",
        "            action =  self.agent.policy[state[0]][state[1]]\n",
        "            R = self.environment.move(self.agent, action)\n",
        "            tuples.append((state,action,R))\n",
        "            state = (self.agent.y, self.agent.x)\n",
        "            step_count+=1\n",
        "        return tuples\n",
        "\n",
        "\n",
        "class SARSA(LearningStrategy):\n",
        "\n",
        "    def __init__(self, lam):\n",
        "        super().__init__()\n",
        "        self.lam = lam\n",
        "        self.episode_R = []\n",
        "        self.episode_length = []\n",
        "        self.Q = None\n",
        "        self.W = np.random.rand(3)\n",
        "        self.time = []\n",
        "\n",
        "    def get_Q(self, x, y, action, linear_approximation = False):\n",
        "        if not linear_approximation:\n",
        "            return self.Q[x,y,action]\n",
        "        \n",
        "        terms = np.array([x,y,action])\n",
        "        return np.dot(self.W, terms).astype(int)\n",
        "        \n",
        "\n",
        "    def get_greedy_action(self,state):\n",
        "        return max(self.agent.actions, key = lambda action: self.get_Q(state[0], state[1], self.agent.action_idx(action)))\n",
        "    \n",
        "    def get_epsilon_greedy(self, exploration_chance, state):\n",
        "        if random.random() < exploration_chance:\n",
        "            return random.choice(self.agent.actions)\n",
        "        else:\n",
        "            return max(self.agent.actions, key = lambda action: self.get_Q(state[0], state[1], self.agent.action_idx(action)))\n",
        "                \n",
        "    def train(self, episodes, random_policy=True, exploration_chance=0.3, alpha=0.001):\n",
        "        shape = self.environment.get_size()\n",
        "        ec = exploration_chance\n",
        "        num_states = shape[0]*shape[1]\n",
        "        linear_decay = exploration_chance/episodes\n",
        "        self.Q = np.zeros((shape[0],shape[1], len(self.agent.actions)))\n",
        "\n",
        "        E = dict()\n",
        "        for ep in range(episodes):\n",
        "            start_time = time.time()\n",
        "            episode_R = []\n",
        "            if ep % (episodes//10) == 0: \n",
        "                print(f\"{ep=}\")\n",
        "                path = self.path_from((1,1))\n",
        "                print('Tamanho do epis√≥dio:', len(path))\n",
        "                print('Recompensa', sum([i[2] for i in path]))\n",
        "                path_dict = {}\n",
        "                for s,a,r in path:\n",
        "                    path_dict[s] = a\n",
        "                self.environment.render.show_path(path_dict)\n",
        "\n",
        "            E = dict()         \n",
        "            \n",
        "            # escolhe posicao aleatoria valida para o agente\n",
        "            while True:\n",
        "                S = (random.randrange(0, shape[0]), random.randrange(0, shape[1]))\n",
        "                if self.environment.original_map[S[0]][S[1]] in {self.environment.default_symbols[\"path\"], self.environment.default_symbols[\"goal\"]}:\n",
        "                    break\n",
        "            self.environment.setAgentPos(S[0], S[1])\n",
        "\n",
        "            A = self.get_epsilon_greedy(ec,S)\n",
        "            A_idx = self.agent.action_idx(A)\n",
        "\n",
        "            step_count = 0\n",
        "            while (not self.environment.in_terminal_state()) and (step_count < num_states):\n",
        "\n",
        "                R = self.environment.move(self.agent, A)\n",
        "                episode_R.append(R)\n",
        "                S_prime = (self.agent.y, self.agent.x)\n",
        "                A_prime = self.get_epsilon_greedy(ec, S_prime)\n",
        "                A_prime_idx = self.agent.action_idx(A)\n",
        "                \n",
        "                pair = (S, A)\n",
        "                E[pair] = E[pair] + 1 if pair in E.keys() else 1\n",
        "\n",
        "                delta = R + self.agent.gamma * self.get_Q(S_prime[0], S_prime[1], A_prime_idx) - self.get_Q(S[0], S[1], A_idx)\n",
        "\n",
        "                for (s, a) in E.keys():\n",
        "                    a_idx = self.agent.action_idx(a)\n",
        "                    self.Q[s[0],s[1], a_idx] += alpha * delta * E[(s,a)]\n",
        "                    E[(s,a)] *= self.agent.gamma * self.lam\n",
        "                \n",
        "                S = S_prime\n",
        "                A = A_prime\n",
        "                step_count += 1\n",
        "            self.episode_R.append(episode_R)\n",
        "            self.episode_length.append(step_count)\n",
        "            ec-=linear_decay\n",
        "            end_time = time.time()\n",
        "            time_difference_seconds = end_time - start_time\n",
        "            self.time.append(time_difference_seconds)\n",
        "            \n",
        "\n",
        "        self.agent.startPolicy(shape, random_policy)\n",
        "        for i in range(shape[0]):\n",
        "            for j in range(shape[1]):\n",
        "                if(self.environment.original_map[i][j] == '#'): self.agent.policy[i][j] = \"wall\"\n",
        "                else: \n",
        "                    self.agent.policy[i][j] = max(self.agent.actions, key = lambda action: self.get_Q(i,j, self.agent.action_idx(action)))\n",
        "\n",
        "    def path_from(self, starting_point):\n",
        "        shape = self.environment.get_size()\n",
        "        max_steps = shape[0] * shape[1]\n",
        "        step_count = 0\n",
        "        S = starting_point\n",
        "        self.environment.setAgentPos(S[0], S[1])\n",
        "        tuples = []\n",
        "        while (not self.environment.in_terminal_state()) and (step_count < max_steps):\n",
        "            A = self.get_greedy_action(S)\n",
        "            R = self.environment.move(self.agent, A)\n",
        "            tuples.append((S,A,R))\n",
        "            S = (self.agent.y, self.agent.x)\n",
        "            step_count+=1\n",
        "        return tuples\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "class LinearFunctionApproximation(LearningStrategy):\n",
        "    ...\n",
        "\n",
        "class QLearning(LearningStrategy):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.episode_R = []\n",
        "        self.episode_length = []\n",
        "        self.Q = None\n",
        "        self.W = np.random.rand(3)\n",
        "        self.time = []\n",
        "\n",
        "    def get_Q(self, x, y, action, linear_approximation = False):\n",
        "        if not linear_approximation:\n",
        "            return self.Q[x,y,action]\n",
        "        \n",
        "        terms = np.array([x,y,action])\n",
        "        return np.dot(self.W, terms).astype(int)\n",
        "\n",
        "    def get_greedy_action(self,state):\n",
        "        return max(self.agent.actions, key = lambda action: self.get_Q(state[0], state[1], self.agent.action_idx(action)))\n",
        "    \n",
        "    def get_epsilon_greedy(self, exploration_chance, state):\n",
        "        if random.random() < exploration_chance:\n",
        "            return random.choice(self.agent.actions)\n",
        "        else:\n",
        "            return max(self.agent.actions, key = lambda action: self.get_Q(state[0], state[1], self.agent.action_idx(action)))\n",
        "                \n",
        "    def train(self, episodes, random_policy=True, exploration_chance=0.3, alpha=0.003):\n",
        "        shape = self.environment.get_size()\n",
        "        ec = exploration_chance\n",
        "        num_states = shape[0]*shape[1]\n",
        "        linear_decay = exploration_chance/episodes\n",
        "        self.Q = np.zeros((shape[0],shape[1], len(self.agent.actions)))\n",
        "\n",
        "        for ep in range(episodes):\n",
        "            start_time = time.time()\n",
        "            episode_R = []\n",
        "            if ep % (episodes//10) == 0: \n",
        "                print(f\"{ep=}\")\n",
        "                path = self.path_from((1,1))\n",
        "                print('Tamanho do epis√≥dio:', len(path))\n",
        "                print('Recompensa', sum([i[2] for i in path]))\n",
        "                path_dict = {}\n",
        "                for s,a,r in path:\n",
        "                    path_dict[s] = a\n",
        "                self.environment.render.show_path(path_dict)\n",
        "\n",
        "            # escolhe posicao aleatoria valida para o agente\n",
        "            while True:\n",
        "                S = (random.randrange(0, shape[0]), random.randrange(0, shape[1]))\n",
        "                if self.environment.original_map[S[0]][S[1]] in {self.environment.default_symbols[\"path\"], self.environment.default_symbols[\"goal\"]}:\n",
        "                    break\n",
        "            self.environment.setAgentPos(S[0], S[1])\n",
        "\n",
        "            #A = random.choice(self.agent.actions)\n",
        "            step_count = 0\n",
        "            while (not self.environment.in_terminal_state()) and (step_count < num_states):\n",
        "                A = self.get_epsilon_greedy(ec,S)\n",
        "                A_idx = self.agent.action_idx(A)\n",
        "\n",
        "                R = self.environment.move(self.agent, A)\n",
        "\n",
        "                episode_R.append(R)\n",
        "\n",
        "                S_prime = (self.agent.y, self.agent.x)\n",
        "                A_prime = self.get_greedy_action(S_prime)\n",
        "                A_prime_idx = self.agent.action_idx(A)\n",
        "\n",
        "\n",
        "                self.Q[S[0], S[1], A_idx] += alpha*(R + self.agent.gamma * self.get_Q(S_prime[0], S_prime[1], A_prime_idx) - self.get_Q(S[0], S[1], A_idx))\n",
        "\n",
        "                \n",
        "                S = S_prime\n",
        "                step_count += 1\n",
        "                \n",
        "            self.episode_R.append(episode_R)\n",
        "            self.episode_length.append(step_count)\n",
        "            ec-=linear_decay\n",
        "            end_time = time.time()\n",
        "            time_difference_seconds = end_time - start_time\n",
        "            self.time.append(time_difference_seconds)\n",
        "            \n",
        "\n",
        "        self.agent.startPolicy(shape, random_policy)\n",
        "        self.agent.startV(shape)\n",
        "        for i in range(shape[0]):\n",
        "            for j in range(shape[1]):\n",
        "                if(self.environment.original_map[i][j] == '#'): self.agent.policy[i][j] = \"wall\"\n",
        "                else: \n",
        "                    self.agent.policy[i][j] = max(self.agent.actions, key = lambda action: self.get_Q(i,j, self.agent.action_idx(action)))\n",
        "                    self.agent.book_V[i][j] = max(self.Q[i,j,:])\n",
        "\n",
        "    def path_from(self, starting_point):\n",
        "        shape = self.environment.get_size()\n",
        "        max_steps = shape[0] * shape[1]\n",
        "        step_count = 0\n",
        "        S = starting_point\n",
        "        self.environment.setAgentPos(S[0], S[1])\n",
        "        tuples = []\n",
        "        while (not self.environment.in_terminal_state()) and (step_count < max_steps):\n",
        "            A = self.get_greedy_action(S)\n",
        "            R = self.environment.move(self.agent, A)\n",
        "            tuples.append((S,A,R))\n",
        "            S = (self.agent.y, self.agent.x)\n",
        "            step_count+=1\n",
        "        return tuples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Environment:\n",
        "    default_symbols = {\"agent\": '@', \"wall\": '#', \"path\": '.', \"goal\":'$'}\n",
        "    def __init__(self, path, display=True, starting_point = (1,1)) -> None:\n",
        "        self.display = display\n",
        "        self.original_map = self.load_map(path)\n",
        "        self.map = self.copy_map(self.original_map)\n",
        "        self.wait_time = 0\n",
        "        self.starting_point = starting_point\n",
        "\n",
        "        if self.display:\n",
        "            self.render = Renderer(self, self.map, \"Ambiente\")\n",
        "            print(self.render)\n",
        "\n",
        "    def copy_map(self, map):\n",
        "        map_copy = []\n",
        "        for row in map:\n",
        "            map_copy.append([])\n",
        "            for cell in row:\n",
        "                map_copy[-1].append(cell)\n",
        "        return map_copy\n",
        "\n",
        "    def getAgent(self) -> Agent:\n",
        "        return self.agent\n",
        "\n",
        "    def in_terminal_state(self):\n",
        "        return self.original_map[self.agent.y][self.agent.x] == self.default_symbols[\"goal\"]\n",
        "\n",
        "\n",
        "    def load_map(self, path):\n",
        "        \"\"\"\n",
        "        Dado o caminho path, le um file txt e retorna uma matriz\n",
        "        O txt consiste de uma row contendo o numero n (n√∫mero de\n",
        "        rows do grid) e m (n√∫mero de caracteres diferentes no grid),\n",
        "        seguido de m rows explicando o que sao os caracteres no \n",
        "        file e por fim n rows contendo o grid que sao caracteres\n",
        "        \"\"\"\n",
        "\n",
        "        grid = []\n",
        "        self.symbols = dict()\n",
        "        self.rewards = {\"agent\": 0, \"wall\": 0, \"path\": 0, \"goal\":0}\n",
        "        with open(path, 'r') as file:\n",
        "            m, n = map(int, file.readline().split())\n",
        "            for _ in range(m):\n",
        "                row = file.readline().split()\n",
        "                self.symbols[row[0]] = row[1]\n",
        "                self.rewards[row[1]] = int(row[2])\n",
        "            for i in range(n):\n",
        "                row = file.readline()\n",
        "                grid.append([])\n",
        "                for j in range(len(row)):\n",
        "                    char = row[j]\n",
        "                    if char == '\\n':\n",
        "                        continue\n",
        "                    if self.symbols[char] == 'agent':\n",
        "                        self.agent = Agent(x=j, y=i, environment=self, display=self.display)\n",
        "                        char = self.default_symbols[\"path\"]\n",
        "                    grid[-1].append(self.default_symbols[self.symbols[char]])\n",
        "        return grid\n",
        "    \n",
        "    def move(self, agent, action):\n",
        "        \"\"\"\n",
        "        Dada uma action, move o agente no map\n",
        "        a action pode ser \"up\", \"down\", \"left\" ou \"right\"\n",
        "        \"\"\"\n",
        "        #time.sleep(self.wait_time)\n",
        "        direction = {\"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)}\n",
        "        final_pos = (agent.y+direction[action][0], agent.x+direction[action][1])\n",
        "        if self.map[final_pos[0]][final_pos[1]] != self.default_symbols[\"wall\"]:\n",
        "            # seta a posicao atual como caminho\n",
        "            self.map[agent.y][agent.x] = self.original_map[agent.y][agent.x]\n",
        "            \n",
        "            # seta a posicao final como o agente\n",
        "            self.map[final_pos[0]][final_pos[1]] = self.default_symbols[\"agent\"]\n",
        "            \n",
        "            # atualiza a posicao do agente\n",
        "            agent.setPos(final_pos)\n",
        "        # retorna o reforco da posicao final \n",
        "        return self.rewards[self.symbols[self.original_map[agent.y][agent.x]]]\n",
        "\n",
        "    def util(self, pos, action):\n",
        "        \"\"\"\n",
        "        Dada uma posicao e uma action, retorna o que tem na posicao destino\n",
        "        \"\"\"\n",
        "        direction = {\"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)}\n",
        "        final_pos = (pos[0]+direction[action][0], pos[1]+direction[action][1])\n",
        "        return self.symbols[self.map[final_pos[0]][final_pos[1]]] != \"wall\"\n",
        "\n",
        "    def get_size(self):\n",
        "        return len(self.map), len(self.map[0])\n",
        "    \n",
        "    def setAgentPos(self, i, j):\n",
        "        self.map[self.agent.y][self.agent.x] = self.original_map[self.agent.y][self.agent.x]\n",
        "        self.agent.setPos((i, j))\n",
        "        self.map[i][j] = self.default_symbols[\"agent\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<__main__.Renderer object at 0x7fc9fe2a0100>\n",
            "ep=0\n",
            "Tamanho do epis√≥dio: 24\n",
            "Recompensa -24\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚¨áÔ∏è‚öΩüß±\n",
            "üß±‚¨õ‚¨õ‚¨ÜÔ∏è‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=1000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=2000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=3000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=4000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=5000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=6000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=7000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=8000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "ep=9000\n",
            "Tamanho do epis√≥dio: 3\n",
            "Recompensa 98\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüëæüß±\n",
            "üß±‚¨õ‚¨õ‚¨õ‚¨õüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏è‚û°Ô∏èüß±\n",
            "üß±‚û°Ô∏è‚û°Ô∏è‚û°Ô∏è‚¨ÜÔ∏èüß±\n",
            "üß±üß±üß±üß±üß±üß±\n",
            "Done\n",
            "0.3901517391204834\n"
          ]
        }
      ],
      "source": [
        "learning_strategy = MonteCarlo()\n",
        "environment = Environment('./2.txt', True)\n",
        "agent = environment.getAgent()\n",
        "learning_strategy.setup(environment, agent)\n",
        "\n",
        "learning_strategy.train(10000, exploration_chance=0.5)\n",
        "agent.render.show()\n",
        "print(\"Done\")\n",
        "print(sum(learning_strategy.time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "Renderer.create_heatmap(learning_strategy.agent.book_V, cmap='inferno', title='Sample Heatmap')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <b style=\"color:#009900\">An√°lise do Monte Carlo em ambiente determin√≠stico</b>\n",
        "\n",
        "O algoritmo monte carlo utilizado foi colocado em diversas salas com diversos par√¢metros diferentes e algumas varia√ß√µes no seu algoritmo. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##  <b style=\"color:#22BBBB\">Sala 1</b>\n",
        "A figura abaixo mostra a estrutura da primeira sala usada para avalia√ß√£o do modelo, a sala mais simples presente.\n",
        "\n",
        "![Sala 1](figs/sala%201.png)\n",
        "\n",
        "Aqui √© poss√≠vel visualizar o objetivo na forma de uma moeda e o agente na forma de um alien verde. Al√©m disso √© poss√≠vel ver os caminhos v√°lidos em preto e as paredes em roza. Cada caminho v√°lido aqui garante uma recompensa de -1 ao chegar, enquanto o objetivo garante uma recompensa de 100. Movimentos para cima ou para baixo nesse mapa s√£o sempre em dire√ß√£o a uma parede, o que n√£o move o agente de lugar mas garante uma recompensa de -1. Assim √© esperado que esse tipo de atitude seja evitada com o passar do tempo. A pol√≠tica √© inicializada de forma aleat√≥ria e pode ser verificada na forma de setas azuis, como na figura abaixo:\n",
        "\n",
        "![Politica inicial da sala 1](figs/politicaInicialSala1.png)\n",
        "\n",
        "Um primeiro obst√°culo foi verificado aqui: Como a pol√≠tica inicial pode iniciar direcionando o agente para um local inv√°lido, o mantendo preso para sempre, √© necess√°rio definir um valor m√°ximo de passos para cada epis√≥dio. Aqui esse valor foi de 10 passos. Ou seja, o epis√≥dio acaba, ou quando o agente atinge o estado terminal, ou ap√≥s 10 passos.\n",
        "Ap√≥s 10 epis√≥dios nesse cen√°rio o algoritmo n√£o apresentou uma converg√™ncia satisfat√≥ria. Por√©m com 12 epis√≥dios a converg√™ncia j√° foi quase absoluta. O n√∫mero 12 aqui √© curioso pois √© m√∫ltiplo to n√∫mero de a√ß√µes dispon√≠veis (4). E nesse cen√°rio h√° 3 poss√≠veis locais onde o personagem pode surgir e tomar uma a√ß√£o v√°lida (j√° que se ele surgir no estado terminal a simula√ß√£o acaba o impedindo de tomar uma a√ß√£o). Assim, 12 √© um n√∫mero suficiente para ele testar, com uma boa probabilidade, todas as possibilidades desse ambiente, chegando por fim na pol√≠tica √≥tima abaixo:\n",
        "\n",
        "![Politica final da sala 1](figs/politicaFinalSala1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##  <b style=\"color:#22BBBB\">Sala 2</b>\n",
        "A figura abaixo mostra a estrutura da segunda sala usada para avalia√ß√£o do modelo, uma das salas mais simples presentes, mas que aumenta levemente o n√∫mero de caminhos v√°lidos dispon√≠veis e possibilita um movimento vertical limitado.\n",
        "\n",
        "![Sala 2](figs/sala%202.png)\n",
        "\n",
        "Diferente da primeira sala, aqui os movimentos s√£o menos limitados e h√° mais de um caminho √≥timo para o agente descobrir. A pol√≠tica inicial aleat√≥ria pode ser vista como abaixo:\n",
        "\n",
        "![Politica inicial da sala 2](figs/politicaInicialSala2.png)\n",
        "\n",
        "Aqui os 12 epis√≥dios n√£o foram suficientes para obter alguma pol√≠tica √≥tima, e nem era esperado que isso acontece, pois o n√∫mero de movimentos v√°lidos e caminhos aumentou. Observando A imagem √© poss√≠vel veerificar que h√° 7 estados n√£o terminais, nos quais 4 possuem apenas 1 movimento √≥timo e 3 com 2 movimentos √≥timos (direita ou cima). Assim, h√° um total de 28 op√ß√µes totais das quais 10 s√£o √≥timas. Isso indica que o agente precisa testar no m√≠nimo 18 op√ß√µes distintas para encontrar uma pol√≠tica √≥tima. Treinando o modelo com 18 epis√≥dios temos (no melhor resultado):\n",
        "\n",
        "![Politica da sala 2 com 18 epis√≥dios](figs/politicaFinalSala2ep18.png)\n",
        "\n",
        "Aqui podemos ver que o agente errou em apenas 1 estado a pol√≠tica √≥tima, por√©m nesse estado ele fica preso. Al√©m disso, esse resultado n√£o reflete o caso m√©dio, que √© bem mais desordenado, mostrando que esse caso foi originado por uma pol√≠tica inicial aleat√≥ria muito boa e 18 epis√≥dios n√£o √© o suficiente para treinar o agente de forma adequada. Colocando 36 epis√≥dios de treinamento temos uma pol√≠tica mais est√°vel mas n√£o √≥tima no caso m√©dio, como √© poss√≠vel ver na imagem abaixo:\n",
        "\n",
        "![Politica da sala 2 com 36 epis√≥dios](figs/politicaFinalSala2ep36.png)\n",
        "\n",
        "Aqui √© poss√≠vel ver que o modelo n√£o est√° conseguindo convergir para a pol√≠tica √≥tima, mas est√° encontrando um caminho v√°lido. Isso pode ser corrigido com mais epis√≥dios ou por altera√ß√£o do valor do refor√ßo final, uma vez que a adi√ß√£o de um √∫nico caminho a mais (refor√ßo somado em -1) n√£o tem muito valor quando comparado com o total ganho no final (100).\n",
        "\n",
        "Ap√≥s um processo de busca bin√°ria no n√∫mero de epis√≥dios, foi poss√≠vel encontrar o valor de 130 como um valor em que todas as pol√≠ticas geradas para 20 amostras foram √≥timas.\n",
        "\n",
        "![Politica final da sala 2](figs/politicaFinalSala2.png)\n",
        "\n",
        "Aqui podemos ver que o valor foi muito superior a 18, evidenciando uma depend√™ncia bem mais complexa que a inferida anteriormente, sendo necess√°rio um teste mais preciso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  <b style=\"color:#22BBBB\">Sala 4</b>\n",
        "\n",
        "Aqui a sala √© bem maior mas ainda completamente ampla\n",
        "\n",
        "![Sala 4](figs/sala%202.png)\n",
        "\n",
        "Nesse cen√°rio, foi necess√°rio a altera√ß√£o doo limite de passos, uma vez que mesmo com a pol√≠tica √≥tima, seria imposs√≠vel que o agente iniciado no canto inferior esquerdo atingisse o estado terminal. Sendo assim ele passou a ser dependente da dimens√£o do mapa, valendo o n√∫mero total de quadrados da sala, nesse caso 400. Com isso, ap√≥s 30000 epis√≥dios a pol√≠tica convergiu para uma pol√≠tica pr√≥xima da √≥tima:\n",
        "\n",
        "![Politica final da sala 4](figs/politicaFinalSala2.png)\n",
        "\n",
        "Aqui √© poss√≠vel verificar que n√£o foram todos os campos com a pol√≠tica perfeita, mas a adic√£o de mais epis√≥dios toma muito tempo para atingir a pol√≠tica perfeita, logo a partir de agora ser√° dada a pol√≠tica ideal com alguma toler√¢ncia em rela√ß√£o a pol√≠tica √≥tima."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  <b style=\"color:#22BBBB\">Sala 3</b>\n",
        "\n",
        "Aqui a sala possui obst√°culos no meio do caminho, tornando mais complexa a identifica√ß√£o da pol√≠tica √≥tima. Al√©m disso o final do caminho (pr√≥ximo ao objetivo) √© muito estreito, o que pode gerar resultados inesperados\n",
        "\n",
        "![Sala 3](figs/sala%203.png)\n",
        "\n",
        "Aqui um resultado peculiar ocorreu:\n",
        "\n",
        "![Politica final da sala 3](figs/politicaFinalSala3ep100.png)\n",
        "\n",
        "Foi verificado que a pol√≠tica √≥tima precisa evoluir a partir do objetivo. O que significa que salas grandes com corredores estreitos podem custar muito para o agente aprender a resolver. Com 100 √©pocas apenas uma regi√£o do corredor estava dispon√≠vel, enquanto que para 200, uma regi√£o maior ficou dispon√≠vel, semelhante ao porcesso de uma busca em largura em um grafo. Assim, foi considerado uma altera√ß√£o no algoritmo principal. Agora o agente possui uma chance n√£o seguir a pol√≠tica e experimentar uma nova possibilidade. Assim ele n√£o fica t√£o restrito a posi√ß√£o inicial nem a pol√≠tica aleat√≥ria inicial. Dessa forma foram necess√°rios 10000 epis√≥dios para converg√™ncia da pol√≠tica. Mas uma nova caracter√≠stica foi verificada: Com essa taxa de explora√ß√£o fixa, o agente esquecia da pol√≠tica aprendida em algumas regi√µes. Para resolver isso, a taxa de explora√ß√£o agora decaia com o n√∫mero de epis√≥dios, sendo multiplicada por 0.999 todo final de epis√≥dio. Ao final foi produzida a seguinte pol√≠tica:\n",
        "\n",
        "![Politica final da sala 3](figs/politicaFinalSala3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  <b style=\"color:#22BBBB\">Salas 5 7 e 8</b>\n",
        "Mais salas foram adicionadas, para a sala 5:\n",
        "\n",
        "![Sala 5](figs/sala%205.png)\n",
        "\n",
        "foram necess√°rios 30000 epis√≥dios para atingir a pol√≠tica ideal:\n",
        "\n",
        "![Politica final da sala 5](figs/politicaFinalSala5.png)\n",
        "\n",
        "enquanto para a sala 7:\n",
        "\n",
        "![Sala 7](figs/sala%207.png)\n",
        "\n",
        "O valor de 30000 levou muito mais tempo para finalizar, gerando a pol√≠tica:\n",
        "\n",
        "![Politica final da sala 7](figs/politicaFinalSala7ep30000.png)\n",
        "\n",
        "Aqui podemos ver que apenas o corredor inicial foi analisado, mas o tempo de treinamento foi de 3 minutos, o que torna o processo muito custoso.\n",
        "\n",
        "Para a √∫ltima sala, a 8 temos:\n",
        "\n",
        "![Sala 8](figs/sala%208.png)\n",
        "\n",
        "O treinamento com 1000 √©pocas custou 45 segundos, indicando um custo computacional elevado para computar todos epis√≥dios. Assim √© poss√≠vel verificar que o Monte Carlo possui muitas dificuldades quando o ambiente possui muitos estados diferentes.\n",
        "\n",
        "![Politica final da sala 7](figs/politicaFinalSala8.png)\n",
        "\n",
        "Com essas 1000 √©pocas o agente n foi capaz de aprender nem mesmo o estado ao lado do terminal, logo nada foi aprendido nesse ambiente com tantos cen√°rios.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <b style=\"color:#009900\">An√°lise do Monte Carlo em ambiente estoc√°stico</b>\n",
        "A estoc√°sticidade do ambiente √© dado por um valor racional de 0 a 1 e simboliza a chance de determinada a√ß√£o trocar para uma outra a√ß√£o aleat√≥ria. Para s = 0.33 significa que h√° 33% de chance da a√ß√£o escolhida (ir para a direita por exemplo), trocar para outra a√ß√£o aleat√≥ria (ir para cima por exemplo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  <b style=\"color:#22BBBB\">Sala 1</b>\n",
        "\n",
        "Para uma estocasticidade de 0.1, nenhuma altera√ß√£o significativa foi percebida, o ambiente se comporta da mesma forma, levando o mesmo n√∫mero de epis√≥dios para convergir ara a pol√≠tica √≥tima. Com o aumento do n√∫mero de √©pocas, nenhuma mudan√ßa na pol√≠tica foi verificada.\n",
        "\n",
        "Para s = 0.5, em metade dos casos a pol√≠tica n√£o convergiu para a √≥tica, assumindo valores como:\n",
        "\n",
        "![Politica final da sala 1 estoc√°stica](figs/politicaFinalSala1Esto.png)\n",
        "\n",
        "Para corrigir esse erro, a quantidade de epis√≥dios aumentou para 100 (que ainda apresentou erro), e posteriormente para 1000, ficando completamente √≥timo.\n",
        "\n",
        "Para s = 0.75, 1000 epis√≥dios n√£o foi o suficiente, enquanto 2000 parece melhorar ao limite, mas n√£o atingindo a pol√≠tica √≥tima. Nesse ambiente, se em algum momento a pol√≠tica atinge um valor para uma dire√ß√£o inv√°lida, esse valor n√£o √© alterado, uma vez que a chance de explora√ß√£o somada a estocasticidade do ambiente fazem com que os movimentos sejam quase totalmente independentes da pol√≠tica, dificultando o aprendizado.\n",
        "\n",
        "Como esperado, para s = 1, nenhuma pol√≠tica √© derivada do aprendizado, uma vez que n√£o importa a decis√£o do agente, a a√ß√£o ser√° completamente aleat√≥ria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  <b style=\"color:#22BBBB\">Sala 2</b>\n",
        "\n",
        "A sala 2 foi avaliada com a estocasticidade de 0.1, o que n√£o gerou nenhuma mudan√ßa significativa no treinamento do agente, 0.5, precisando de 1000 epis√≥dios para obter uma pol√≠tica aceit√°vel (com um estado erro no m√°ximo), e 0.75 foi necess√°rio 2000 epis√≥dios para convergir.\n",
        "\n",
        "Na sala 3 e estocasticidade 0.1, o modelo foi treinado com 10000 epis√≥dios gerando a pol√≠tica abaixo\n",
        "\n",
        "![Politica final da sala 3 estoc√°stica](figs/politicaFinalSala3sto01e1000.png)\n",
        "\n",
        "Aqui podemos ver que mesmo com uma estocasticidade baixa, o agente n√£o consegue aprender a pol√≠tica ideal, possivelmente por que para um caminho maior, esse valor se torna bem mais relevante. Para atingir uma qualidade aceit√°vel da pol√≠tica, foram necess√°rios 160000 epis√≥dios.\n",
        "Para estocasticidade de 0.5, o modelo deixou de convergir, encontrando pol√≠ticas sem sentido mesmo para 320000 epis√≥dios\n",
        "\n",
        "![Politica final da sala 3 estoc√°stica](figs/politicaFinalSala3sto05ep320000.png)\n",
        "\n",
        "Para a sala 4 \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <b style=\"color:#009900\">Metricas</b>\n",
        "\n",
        "Para as m√©tricas abaixo, foi calculado para treinos e testes com a seed 42.\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte carlo sem aproximador e em ambiente determin√≠stico, 1000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo |taxa |recompensa|passos |\n",
        "|-----------------|------|-----|----------|-------|\n",
        "|1  |0.04  |100.0|98.0      |2.0    |\n",
        "|2  |0.046 |100.0|97.71     |2.29   |\n",
        "|3  |0.264 |22.22|-57.06    |78.5   |\n",
        "|3.1|0.111 |100.0|92.35     |7.65   |\n",
        "|3.2|0.076 |100.0|95.08     |4.92   |\n",
        "|4  |0.632 |52.94|-143.03   |195.5  |\n",
        "|5  |1.142 |19.05|-306.1    |324.33 |\n",
        "|6  |8.198 |78.69|-472.26   |550.75 |\n",
        "|7  |12.497|8.51 |-2279.85  |2287.45|\n",
        "|8  |17.299|0.0  |-3001.0   |3000.0 |\n",
        "|9  |1.45  |25.64|-283.72   |298.64 |\n",
        "|10 |0.27  |47.83|-24.83    |72.13  |\n",
        "|11 |0.253 |63.16|15.79     |47.0   |\n",
        "|12 |0.249 |82.86|54.86     |27.83  |\n",
        "\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte carlo sem aproximador e em ambiente determin√≠stico, 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo  |taxa |recompensa|passos |\n",
        "|-----------------|-------|-----|----------|-------|\n",
        "|1  |0.336  |100.0|98.0      |2.0    |\n",
        "|2  |0.433  |100.0|97.71     |2.29   |\n",
        "|3  |2.338  |38.89|-25.78    |64.06  |\n",
        "|3.1|0.823  |100.0|92.7      |7.3    |\n",
        "|3.2|0.521  |100.0|95.42     |4.58   |\n",
        "|4  |1.792  |100.0|87.94     |12.06  |\n",
        "|5  |9.218  |35.71|-224.19   |259.26 |\n",
        "|6  |70.496 |96.08|-23.51    |119.5  |\n",
        "|7  |82.09  |12.77|-2169.4   |2181.3 |\n",
        "|8  |102.726|11.11|-2656.81  |2667.03|\n",
        "|9  |11.838 |43.59|-188.26   |228.97 |\n",
        "|10 |2.162  |86.96|62.26     |24.57  |\n",
        "|11 |1.22   |100.0|92.79     |7.21   |\n",
        "|12 |1.445  |100.0|91.26     |8.74   |\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte carlo sem aproximador e em ambiente determin√≠stico, 100000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo   |taxa |recompensa|passos |\n",
        "|-----------------|--------|-----|----------|-------|\n",
        "|1  |5.119   |100.0|98.0      |2.0    |\n",
        "|2  |4.104   |100.0|97.71     |2.29   |\n",
        "|3  |20.927  |44.44|-15.61    |59.5   |\n",
        "|3.1|8.976   |100.0|92.7      |7.3    |\n",
        "|3.2|6.324   |100.0|95.42     |4.58   |\n",
        "|4  |11.079  |100.0|88.41     |11.59  |\n",
        "|5  |39.406  |83.33|7.26      |75.9   |\n",
        "|6  |711.838 |98.69|43.81     |54.86  |\n",
        "|7  |912.406 |17.02|-2059.04  |2075.23|\n",
        "|8  |1366.857|36.11|-1883.43  |1918.9 |\n",
        "|9  |74.364  |66.67|-76.97    |140.31 |\n",
        "|10 |21.304  |100.0|90.83     |9.17   |\n",
        "|11 |14.014  |100.0|92.79     |7.21   |\n",
        "|12 |15.336  |100.0|91.26     |8.74   |\n",
        "\n",
        "Assim podemos verificar o ganho de efici√™ncia do algoritmo com o aumento do n√∫mero de epis√≥dios mas como o custo computacional aumenta muito, 1e4 apresenta um bom custo benef√≠cio e ser√° mantido para os outros testes.\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte carlo sem aproximador e em ambiente estoc√°stico, 0.1 , 10000 epis√≥dios</b>\n",
        "\n",
        "\n",
        "|sala             |tempo   |taxa |recompensa|passos |\n",
        "|-----------------|--------|-----|----------|-------|\n",
        "|salas/sala1.txt  |0.439   |100.0|98.0      |2.0    |\n",
        "|salas/sala2.txt  |0.431   |100.0|97.57     |2.43   |\n",
        "|salas/sala3.txt  |2.296   |44.44|-19.44    |63.33  |\n",
        "|salas/sala3.1.txt|0.778   |100.0|92.26     |7.74   |\n",
        "|salas/sala3.2.txt|0.565   |100.0|94.29     |5.71   |\n",
        "|salas/sala4.txt  |1.995   |100.0|84.44     |15.56  |\n",
        "|salas/sala5.txt  |7.895   |73.81|-100.76   |174.31 |\n",
        "|salas/sala6.txt  |60.119  |56.52|-1221.26  |1277.35|\n",
        "|salas/sala7.txt  |85.739  |17.02|-2085.98  |2102.17|\n",
        "|salas/sala8.txt  |94.089  |33.33|-2174.92  |2207.58|\n",
        "|salas/sala9.txt  |7.157   |82.05|-50.38    |122.51 |\n",
        "|salas/sala10.txt |1.953   |82.61|12.83     |43.52  |\n",
        "|salas/sala11.txt |1.776   |100.0|82.32     |7.16   |\n",
        "|salas/sala12.txt |1.266   |100.0|87.91     |12.09  |\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte carlo sem aproximador e em ambiente estoc√°stico, 0.5 , 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo   |taxa |recompensa|passos |\n",
        "|-----------------|--------|-----|----------|-------|\n",
        "|salas/sala1.txt  |0.531   |100.0|92.0      |8.0    |\n",
        "|salas/sala2.txt  |0.731   |100.0|94.29     |5.71   |\n",
        "|salas/sala3.txt  |3.615   |22.22|-61.61    |83.06  |\n",
        "|salas/sala3.1.txt|4.015   |100.0|80.61     |19.39  |\n",
        "|salas/sala3.2.txt|1.061   |100.0|91.25     |8.75   |\n",
        "|salas/sala4.txt  |3.456   |100.0|62.26     |37.74  |\n",
        "|salas/sala5.txt  |7.088   |83.33|-79.26    |162.43 |\n",
        "|salas/sala6.txt  |56.169  |66.52|-1017.18  |1083.37|\n",
        "|salas/sala7.txt  |109.078 |21.28|-2051.02  |2071.51|\n",
        "|salas/sala8.txt  |104.403 |40.28|-1856.5   |1896.18|\n",
        "|salas/sala9.txt  |5.61    |87.18|-62.36    |115.51 |\n",
        "|salas/sala10.txt |3.261   |52.17|-38.48    |69.13  |\n",
        "|salas/sala11.txt |3.617   |63.16|-11.32    |53.05  |\n",
        "|salas/sala12.txt |2.02    |97.14|57.14     |28.54  |\n",
        "\n",
        "\n",
        "##  <b style=\"color:#22BBBB\">Monte carlo sem aproximador e em ambiente estoc√°stico, 0.7 , 10000 epis√≥dios</b>\n",
        "\n",
        "|sala             |tempo   |taxa |recompensa|passos |\n",
        "|-----------------|--------|-----|----------|-------|\n",
        "|salas/sala1.txt  |0.563   |100.0|95.33     |4.67   |\n",
        "|salas/sala2.txt  |3.551   |100.0|90.57     |9.43   |\n",
        "|salas/sala3.txt  |3.477   |22.22|-60.33    |81.78  |\n",
        "|salas/sala3.1.txt|2.992   |78.26|30.04     |48.0   |\n",
        "|salas/sala3.2.txt|1.649   |100.0|79.54     |20.46  |\n",
        "|salas/sala4.txt  |7.842   |94.12|-30.68    |124.74 |\n",
        "|salas/sala5.txt  |11.378  |57.14|-168.4    |225.12 |\n",
        "|salas/sala6.txt  |105.498 |23.48|-1982.65  |2005.37|\n",
        "|salas/sala7.txt  |116.68  |17.02|-2150.62  |2166.81|\n",
        "|salas/sala8.txt  |112.991 |54.17|-1585.78  |1639.49|\n",
        "|salas/sala9.txt  |8.319   |64.1 |-188.23   |211.92 |\n",
        "|salas/sala10.txt |2.864   |56.52|-56.26    |61.91  |\n",
        "|salas/sala11.txt |3.286   |73.68|-28.68    |38.95  |\n",
        "|salas/sala12.txt |3.196   |65.71|-56.49    |64.71  |\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
